{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.new('RGB', (128, 128), color = 'red')\n",
    "print(type(img))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array(img)\n",
    "img_array = img_array/255\n",
    "img_array = img_array.transpose(2, 0, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = torch.from_numpy(img_array)\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image: Image) -> torch.Tensor:\n",
    "    img_array = np.array(image)\n",
    "    img_array = img_array/255\n",
    "    img_array = img_array.transpose(2, 0, 1).astype(np.float32)\n",
    "    img_tensor = torch.from_numpy(img_array)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image:\n",
    "    img_array = tensor.squeeze(0).numpy()\n",
    "    img_array = img_array.transpose(1, 2, 0)\n",
    "    img_array = img_array*255\n",
    "    img_array = img_array.astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = tuple[int, int, int]\n",
    "def create_empty_image(resolution: int, color: color=(0,0,0)) -> Image:\n",
    "    return Image.new('RGB', (resolution, resolution), color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = image_to_tensor(create_empty_image(128, color=(255, 0, 0)))\n",
    "target_tensor = image_to_tensor(create_empty_image(128, color=(0, 255, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss :  197.142807)\n",
      "step : 1, loss :  195.537323)\n",
      "step : 2, loss :  192.305786)\n",
      "step : 3, loss :  187.487595)\n",
      "step : 4, loss :  181.057846)\n",
      "step : 5, loss :  173.021378)\n",
      "step : 6, loss :  163.378113)\n",
      "step : 7, loss :  152.129456)\n",
      "step : 8, loss :  139.289001)\n",
      "step : 9, loss :  124.844475)\n",
      "step : 10, loss :  108.810753)\n",
      "step : 11, loss :  91.206413)\n",
      "step : 12, loss :  72.061035)\n",
      "step : 13, loss :  51.459583)\n",
      "step : 14, loss :  29.770758)\n",
      "step : 15, loss :  10.977227)\n",
      "step : 16, loss :  23.008425)\n",
      "step : 17, loss :  44.916439)\n",
      "step : 18, loss :  66.090446)\n",
      "step : 19, loss :  85.873528)\n",
      "step : 20, loss :  104.130600)\n",
      "step : 21, loss :  120.815468)\n",
      "step : 22, loss :  135.920227)\n",
      "step : 23, loss :  149.437439)\n",
      "step : 24, loss :  161.338669)\n",
      "step : 25, loss :  171.651688)\n",
      "step : 26, loss :  180.352875)\n",
      "step : 27, loss :  187.449570)\n",
      "step : 28, loss :  192.937790)\n",
      "step : 29, loss :  196.819717)\n",
      "step : 30, loss :  199.093430)\n",
      "step : 31, loss :  199.759125)\n",
      "step : 32, loss :  198.824387)\n",
      "step : 33, loss :  196.264404)\n",
      "step : 34, loss :  192.103668)\n",
      "step : 35, loss :  186.341339)\n",
      "step : 36, loss :  178.965225)\n",
      "step : 37, loss :  169.987442)\n",
      "step : 38, loss :  159.404053)\n",
      "step : 39, loss :  147.205688)\n",
      "step : 40, loss :  133.422577)\n",
      "step : 41, loss :  118.043686)\n",
      "step : 42, loss :  101.069473)\n",
      "step : 43, loss :  82.544098)\n",
      "step : 44, loss :  62.494877)\n",
      "step : 45, loss :  41.085533)\n",
      "step : 46, loss :  19.229834)\n",
      "step : 47, loss :  12.948452)\n",
      "step : 48, loss :  33.555077)\n",
      "step : 49, loss :  55.155762)\n",
      "step : 50, loss :  75.524307)\n",
      "step : 51, loss :  94.413635)\n",
      "step : 52, loss :  111.744873)\n",
      "step : 53, loss :  127.500313)\n",
      "step : 54, loss :  141.656815)\n",
      "step : 55, loss :  154.214371)\n",
      "step : 56, loss :  165.181488)\n",
      "step : 57, loss :  174.535019)\n",
      "step : 58, loss :  182.281769)\n",
      "step : 59, loss :  188.422913)\n",
      "step : 60, loss :  192.957214)\n",
      "step : 61, loss :  195.892212)\n",
      "step : 62, loss :  197.216446)\n",
      "step : 63, loss :  196.916489)\n",
      "step : 64, loss :  195.018875)\n",
      "step : 65, loss :  191.519241)\n",
      "step : 66, loss :  186.402374)\n",
      "step : 67, loss :  179.680130)\n",
      "step : 68, loss :  171.345917)\n",
      "step : 69, loss :  161.417542)\n",
      "step : 70, loss :  149.876022)\n",
      "step : 71, loss :  136.729507)\n",
      "step : 72, loss :  121.992676)\n",
      "step : 73, loss :  105.660957)\n",
      "step : 74, loss :  87.756508)\n",
      "step : 75, loss :  68.304848)\n",
      "step : 76, loss :  47.399555)\n",
      "step : 77, loss :  25.471483)\n",
      "step : 78, loss :  9.361043)\n",
      "step : 79, loss :  26.452318)\n",
      "step : 80, loss :  48.321426)\n",
      "step : 81, loss :  69.119507)\n",
      "step : 82, loss :  88.463730)\n",
      "step : 83, loss :  106.256828)\n",
      "step : 84, loss :  122.476074)\n",
      "step : 85, loss :  137.101456)\n",
      "step : 86, loss :  150.123245)\n",
      "step : 87, loss :  161.555084)\n",
      "step : 88, loss :  171.370697)\n",
      "step : 89, loss :  179.583633)\n",
      "step : 90, loss :  186.199539)\n",
      "step : 91, loss :  191.189499)\n",
      "step : 92, loss :  194.576035)\n",
      "step : 93, loss :  196.363113)\n",
      "step : 94, loss :  196.525757)\n",
      "step : 95, loss :  195.094940)\n",
      "step : 96, loss :  192.040161)\n",
      "step : 97, loss :  187.389725)\n",
      "step : 98, loss :  181.125473)\n",
      "step : 99, loss :  173.253769)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAB30lEQVR4Ae3TsQ1BARRG4UdeSChsIAqr2ExjA5XOCLawgA00opAQnTG+wrkLnJtz8k+e193isBo6YeBzmY3v/eZ12wp6zGF9XE3TYA0UwPofClAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjWwAOMC7Pj/tljr/4V/x4+v4AGdsOgyBfwywAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init_tensor.grad  \n",
    "conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n",
    "lr = 1e-5\n",
    "num_step = 100\n",
    "\n",
    "for step in range(num_step):\n",
    "    y = conv(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step : {step}, loss : {loss.item(): 3f})')\n",
    "        for param in conv.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr * param.grad\n",
    "    # conv.zero_grad()\n",
    "\n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAB30lEQVR4Ae3TsQ1BARRG4UdeSChsIAqr2ExjA5XOCLawgA00opAQnTG+wrkLnJtz8k+e193isBo6YeBzmY3v/eZ12wp6zGF9XE3TYA0UwPofClAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjWwAOMC7Pj/tljr/4V/x4+v4AGdsOgyBfwywAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(conv(init_tensor).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDWooor80PyAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABr0lEQVR4Ae3TwREAMAiEQJP+ezbpYj/YADPgndnpoIEL2aG/gQLgNyhAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4F4AAPzuIB/0LykZIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int=1, out_features: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "linear = Linear(in_features=3, out_features=3)\n",
    "x = torch.randn(1, 3)\n",
    "target_tensor = torch.randn(1, 3)\n",
    "y = linear(x)\n",
    "loss = ((y - target_tensor)**2).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2100,  0.0452, -0.6306],\n",
       "        [ 0.2792, -0.0601,  0.8381],\n",
       "        [-0.2378,  0.0512, -0.7139]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resblock(nn.Module):\n",
    "    def __init__(self, in_channels : int = 1, out_channels : int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.conv2(y)\n",
    "        # y = self.activation(y)\n",
    "        return y + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/laure/vton/laure/autoencoder_laure.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_step):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     y \u001b[39m=\u001b[39m block(init_tensor)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m (y \u001b[39m-\u001b[39;49m target_tensor)\u001b[39m.\u001b[39mnorm()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "block = Resblock(in_channels = 3, out_channels =3)\n",
    "\n",
    "lr = 1e-5\n",
    "num_step = 10000\n",
    "for step in range(num_step):\n",
    "    y = block(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step : {step}, loss : {loss.item(): 3f})')\n",
    "        for param in block.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr * param.grad\n",
    "    # conv.zero_grad()\n",
    "            \n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 4, 1, padding=0)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv5(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(4, 256, 1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, output_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv0(x)\n",
    "        y = self.conv1(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        # print(\"decoder, conv1_1 et conv 1\" ,y.shape)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        # print(\"decoder, conv2\" ,y.shape)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        # print(\"decoder, conv3\" ,y.shape)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        # print(\"decoder, conv4\" ,y.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() # type: ignore\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self) -> None:\n",
    "        self.data = list(range(100))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Dataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "    \n",
    "    def __getitem__(self, key : str|int) -> int:\n",
    "        match key:\n",
    "            case key if isinstance(key, str):\n",
    "                raise ValueError('Dataset does not take string as index.')\n",
    "            case _:\n",
    "                return self.data[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset[2]\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 124.7420883178711\n",
      "step 1 : loss 124.46776580810547\n",
      "step 2 : loss 124.17717742919922\n",
      "step 3 : loss 123.85760498046875\n",
      "step 4 : loss 123.49578094482422\n",
      "step 5 : loss 123.06446838378906\n",
      "step 6 : loss 122.53224182128906\n",
      "step 7 : loss 121.86459350585938\n",
      "step 8 : loss 121.01408386230469\n",
      "step 9 : loss 119.90970611572266\n",
      "step 10 : loss 118.47626495361328\n",
      "step 11 : loss 116.60877990722656\n",
      "step 12 : loss 114.1825942993164\n",
      "step 13 : loss 111.03224182128906\n",
      "step 14 : loss 106.95996856689453\n",
      "step 15 : loss 101.7313461303711\n",
      "step 16 : loss 95.07386016845703\n",
      "step 17 : loss 86.69741821289062\n",
      "step 18 : loss 76.39803314208984\n",
      "step 19 : loss 64.34163665771484\n",
      "step 20 : loss 52.09644317626953\n",
      "step 21 : loss 45.83066177368164\n",
      "step 22 : loss 52.42335891723633\n",
      "step 23 : loss 57.16010284423828\n",
      "step 24 : loss 54.325843811035156\n",
      "step 25 : loss 47.93583679199219\n",
      "step 26 : loss 42.39128494262695\n",
      "step 27 : loss 40.17490005493164\n",
      "step 28 : loss 40.76582336425781\n",
      "step 29 : loss 42.22091293334961\n",
      "step 30 : loss 43.23994445800781\n",
      "step 31 : loss 43.36878967285156\n",
      "step 32 : loss 42.584896087646484\n",
      "step 33 : loss 41.04551696777344\n",
      "step 34 : loss 39.00263977050781\n",
      "step 35 : loss 36.78874588012695\n",
      "step 36 : loss 34.79612350463867\n",
      "step 37 : loss 33.39086151123047\n",
      "step 38 : loss 32.71719741821289\n",
      "step 39 : loss 32.54681396484375\n",
      "step 40 : loss 32.39885711669922\n",
      "step 41 : loss 31.87609100341797\n",
      "step 42 : loss 30.88205337524414\n",
      "step 43 : loss 29.59592628479004\n",
      "step 44 : loss 28.325220108032227\n",
      "step 45 : loss 27.332578659057617\n",
      "step 46 : loss 26.714054107666016\n",
      "step 47 : loss 26.389036178588867\n",
      "step 48 : loss 26.176668167114258\n",
      "step 49 : loss 25.88829803466797\n",
      "step 50 : loss 25.391101837158203\n",
      "step 51 : loss 24.652894973754883\n",
      "step 52 : loss 23.749469757080078\n",
      "step 53 : loss 22.84598731994629\n",
      "step 54 : loss 22.131851196289062\n",
      "step 55 : loss 21.699966430664062\n",
      "step 56 : loss 21.441234588623047\n",
      "step 57 : loss 21.114601135253906\n",
      "step 58 : loss 20.563724517822266\n",
      "step 59 : loss 19.866436004638672\n",
      "step 60 : loss 19.273889541625977\n",
      "step 61 : loss 18.962894439697266\n",
      "step 62 : loss 18.81789779663086\n",
      "step 63 : loss 18.547780990600586\n",
      "step 64 : loss 18.028837203979492\n",
      "step 65 : loss 17.455495834350586\n",
      "step 66 : loss 17.114709854125977\n",
      "step 67 : loss 16.97042465209961\n",
      "step 68 : loss 16.706951141357422\n",
      "step 69 : loss 16.255329132080078\n",
      "step 70 : loss 15.862043380737305\n",
      "step 71 : loss 15.659345626831055\n",
      "step 72 : loss 15.482170104980469\n",
      "step 73 : loss 15.163753509521484\n",
      "step 74 : loss 14.792868614196777\n",
      "step 75 : loss 14.574877738952637\n",
      "step 76 : loss 14.438593864440918\n",
      "step 77 : loss 14.152137756347656\n",
      "step 78 : loss 13.845890045166016\n",
      "step 79 : loss 13.706380844116211\n",
      "step 80 : loss 13.534822463989258\n",
      "step 81 : loss 13.24682903289795\n",
      "step 82 : loss 13.051287651062012\n",
      "step 83 : loss 12.917190551757812\n",
      "step 84 : loss 12.674283981323242\n",
      "step 85 : loss 12.456520080566406\n",
      "step 86 : loss 12.319266319274902\n",
      "step 87 : loss 12.09184741973877\n",
      "step 88 : loss 11.887382507324219\n",
      "step 89 : loss 11.751428604125977\n",
      "step 90 : loss 11.513233184814453\n",
      "step 91 : loss 11.358182907104492\n",
      "step 92 : loss 11.180697441101074\n",
      "step 93 : loss 10.98099422454834\n",
      "step 94 : loss 10.842533111572266\n",
      "step 95 : loss 10.631911277770996\n",
      "step 96 : loss 10.490951538085938\n",
      "step 97 : loss 10.293822288513184\n",
      "step 98 : loss 10.142075538635254\n",
      "step 99 : loss 9.95730209350586\n",
      "step 100 : loss 9.802809715270996\n",
      "step 101 : loss 9.627227783203125\n",
      "step 102 : loss 9.481529235839844\n",
      "step 103 : loss 9.310887336730957\n",
      "step 104 : loss 9.176655769348145\n",
      "step 105 : loss 9.010056495666504\n",
      "step 106 : loss 8.882487297058105\n",
      "step 107 : loss 8.721991539001465\n",
      "step 108 : loss 8.590359687805176\n",
      "step 109 : loss 8.447903633117676\n",
      "step 110 : loss 8.306612014770508\n",
      "step 111 : loss 8.194684028625488\n",
      "step 112 : loss 8.095148086547852\n",
      "step 113 : loss 8.098938941955566\n",
      "step 114 : loss 8.197709083557129\n",
      "step 115 : loss 7.912267208099365\n",
      "step 116 : loss 7.597428321838379\n",
      "step 117 : loss 7.650055408477783\n",
      "step 118 : loss 7.619634628295898\n",
      "step 119 : loss 7.332236289978027\n",
      "step 120 : loss 7.268025875091553\n",
      "step 121 : loss 7.291615009307861\n",
      "step 122 : loss 7.085897922515869\n",
      "step 123 : loss 6.969330310821533\n",
      "step 124 : loss 6.97951602935791\n",
      "step 125 : loss 6.8471293449401855\n",
      "step 126 : loss 6.706279754638672\n",
      "step 127 : loss 6.678657531738281\n",
      "step 128 : loss 6.61463737487793\n",
      "step 129 : loss 6.483481407165527\n",
      "step 130 : loss 6.3928704261779785\n",
      "step 131 : loss 6.353971481323242\n",
      "step 132 : loss 6.288694381713867\n",
      "step 133 : loss 6.16945219039917\n",
      "step 134 : loss 6.083752632141113\n",
      "step 135 : loss 6.058629035949707\n",
      "step 136 : loss 5.992129802703857\n",
      "step 137 : loss 5.878505706787109\n",
      "step 138 : loss 5.813688278198242\n",
      "step 139 : loss 5.776245594024658\n",
      "step 140 : loss 5.709991931915283\n",
      "step 141 : loss 5.636614799499512\n",
      "step 142 : loss 5.569769859313965\n",
      "step 143 : loss 5.501336097717285\n",
      "step 144 : loss 5.44733190536499\n",
      "step 145 : loss 5.410060405731201\n",
      "step 146 : loss 5.3609161376953125\n",
      "step 147 : loss 5.294526100158691\n",
      "step 148 : loss 5.238359451293945\n",
      "step 149 : loss 5.211272239685059\n",
      "step 150 : loss 5.219822883605957\n",
      "step 151 : loss 5.3059892654418945\n",
      "step 152 : loss 5.400583267211914\n",
      "step 153 : loss 5.3408708572387695\n",
      "step 154 : loss 5.026799201965332\n",
      "step 155 : loss 4.896524906158447\n",
      "step 156 : loss 4.988734722137451\n",
      "step 157 : loss 5.065941333770752\n",
      "step 158 : loss 4.959782123565674\n",
      "step 159 : loss 4.770920753479004\n",
      "step 160 : loss 4.730282306671143\n",
      "step 161 : loss 4.8060784339904785\n",
      "step 162 : loss 4.806081771850586\n",
      "step 163 : loss 4.682000637054443\n",
      "step 164 : loss 4.567831993103027\n",
      "step 165 : loss 4.572911262512207\n",
      "step 166 : loss 4.621994495391846\n",
      "step 167 : loss 4.577053546905518\n",
      "step 168 : loss 4.478930473327637\n",
      "step 169 : loss 4.419254302978516\n",
      "step 170 : loss 4.414213180541992\n",
      "step 171 : loss 4.419983863830566\n",
      "step 172 : loss 4.39603328704834\n",
      "step 173 : loss 4.34735631942749\n",
      "step 174 : loss 4.287298202514648\n",
      "step 175 : loss 4.248508453369141\n",
      "step 176 : loss 4.240312099456787\n",
      "step 177 : loss 4.24079704284668\n",
      "step 178 : loss 4.224051475524902\n",
      "step 179 : loss 4.1808037757873535\n",
      "step 180 : loss 4.136662483215332\n",
      "step 181 : loss 4.101368427276611\n",
      "step 182 : loss 4.075906276702881\n",
      "step 183 : loss 4.0595622062683105\n",
      "step 184 : loss 4.053538799285889\n",
      "step 185 : loss 4.053393363952637\n",
      "step 186 : loss 4.043428897857666\n",
      "step 187 : loss 4.015964508056641\n",
      "step 188 : loss 3.975687265396118\n",
      "step 189 : loss 3.9430861473083496\n",
      "step 190 : loss 3.9344356060028076\n",
      "step 191 : loss 3.9575438499450684\n",
      "step 192 : loss 4.020942687988281\n",
      "step 193 : loss 4.064566612243652\n",
      "step 194 : loss 4.053803443908691\n",
      "step 195 : loss 3.9278945922851562\n",
      "step 196 : loss 3.820314407348633\n",
      "step 197 : loss 3.76680064201355\n",
      "step 198 : loss 3.7610867023468018\n",
      "step 199 : loss 3.779017925262451\n",
      "step 200 : loss 3.7940714359283447\n",
      "step 201 : loss 3.8029093742370605\n",
      "step 202 : loss 3.781891345977783\n",
      "step 203 : loss 3.7527594566345215\n",
      "step 204 : loss 3.7101118564605713\n",
      "step 205 : loss 3.6714656352996826\n",
      "step 206 : loss 3.635220527648926\n",
      "step 207 : loss 3.6031055450439453\n",
      "step 208 : loss 3.5798394680023193\n",
      "step 209 : loss 3.5703043937683105\n",
      "step 210 : loss 3.580291986465454\n",
      "step 211 : loss 3.6002962589263916\n",
      "step 212 : loss 3.6179771423339844\n",
      "step 213 : loss 3.5989649295806885\n",
      "step 214 : loss 3.556774854660034\n",
      "step 215 : loss 3.5019643306732178\n",
      "step 216 : loss 3.4613773822784424\n",
      "step 217 : loss 3.4368765354156494\n",
      "step 218 : loss 3.4267640113830566\n",
      "step 219 : loss 3.4243669509887695\n",
      "step 220 : loss 3.424048900604248\n",
      "step 221 : loss 3.4160118103027344\n",
      "step 222 : loss 3.397462844848633\n",
      "step 223 : loss 3.3689019680023193\n",
      "step 224 : loss 3.339576005935669\n",
      "step 225 : loss 3.3147637844085693\n",
      "step 226 : loss 3.296766519546509\n",
      "step 227 : loss 3.285126209259033\n",
      "step 228 : loss 3.2810511589050293\n",
      "step 229 : loss 3.28974986076355\n",
      "step 230 : loss 3.3260250091552734\n",
      "step 231 : loss 3.402541160583496\n",
      "step 232 : loss 3.5249762535095215\n",
      "step 233 : loss 3.5399041175842285\n",
      "step 234 : loss 3.464991807937622\n",
      "step 235 : loss 3.324345588684082\n",
      "step 236 : loss 3.2574315071105957\n",
      "step 237 : loss 3.2379887104034424\n",
      "step 238 : loss 3.2397093772888184\n",
      "step 239 : loss 3.244556427001953\n",
      "step 240 : loss 3.2481155395507812\n",
      "step 241 : loss 3.2704074382781982\n",
      "step 242 : loss 3.2760515213012695\n",
      "step 243 : loss 3.2626614570617676\n",
      "step 244 : loss 3.1863584518432617\n",
      "step 245 : loss 3.1086504459381104\n",
      "step 246 : loss 3.059558391571045\n",
      "step 247 : loss 3.0536975860595703\n",
      "step 248 : loss 3.084487199783325\n",
      "step 249 : loss 3.126924991607666\n",
      "step 250 : loss 3.1544392108917236\n",
      "step 251 : loss 3.1277530193328857\n",
      "step 252 : loss 3.090803384780884\n",
      "step 253 : loss 3.0575380325317383\n",
      "step 254 : loss 3.044386863708496\n",
      "step 255 : loss 3.0314395427703857\n",
      "step 256 : loss 3.010720729827881\n",
      "step 257 : loss 2.9795780181884766\n",
      "step 258 : loss 2.9480409622192383\n",
      "step 259 : loss 2.9240059852600098\n",
      "step 260 : loss 2.9103641510009766\n",
      "step 261 : loss 2.905458450317383\n",
      "step 262 : loss 2.9056379795074463\n",
      "step 263 : loss 2.9061834812164307\n",
      "step 264 : loss 2.9038448333740234\n",
      "step 265 : loss 2.895895481109619\n",
      "step 266 : loss 2.883704662322998\n",
      "step 267 : loss 2.8683664798736572\n",
      "step 268 : loss 2.8527441024780273\n",
      "step 269 : loss 2.838834285736084\n",
      "step 270 : loss 2.83004093170166\n",
      "step 271 : loss 2.8341939449310303\n",
      "step 272 : loss 2.869131326675415\n",
      "step 273 : loss 2.9770445823669434\n",
      "step 274 : loss 3.1356616020202637\n",
      "step 275 : loss 3.227943181991577\n",
      "step 276 : loss 3.0028133392333984\n",
      "step 277 : loss 2.8281495571136475\n",
      "step 278 : loss 2.751692771911621\n",
      "step 279 : loss 2.743196725845337\n",
      "step 280 : loss 2.786186456680298\n",
      "step 281 : loss 2.8826723098754883\n",
      "step 282 : loss 3.0088188648223877\n",
      "step 283 : loss 2.981735944747925\n",
      "step 284 : loss 2.8724260330200195\n",
      "step 285 : loss 2.7540717124938965\n",
      "step 286 : loss 2.7006778717041016\n",
      "step 287 : loss 2.69266414642334\n",
      "step 288 : loss 2.716046094894409\n",
      "step 289 : loss 2.759347677230835\n",
      "step 290 : loss 2.792771816253662\n",
      "step 291 : loss 2.7956511974334717\n",
      "step 292 : loss 2.749516725540161\n",
      "step 293 : loss 2.708773612976074\n",
      "step 294 : loss 2.688270092010498\n",
      "step 295 : loss 2.705587387084961\n",
      "step 296 : loss 2.7359185218811035\n",
      "step 297 : loss 2.7581851482391357\n",
      "step 298 : loss 2.7167398929595947\n",
      "step 299 : loss 2.653010129928589\n",
      "step 300 : loss 2.596038341522217\n",
      "step 301 : loss 2.5683131217956543\n",
      "step 302 : loss 2.5655810832977295\n",
      "step 303 : loss 2.582334041595459\n",
      "step 304 : loss 2.6103880405426025\n",
      "step 305 : loss 2.6383559703826904\n",
      "step 306 : loss 2.644911050796509\n",
      "step 307 : loss 2.6361677646636963\n",
      "step 308 : loss 2.622654676437378\n",
      "step 309 : loss 2.6433887481689453\n",
      "step 310 : loss 2.6795899868011475\n",
      "step 311 : loss 2.719484329223633\n",
      "step 312 : loss 2.666372060775757\n",
      "step 313 : loss 2.5903213024139404\n",
      "step 314 : loss 2.5176897048950195\n",
      "step 315 : loss 2.4796149730682373\n",
      "step 316 : loss 2.4657859802246094\n",
      "step 317 : loss 2.4684016704559326\n",
      "step 318 : loss 2.480923652648926\n",
      "step 319 : loss 2.4997572898864746\n",
      "step 320 : loss 2.5174248218536377\n",
      "step 321 : loss 2.533886432647705\n",
      "step 322 : loss 2.543691396713257\n",
      "step 323 : loss 2.5699822902679443\n",
      "step 324 : loss 2.5934810638427734\n",
      "step 325 : loss 2.626189947128296\n",
      "step 326 : loss 2.5847692489624023\n",
      "step 327 : loss 2.525177240371704\n",
      "step 328 : loss 2.4508447647094727\n",
      "step 329 : loss 2.4019625186920166\n",
      "step 330 : loss 2.3718667030334473\n",
      "step 331 : loss 2.355560302734375\n",
      "step 332 : loss 2.3468551635742188\n",
      "step 333 : loss 2.3426899909973145\n",
      "step 334 : loss 2.342430353164673\n",
      "step 335 : loss 2.3485891819000244\n",
      "step 336 : loss 2.3670146465301514\n",
      "step 337 : loss 2.4099090099334717\n",
      "step 338 : loss 2.478245258331299\n",
      "step 339 : loss 2.5616672039031982\n",
      "step 340 : loss 2.5590109825134277\n",
      "step 341 : loss 2.526062488555908\n",
      "step 342 : loss 2.462829828262329\n",
      "step 343 : loss 2.443964719772339\n",
      "step 344 : loss 2.4184482097625732\n",
      "step 345 : loss 2.390634536743164\n",
      "step 346 : loss 2.3439149856567383\n",
      "step 347 : loss 2.3041203022003174\n",
      "step 348 : loss 2.274639129638672\n",
      "step 349 : loss 2.2589755058288574\n",
      "step 350 : loss 2.2570199966430664\n",
      "step 351 : loss 2.279017210006714\n",
      "step 352 : loss 2.345592975616455\n",
      "step 353 : loss 2.4829251766204834\n",
      "step 354 : loss 2.55033016204834\n",
      "step 355 : loss 2.4954638481140137\n",
      "step 356 : loss 2.3360815048217773\n",
      "step 357 : loss 2.2500202655792236\n",
      "step 358 : loss 2.215101718902588\n",
      "step 359 : loss 2.2079570293426514\n",
      "step 360 : loss 2.2160542011260986\n",
      "step 361 : loss 2.2380661964416504\n",
      "step 362 : loss 2.2698733806610107\n",
      "step 363 : loss 2.2994425296783447\n",
      "step 364 : loss 2.3048622608184814\n",
      "step 365 : loss 2.280806303024292\n",
      "step 366 : loss 2.258942127227783\n",
      "step 367 : loss 2.268721580505371\n",
      "step 368 : loss 2.3506791591644287\n",
      "step 369 : loss 2.432752847671509\n",
      "step 370 : loss 2.4468538761138916\n",
      "step 371 : loss 2.2966959476470947\n",
      "step 372 : loss 2.1913864612579346\n",
      "step 373 : loss 2.1377017498016357\n",
      "step 374 : loss 2.117248773574829\n",
      "step 375 : loss 2.112138032913208\n",
      "step 376 : loss 2.1178581714630127\n",
      "step 377 : loss 2.1376240253448486\n",
      "step 378 : loss 2.184481382369995\n",
      "step 379 : loss 2.2628908157348633\n",
      "step 380 : loss 2.359591007232666\n",
      "step 381 : loss 2.3494997024536133\n",
      "step 382 : loss 2.3075151443481445\n",
      "step 383 : loss 2.242692470550537\n",
      "step 384 : loss 2.224867343902588\n",
      "step 385 : loss 2.1954782009124756\n",
      "step 386 : loss 2.1612253189086914\n",
      "step 387 : loss 2.1158039569854736\n",
      "step 388 : loss 2.0813188552856445\n",
      "step 389 : loss 2.058159112930298\n",
      "step 390 : loss 2.0452327728271484\n",
      "step 391 : loss 2.039963722229004\n",
      "step 392 : loss 2.047562837600708\n",
      "step 393 : loss 2.0859997272491455\n",
      "step 394 : loss 2.1991002559661865\n",
      "step 395 : loss 2.34511399269104\n",
      "step 396 : loss 2.390577793121338\n",
      "step 397 : loss 2.1847307682037354\n",
      "step 398 : loss 2.059190273284912\n",
      "step 399 : loss 2.007840871810913\n",
      "step 400 : loss 1.9915075302124023\n",
      "step 401 : loss 1.9911983013153076\n",
      "step 402 : loss 2.0076231956481934\n",
      "step 403 : loss 2.0575625896453857\n",
      "step 404 : loss 2.1589059829711914\n",
      "step 405 : loss 2.3023948669433594\n",
      "step 406 : loss 2.2684288024902344\n",
      "step 407 : loss 2.187947988510132\n",
      "step 408 : loss 2.1162753105163574\n",
      "step 409 : loss 2.1164724826812744\n",
      "step 410 : loss 2.114131212234497\n",
      "step 411 : loss 2.081826686859131\n",
      "step 412 : loss 2.0257084369659424\n",
      "step 413 : loss 1.9781053066253662\n",
      "step 414 : loss 1.9497039318084717\n",
      "step 415 : loss 1.9352539777755737\n",
      "step 416 : loss 1.929860234260559\n",
      "step 417 : loss 1.9349843263626099\n",
      "step 418 : loss 1.9687532186508179\n",
      "step 419 : loss 2.0681889057159424\n",
      "step 420 : loss 2.256274461746216\n",
      "step 421 : loss 2.248155117034912\n",
      "step 422 : loss 2.1174726486206055\n",
      "step 423 : loss 1.9840103387832642\n",
      "step 424 : loss 1.931505560874939\n",
      "step 425 : loss 1.9177143573760986\n",
      "step 426 : loss 1.926999807357788\n",
      "step 427 : loss 1.9558649063110352\n",
      "step 428 : loss 1.9994176626205444\n",
      "step 429 : loss 2.0341603755950928\n",
      "step 430 : loss 2.031766653060913\n",
      "step 431 : loss 1.99805748462677\n",
      "step 432 : loss 1.9899290800094604\n",
      "step 433 : loss 2.03865122795105\n",
      "step 434 : loss 2.163914680480957\n",
      "step 435 : loss 2.1532697677612305\n",
      "step 436 : loss 2.0583271980285645\n",
      "step 437 : loss 1.935869574546814\n",
      "step 438 : loss 1.8761730194091797\n",
      "step 439 : loss 1.8506311178207397\n",
      "step 440 : loss 1.8465282917022705\n",
      "step 441 : loss 1.8652665615081787\n",
      "step 442 : loss 1.9374750852584839\n",
      "step 443 : loss 2.085587978363037\n",
      "step 444 : loss 2.2351150512695312\n",
      "step 445 : loss 2.05721378326416\n",
      "step 446 : loss 1.9081392288208008\n",
      "step 447 : loss 1.8375887870788574\n",
      "step 448 : loss 1.8124585151672363\n",
      "step 449 : loss 1.801201581954956\n",
      "step 450 : loss 1.7964125871658325\n",
      "step 451 : loss 1.8006677627563477\n",
      "step 452 : loss 1.8270373344421387\n",
      "step 453 : loss 1.9062968492507935\n",
      "step 454 : loss 2.080129861831665\n",
      "step 455 : loss 2.1451947689056396\n",
      "step 456 : loss 2.0530083179473877\n",
      "step 457 : loss 1.8897758722305298\n",
      "step 458 : loss 1.8299297094345093\n",
      "step 459 : loss 1.8274420499801636\n",
      "step 460 : loss 1.8638118505477905\n",
      "step 461 : loss 1.9122016429901123\n",
      "step 462 : loss 1.9404094219207764\n",
      "step 463 : loss 1.902071237564087\n",
      "step 464 : loss 1.8462815284729004\n",
      "step 465 : loss 1.8017010688781738\n",
      "step 466 : loss 1.7837610244750977\n",
      "step 467 : loss 1.793137550354004\n",
      "step 468 : loss 1.8563798666000366\n",
      "step 469 : loss 1.9776188135147095\n",
      "step 470 : loss 2.096630334854126\n",
      "step 471 : loss 1.962978720664978\n",
      "step 472 : loss 1.850612759590149\n",
      "step 473 : loss 1.797640323638916\n",
      "step 474 : loss 1.7942101955413818\n",
      "step 475 : loss 1.8108505010604858\n",
      "step 476 : loss 1.833648681640625\n",
      "step 477 : loss 1.842747449874878\n",
      "step 478 : loss 1.838782548904419\n",
      "step 479 : loss 1.8335514068603516\n",
      "step 480 : loss 1.8724619150161743\n",
      "step 481 : loss 1.942064881324768\n",
      "step 482 : loss 2.016371488571167\n",
      "step 483 : loss 1.9100762605667114\n",
      "step 484 : loss 1.8121041059494019\n",
      "step 485 : loss 1.7461894750595093\n",
      "step 486 : loss 1.717808723449707\n",
      "step 487 : loss 1.706406831741333\n",
      "step 488 : loss 1.710052728652954\n",
      "step 489 : loss 1.7300655841827393\n",
      "step 490 : loss 1.7815883159637451\n",
      "step 491 : loss 1.8598428964614868\n",
      "step 492 : loss 1.9560068845748901\n",
      "step 493 : loss 1.9216384887695312\n",
      "step 494 : loss 1.8822046518325806\n",
      "step 495 : loss 1.8261737823486328\n",
      "step 496 : loss 1.7982633113861084\n",
      "step 497 : loss 1.759548306465149\n",
      "step 498 : loss 1.722900152206421\n",
      "step 499 : loss 1.6938847303390503\n",
      "step 500 : loss 1.6797996759414673\n",
      "step 501 : loss 1.6824908256530762\n",
      "step 502 : loss 1.7144651412963867\n",
      "step 503 : loss 1.7950692176818848\n",
      "step 504 : loss 1.9348801374435425\n",
      "step 505 : loss 1.927825927734375\n",
      "step 506 : loss 1.8458259105682373\n",
      "step 507 : loss 1.74216890335083\n",
      "step 508 : loss 1.7027796506881714\n",
      "step 509 : loss 1.6869525909423828\n",
      "step 510 : loss 1.684944987297058\n",
      "step 511 : loss 1.6844197511672974\n",
      "step 512 : loss 1.6904692649841309\n",
      "step 513 : loss 1.6979721784591675\n",
      "step 514 : loss 1.7221328020095825\n",
      "step 515 : loss 1.7619333267211914\n",
      "step 516 : loss 1.848618745803833\n",
      "step 517 : loss 1.8728617429733276\n",
      "step 518 : loss 1.8536053895950317\n",
      "step 519 : loss 1.7488007545471191\n",
      "step 520 : loss 1.6833988428115845\n",
      "step 521 : loss 1.6448837518692017\n",
      "step 522 : loss 1.6261177062988281\n",
      "step 523 : loss 1.6190087795257568\n",
      "step 524 : loss 1.6225825548171997\n",
      "step 525 : loss 1.6366103887557983\n",
      "step 526 : loss 1.6638938188552856\n",
      "step 527 : loss 1.7053492069244385\n",
      "step 528 : loss 1.7709007263183594\n",
      "step 529 : loss 1.8137847185134888\n",
      "step 530 : loss 1.8465172052383423\n",
      "step 531 : loss 1.7722588777542114\n",
      "step 532 : loss 1.7152159214019775\n",
      "step 533 : loss 1.655995488166809\n",
      "step 534 : loss 1.6202512979507446\n",
      "step 535 : loss 1.5964186191558838\n",
      "step 536 : loss 1.5894043445587158\n",
      "step 537 : loss 1.5989974737167358\n",
      "step 538 : loss 1.6432615518569946\n",
      "step 539 : loss 1.734748125076294\n",
      "step 540 : loss 1.8693856000900269\n",
      "step 541 : loss 1.812119483947754\n",
      "step 542 : loss 1.7260236740112305\n",
      "step 543 : loss 1.6520367860794067\n",
      "step 544 : loss 1.6240875720977783\n",
      "step 545 : loss 1.6059504747390747\n",
      "step 546 : loss 1.5932574272155762\n",
      "step 547 : loss 1.5861369371414185\n",
      "step 548 : loss 1.5952814817428589\n",
      "step 549 : loss 1.631123423576355\n",
      "step 550 : loss 1.7218433618545532\n",
      "step 551 : loss 1.8018178939819336\n",
      "step 552 : loss 1.8214149475097656\n",
      "step 553 : loss 1.684476375579834\n",
      "step 554 : loss 1.6077115535736084\n",
      "step 555 : loss 1.571054220199585\n",
      "step 556 : loss 1.5588432550430298\n",
      "step 557 : loss 1.5523320436477661\n",
      "step 558 : loss 1.5543140172958374\n",
      "step 559 : loss 1.5637019872665405\n",
      "step 560 : loss 1.5907783508300781\n",
      "step 561 : loss 1.634250283241272\n",
      "step 562 : loss 1.7172106504440308\n",
      "step 563 : loss 1.753588318824768\n",
      "step 564 : loss 1.7591959238052368\n",
      "step 565 : loss 1.661780834197998\n",
      "step 566 : loss 1.598819613456726\n",
      "step 567 : loss 1.5568161010742188\n",
      "step 568 : loss 1.5334886312484741\n",
      "step 569 : loss 1.5198938846588135\n",
      "step 570 : loss 1.5181092023849487\n",
      "step 571 : loss 1.5310628414154053\n",
      "step 572 : loss 1.570675253868103\n",
      "step 573 : loss 1.6429768800735474\n",
      "step 574 : loss 1.7466239929199219\n",
      "step 575 : loss 1.7271616458892822\n",
      "step 576 : loss 1.676567792892456\n",
      "step 577 : loss 1.59610915184021\n",
      "step 578 : loss 1.5565332174301147\n",
      "step 579 : loss 1.5259687900543213\n",
      "step 580 : loss 1.508244276046753\n",
      "step 581 : loss 1.4986517429351807\n",
      "step 582 : loss 1.5074591636657715\n",
      "step 583 : loss 1.5412155389785767\n",
      "step 584 : loss 1.6280008554458618\n",
      "step 585 : loss 1.7118394374847412\n",
      "step 586 : loss 1.7459189891815186\n",
      "step 587 : loss 1.6202874183654785\n",
      "step 588 : loss 1.550533652305603\n",
      "step 589 : loss 1.5204391479492188\n",
      "step 590 : loss 1.5115065574645996\n",
      "step 591 : loss 1.5032539367675781\n",
      "step 592 : loss 1.4977521896362305\n",
      "step 593 : loss 1.4996099472045898\n",
      "step 594 : loss 1.5231815576553345\n",
      "step 595 : loss 1.5773239135742188\n",
      "step 596 : loss 1.6775544881820679\n",
      "step 597 : loss 1.6878615617752075\n",
      "step 598 : loss 1.6456212997436523\n",
      "step 599 : loss 1.5468107461929321\n",
      "step 600 : loss 1.49742591381073\n",
      "step 601 : loss 1.470018982887268\n",
      "step 602 : loss 1.4603099822998047\n",
      "step 603 : loss 1.4610470533370972\n",
      "step 604 : loss 1.484508752822876\n",
      "step 605 : loss 1.5353984832763672\n",
      "step 606 : loss 1.6299183368682861\n",
      "step 607 : loss 1.6568055152893066\n",
      "step 608 : loss 1.639755368232727\n",
      "step 609 : loss 1.551527500152588\n",
      "step 610 : loss 1.507979154586792\n",
      "step 611 : loss 1.4829392433166504\n",
      "step 612 : loss 1.4727783203125\n",
      "step 613 : loss 1.4661414623260498\n",
      "step 614 : loss 1.474584698677063\n",
      "step 615 : loss 1.5023747682571411\n",
      "step 616 : loss 1.5692694187164307\n",
      "step 617 : loss 1.6126888990402222\n",
      "step 618 : loss 1.624904990196228\n",
      "step 619 : loss 1.5392112731933594\n",
      "step 620 : loss 1.487363576889038\n",
      "step 621 : loss 1.4559905529022217\n",
      "step 622 : loss 1.450886845588684\n",
      "step 623 : loss 1.4544520378112793\n",
      "step 624 : loss 1.479225516319275\n",
      "step 625 : loss 1.5106580257415771\n",
      "step 626 : loss 1.560874104499817\n",
      "step 627 : loss 1.5568413734436035\n",
      "step 628 : loss 1.5445120334625244\n",
      "step 629 : loss 1.4979599714279175\n",
      "step 630 : loss 1.475873351097107\n",
      "step 631 : loss 1.4593956470489502\n",
      "step 632 : loss 1.4622266292572021\n",
      "step 633 : loss 1.4668524265289307\n",
      "step 634 : loss 1.4871083498001099\n",
      "step 635 : loss 1.4954676628112793\n",
      "step 636 : loss 1.5117247104644775\n",
      "step 637 : loss 1.4959797859191895\n",
      "step 638 : loss 1.4899213314056396\n",
      "step 639 : loss 1.4672695398330688\n",
      "step 640 : loss 1.4623961448669434\n",
      "step 641 : loss 1.451990008354187\n",
      "step 642 : loss 1.4579463005065918\n",
      "step 643 : loss 1.4562656879425049\n",
      "step 644 : loss 1.4681562185287476\n",
      "step 645 : loss 1.464463233947754\n",
      "step 646 : loss 1.471261739730835\n",
      "step 647 : loss 1.4592983722686768\n",
      "step 648 : loss 1.4589684009552002\n",
      "step 649 : loss 1.446445107460022\n",
      "step 650 : loss 1.4467774629592896\n",
      "step 651 : loss 1.4387528896331787\n",
      "step 652 : loss 1.4422650337219238\n",
      "step 653 : loss 1.4366481304168701\n",
      "step 654 : loss 1.4415513277053833\n",
      "step 655 : loss 1.4355978965759277\n",
      "step 656 : loss 1.440794825553894\n",
      "step 657 : loss 1.4339908361434937\n",
      "step 658 : loss 1.4393320083618164\n",
      "step 659 : loss 1.431570291519165\n",
      "step 660 : loss 1.4360243082046509\n",
      "step 661 : loss 1.4270761013031006\n",
      "step 662 : loss 1.4301844835281372\n",
      "step 663 : loss 1.4211887121200562\n",
      "step 664 : loss 1.4240976572036743\n",
      "step 665 : loss 1.416085958480835\n",
      "step 666 : loss 1.419159173965454\n",
      "step 667 : loss 1.4115300178527832\n",
      "step 668 : loss 1.4140818119049072\n",
      "step 669 : loss 1.4062532186508179\n",
      "step 670 : loss 1.4085623025894165\n",
      "step 671 : loss 1.4012707471847534\n",
      "step 672 : loss 1.404604434967041\n",
      "step 673 : loss 1.3985779285430908\n",
      "step 674 : loss 1.4038234949111938\n",
      "step 675 : loss 1.3990031480789185\n",
      "step 676 : loss 1.406519889831543\n",
      "step 677 : loss 1.4029242992401123\n",
      "step 678 : loss 1.4130017757415771\n",
      "step 679 : loss 1.410418152809143\n",
      "step 680 : loss 1.420767068862915\n",
      "step 681 : loss 1.4165011644363403\n",
      "step 682 : loss 1.4206281900405884\n",
      "step 683 : loss 1.412230134010315\n",
      "step 684 : loss 1.407721757888794\n",
      "step 685 : loss 1.3979219198226929\n",
      "step 686 : loss 1.3903933763504028\n",
      "step 687 : loss 1.383458137512207\n",
      "step 688 : loss 1.3778233528137207\n",
      "step 689 : loss 1.3743972778320312\n",
      "step 690 : loss 1.370527982711792\n",
      "step 691 : loss 1.368043065071106\n",
      "step 692 : loss 1.3640575408935547\n",
      "step 693 : loss 1.3610894680023193\n",
      "step 694 : loss 1.358025074005127\n",
      "step 695 : loss 1.3567109107971191\n",
      "step 696 : loss 1.358599066734314\n",
      "step 697 : loss 1.3629794120788574\n",
      "step 698 : loss 1.3766388893127441\n",
      "step 699 : loss 1.3915890455245972\n",
      "step 700 : loss 1.4231003522872925\n",
      "step 701 : loss 1.4304111003875732\n",
      "step 702 : loss 1.4431602954864502\n",
      "step 703 : loss 1.4070125818252563\n",
      "step 704 : loss 1.385886549949646\n",
      "step 705 : loss 1.356918454170227\n",
      "step 706 : loss 1.3481836318969727\n",
      "step 707 : loss 1.3422356843948364\n",
      "step 708 : loss 1.3558639287948608\n",
      "step 709 : loss 1.3706748485565186\n",
      "step 710 : loss 1.40421724319458\n",
      "step 711 : loss 1.4075913429260254\n",
      "step 712 : loss 1.4145220518112183\n",
      "step 713 : loss 1.3829267024993896\n",
      "step 714 : loss 1.367699384689331\n",
      "step 715 : loss 1.3465927839279175\n",
      "step 716 : loss 1.3404273986816406\n",
      "step 717 : loss 1.3330787420272827\n",
      "step 718 : loss 1.3381812572479248\n",
      "step 719 : loss 1.343291163444519\n",
      "step 720 : loss 1.3635032176971436\n",
      "step 721 : loss 1.3714085817337036\n",
      "step 722 : loss 1.3871634006500244\n",
      "step 723 : loss 1.3683574199676514\n",
      "step 724 : loss 1.358959674835205\n",
      "step 725 : loss 1.3341808319091797\n",
      "step 726 : loss 1.326164960861206\n",
      "step 727 : loss 1.315002202987671\n",
      "step 728 : loss 1.3185182809829712\n",
      "step 729 : loss 1.319228172302246\n",
      "step 730 : loss 1.3335002660751343\n",
      "step 731 : loss 1.3370522260665894\n",
      "step 732 : loss 1.3508310317993164\n",
      "step 733 : loss 1.3428318500518799\n",
      "step 734 : loss 1.3455042839050293\n",
      "step 735 : loss 1.3306158781051636\n",
      "step 736 : loss 1.328412652015686\n",
      "step 737 : loss 1.3161026239395142\n",
      "step 738 : loss 1.3141082525253296\n",
      "step 739 : loss 1.306362509727478\n",
      "step 740 : loss 1.3076668977737427\n",
      "step 741 : loss 1.3050777912139893\n",
      "step 742 : loss 1.3108794689178467\n",
      "step 743 : loss 1.3101248741149902\n",
      "step 744 : loss 1.3175320625305176\n",
      "step 745 : loss 1.313888430595398\n",
      "step 746 : loss 1.320000410079956\n",
      "step 747 : loss 1.3119441270828247\n",
      "step 748 : loss 1.3154687881469727\n",
      "step 749 : loss 1.3054512739181519\n",
      "step 750 : loss 1.3081247806549072\n",
      "step 751 : loss 1.299921989440918\n",
      "step 752 : loss 1.3036253452301025\n",
      "step 753 : loss 1.2967628240585327\n",
      "step 754 : loss 1.299562931060791\n",
      "step 755 : loss 1.2916826009750366\n",
      "step 756 : loss 1.293249487876892\n",
      "step 757 : loss 1.285954475402832\n",
      "step 758 : loss 1.2893716096878052\n",
      "step 759 : loss 1.2842222452163696\n",
      "step 760 : loss 1.2902946472167969\n",
      "step 761 : loss 1.2860108613967896\n",
      "step 762 : loss 1.2937883138656616\n",
      "step 763 : loss 1.2895911931991577\n",
      "step 764 : loss 1.2976090908050537\n",
      "step 765 : loss 1.2917572259902954\n",
      "step 766 : loss 1.2958141565322876\n",
      "step 767 : loss 1.2869136333465576\n",
      "step 768 : loss 1.2862718105316162\n",
      "step 769 : loss 1.2777832746505737\n",
      "step 770 : loss 1.2767726182937622\n",
      "step 771 : loss 1.2712366580963135\n",
      "step 772 : loss 1.2713706493377686\n",
      "step 773 : loss 1.2672817707061768\n",
      "step 774 : loss 1.2676112651824951\n",
      "step 775 : loss 1.2637032270431519\n",
      "step 776 : loss 1.2649919986724854\n",
      "step 777 : loss 1.262200117111206\n",
      "step 778 : loss 1.2665538787841797\n",
      "step 779 : loss 1.265239953994751\n",
      "step 780 : loss 1.2732925415039062\n",
      "step 781 : loss 1.2718969583511353\n",
      "step 782 : loss 1.28249990940094\n",
      "step 783 : loss 1.2784806489944458\n",
      "step 784 : loss 1.2875019311904907\n",
      "step 785 : loss 1.278181552886963\n",
      "step 786 : loss 1.2806228399276733\n",
      "step 787 : loss 1.267802119255066\n",
      "step 788 : loss 1.2658792734146118\n",
      "step 789 : loss 1.2551430463790894\n",
      "step 790 : loss 1.2544270753860474\n",
      "step 791 : loss 1.247873067855835\n",
      "step 792 : loss 1.2500816583633423\n",
      "step 793 : loss 1.2459591627120972\n",
      "step 794 : loss 1.2502491474151611\n",
      "step 795 : loss 1.2467575073242188\n",
      "step 796 : loss 1.2536342144012451\n",
      "step 797 : loss 1.2510383129119873\n",
      "step 798 : loss 1.2611662149429321\n",
      "step 799 : loss 1.2573106288909912\n",
      "step 800 : loss 1.2653950452804565\n",
      "step 801 : loss 1.255739688873291\n",
      "step 802 : loss 1.25663161277771\n",
      "step 803 : loss 1.2449743747711182\n",
      "step 804 : loss 1.2434442043304443\n",
      "step 805 : loss 1.2352941036224365\n",
      "step 806 : loss 1.2355968952178955\n",
      "step 807 : loss 1.2308100461959839\n",
      "step 808 : loss 1.233185887336731\n",
      "step 809 : loss 1.2298164367675781\n",
      "step 810 : loss 1.2342274188995361\n",
      "step 811 : loss 1.2312837839126587\n",
      "step 812 : loss 1.2379581928253174\n",
      "step 813 : loss 1.2341331243515015\n",
      "step 814 : loss 1.241377830505371\n",
      "step 815 : loss 1.2347291707992554\n",
      "step 816 : loss 1.2398735284805298\n",
      "step 817 : loss 1.2306973934173584\n",
      "step 818 : loss 1.2333545684814453\n",
      "step 819 : loss 1.2239903211593628\n",
      "step 820 : loss 1.225592851638794\n",
      "step 821 : loss 1.2173724174499512\n",
      "step 822 : loss 1.218953013420105\n",
      "step 823 : loss 1.2120158672332764\n",
      "step 824 : loss 1.2141802310943604\n",
      "step 825 : loss 1.2083182334899902\n",
      "step 826 : loss 1.2115274667739868\n",
      "step 827 : loss 1.2064152956008911\n",
      "step 828 : loss 1.2109659910202026\n",
      "step 829 : loss 1.206435203552246\n",
      "step 830 : loss 1.2127736806869507\n",
      "step 831 : loss 1.208838701248169\n",
      "step 832 : loss 1.2167011499404907\n",
      "step 833 : loss 1.2124214172363281\n",
      "step 834 : loss 1.2194080352783203\n",
      "step 835 : loss 1.2133909463882446\n",
      "step 836 : loss 1.2167456150054932\n",
      "step 837 : loss 1.2096223831176758\n",
      "step 838 : loss 1.2095016241073608\n",
      "step 839 : loss 1.2028110027313232\n",
      "step 840 : loss 1.2009351253509521\n",
      "step 841 : loss 1.1956058740615845\n",
      "step 842 : loss 1.1938376426696777\n",
      "step 843 : loss 1.1901881694793701\n",
      "step 844 : loss 1.1894747018814087\n",
      "step 845 : loss 1.1869317293167114\n",
      "step 846 : loss 1.1874815225601196\n",
      "step 847 : loss 1.1856063604354858\n",
      "step 848 : loss 1.1884658336639404\n",
      "step 849 : loss 1.187961459159851\n",
      "step 850 : loss 1.1953215599060059\n",
      "step 851 : loss 1.196368932723999\n",
      "step 852 : loss 1.20823335647583\n",
      "step 853 : loss 1.2061784267425537\n",
      "step 854 : loss 1.2152491807937622\n",
      "step 855 : loss 1.204409122467041\n",
      "step 856 : loss 1.205016851425171\n",
      "step 857 : loss 1.1912317276000977\n",
      "step 858 : loss 1.189467191696167\n",
      "step 859 : loss 1.1797916889190674\n",
      "step 860 : loss 1.1807904243469238\n",
      "step 861 : loss 1.1755049228668213\n",
      "step 862 : loss 1.1795786619186401\n",
      "step 863 : loss 1.1761854887008667\n",
      "step 864 : loss 1.1823025941848755\n",
      "step 865 : loss 1.178896427154541\n",
      "step 866 : loss 1.186390995979309\n",
      "step 867 : loss 1.181525707244873\n",
      "step 868 : loss 1.1882129907608032\n",
      "step 869 : loss 1.180506706237793\n",
      "step 870 : loss 1.183901309967041\n",
      "step 871 : loss 1.174903392791748\n",
      "step 872 : loss 1.1758345365524292\n",
      "step 873 : loss 1.1679840087890625\n",
      "step 874 : loss 1.1682924032211304\n",
      "step 875 : loss 1.1622140407562256\n",
      "step 876 : loss 1.1632338762283325\n",
      "step 877 : loss 1.1587563753128052\n",
      "step 878 : loss 1.1613088846206665\n",
      "step 879 : loss 1.1577028036117554\n",
      "step 880 : loss 1.1617920398712158\n",
      "step 881 : loss 1.1580866575241089\n",
      "step 882 : loss 1.1636604070663452\n",
      "step 883 : loss 1.1596369743347168\n",
      "step 884 : loss 1.1663810014724731\n",
      "step 885 : loss 1.1610630750656128\n",
      "step 886 : loss 1.1665831804275513\n",
      "step 887 : loss 1.1587938070297241\n",
      "step 888 : loss 1.1615418195724487\n",
      "step 889 : loss 1.1530436277389526\n",
      "step 890 : loss 1.154749870300293\n",
      "step 891 : loss 1.1476588249206543\n",
      "step 892 : loss 1.1495362520217896\n",
      "step 893 : loss 1.14350163936615\n",
      "step 894 : loss 1.1450873613357544\n",
      "step 895 : loss 1.1393468379974365\n",
      "step 896 : loss 1.1407665014266968\n",
      "step 897 : loss 1.1355204582214355\n",
      "step 898 : loss 1.1374891996383667\n",
      "step 899 : loss 1.1327906847000122\n",
      "step 900 : loss 1.1355799436569214\n",
      "step 901 : loss 1.1313163042068481\n",
      "step 902 : loss 1.1356251239776611\n",
      "step 903 : loss 1.1325303316116333\n",
      "step 904 : loss 1.1398470401763916\n",
      "step 905 : loss 1.1383012533187866\n",
      "step 906 : loss 1.1482914686203003\n",
      "step 907 : loss 1.1459014415740967\n",
      "step 908 : loss 1.153915524482727\n",
      "step 909 : loss 1.1478794813156128\n",
      "step 910 : loss 1.1497763395309448\n",
      "step 911 : loss 1.1416453123092651\n",
      "step 912 : loss 1.1389625072479248\n",
      "step 913 : loss 1.1316852569580078\n",
      "step 914 : loss 1.128378987312317\n",
      "step 915 : loss 1.1239672899246216\n",
      "step 916 : loss 1.1230101585388184\n",
      "step 917 : loss 1.1215004920959473\n",
      "step 918 : loss 1.123038649559021\n",
      "step 919 : loss 1.1224113702774048\n",
      "step 920 : loss 1.1258715391159058\n",
      "step 921 : loss 1.1254124641418457\n",
      "step 922 : loss 1.1323072910308838\n",
      "step 923 : loss 1.131706714630127\n",
      "step 924 : loss 1.1405147314071655\n",
      "step 925 : loss 1.1348835229873657\n",
      "step 926 : loss 1.1390798091888428\n",
      "step 927 : loss 1.1276684999465942\n",
      "step 928 : loss 1.1278085708618164\n",
      "step 929 : loss 1.1174598932266235\n",
      "step 930 : loss 1.1187636852264404\n",
      "step 931 : loss 1.1119611263275146\n",
      "step 932 : loss 1.115293025970459\n",
      "step 933 : loss 1.1101329326629639\n",
      "step 934 : loss 1.1143344640731812\n",
      "step 935 : loss 1.1093769073486328\n",
      "step 936 : loss 1.1143381595611572\n",
      "step 937 : loss 1.1094670295715332\n",
      "step 938 : loss 1.1150728464126587\n",
      "step 939 : loss 1.109730839729309\n",
      "step 940 : loss 1.1148818731307983\n",
      "step 941 : loss 1.108994722366333\n",
      "step 942 : loss 1.1130503416061401\n",
      "step 943 : loss 1.1073077917099\n",
      "step 944 : loss 1.1097394227981567\n",
      "step 945 : loss 1.1040072441101074\n",
      "step 946 : loss 1.104371190071106\n",
      "step 947 : loss 1.098893404006958\n",
      "step 948 : loss 1.0984179973602295\n",
      "step 949 : loss 1.0941952466964722\n",
      "step 950 : loss 1.094228982925415\n",
      "step 951 : loss 1.0910576581954956\n",
      "step 952 : loss 1.0914688110351562\n",
      "step 953 : loss 1.0885449647903442\n",
      "step 954 : loss 1.0895612239837646\n",
      "step 955 : loss 1.087124228477478\n",
      "step 956 : loss 1.0898029804229736\n",
      "step 957 : loss 1.0880630016326904\n",
      "step 958 : loss 1.0929663181304932\n",
      "step 959 : loss 1.0914500951766968\n",
      "step 960 : loss 1.0988438129425049\n",
      "step 961 : loss 1.096864938735962\n",
      "step 962 : loss 1.1057558059692383\n",
      "step 963 : loss 1.101304531097412\n",
      "step 964 : loss 1.1074228286743164\n",
      "step 965 : loss 1.0988811254501343\n",
      "step 966 : loss 1.0999749898910522\n",
      "step 967 : loss 1.0904531478881836\n",
      "step 968 : loss 1.0898200273513794\n",
      "step 969 : loss 1.0825077295303345\n",
      "step 970 : loss 1.0827746391296387\n",
      "step 971 : loss 1.0776766538619995\n",
      "step 972 : loss 1.079161286354065\n",
      "step 973 : loss 1.0751901865005493\n",
      "step 974 : loss 1.0779163837432861\n",
      "step 975 : loss 1.0745923519134521\n",
      "step 976 : loss 1.0792258977890015\n",
      "step 977 : loss 1.0766102075576782\n",
      "step 978 : loss 1.0835716724395752\n",
      "step 979 : loss 1.0806903839111328\n",
      "step 980 : loss 1.0880794525146484\n",
      "step 981 : loss 1.0824146270751953\n",
      "step 982 : loss 1.0864572525024414\n",
      "step 983 : loss 1.0779982805252075\n",
      "step 984 : loss 1.0787608623504639\n",
      "step 985 : loss 1.0710490942001343\n",
      "step 986 : loss 1.0713984966278076\n",
      "step 987 : loss 1.0658788681030273\n",
      "step 988 : loss 1.0668683052062988\n",
      "step 989 : loss 1.0627937316894531\n",
      "step 990 : loss 1.0645233392715454\n",
      "step 991 : loss 1.061131238937378\n",
      "step 992 : loss 1.0639232397079468\n",
      "step 993 : loss 1.0608363151550293\n",
      "step 994 : loss 1.0649502277374268\n",
      "step 995 : loss 1.061606526374817\n",
      "step 996 : loss 1.0667344331741333\n",
      "step 997 : loss 1.0625447034835815\n",
      "step 998 : loss 1.0679347515106201\n",
      "step 999 : loss 1.0625381469726562\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAJiklEQVR4Ae1dy3YbNwydl1yni5zmP/r/H5JFd91305yebpLU1kwJXNC6Y4pjKhJFS8HkhEOBIAncC5CciR31n7996rru98cvoey6Ifydeyn7RT733R43qfdStUtbpb3rlvBHb9YUb6weZfdxh2eAIFOnxmERPLtFRF+evofyw/evofz86WMotS3c/WqEQD8pM7/+J/ws43Mo/5keQznOYtHYyQ0USlu4lPJFe4FmE6FRRciV2KoNN15wnOdcWdTtXpEZgYOqjhrlYy9I7qcplA97qf+1k9XFMyCA0PIKS72wMWt475S9p192QbJoqPeztIL/WbcF7A2IbshT83PyVPN9StLczWUA4zCrEjSHRe8qwj7aD4LkOD6Iy3uJ/aed1D0DBJCGVz8riU9qwoOuU90g7O2VPRA5LMIe2ALD+jFkxiECLGr01muaIIegeX8lcAAux7wTIGZdO5ANo2IyqGRW3HZ62vQMOIbeFWUTgnjsxzDpEE49dmDF+T/wpUzzI4DGeIxuzgexGhkTnw9Ecn8XfDZPLfHFS0gMKmwI6jyq+jzV4cyJTpMeND0DGkdIWPWVj1mYwDqFdc34JPOIbFr7owK3vs6LqJPeuVfaWk/CFsIGlmDenBz5jf2P7Y/6cu9xttT1Y6GTzgS57rJPk8zpGVCP5aKR5cEsXMsgvD1rZGPvDrIg6Xn1F0W80tDaukgjaN3++hPi5bX0Nj6r7XA4ccN2zQVrvvhjZ0IN9VnFWF0mffbyDGjMOR5sdWVCzCtnwptmwKAZgPedUEV+gP74HCB9klAQ4U1f8CiX2ZwAXEdEoy+vJSk+8r7B94DmITKlzHA0I/ahE0u5o56LjuZeXdAAeMoDpr5DkpbAyeQKli04Ohzkvgcwtg3qdgrCzBzRtr7bA5+080qHFOAjEvhMx0nlLIH+ey4ZE7aT0Uh1ENd4osK5CP+C0tvjlWCAXp4BjGqDur0LwnnfTjtmhjCUcmuStEF7pWJIbiXqT7U29Rfgwd/odbyHPUE3AZ7FM8DCrdVtikF+YIlNOS6NGmjlKGB95hk9uDWO8b7up1qYIpDzJ4eGZ0AOsSvJs6cgnp955jrreB0IcA4xVqkc+p4BjSNnlQGwJV2t2EZb8ZVQfg4wnZxcm7dH5llupc5orB6UrEH80B81sZ8r5Dp89AxozLVlwHZsonWlQwyvPMjJV0q39GF7HTdM9GaaKQIs4brC4BnQOBosA8AeSljE9ZyNOR3IE7Jv+N8Mcp7mkIEcCHDfFBPPgG0Mq7ceOQVdak5m/lJjvrdxtn3cboUvngGNOV1lAK9Q6fp1qqXpCKnk1DFb6ZdYDh22sKSXZwAj1qDuBJwFeojxNPBPGtEJOAmuyyuv9oB0+JJVLO3180hKzjnbaHgGbONTvfWNDKg+f6MJthdujuttzfPN9ww4H8OzRtCfUY8/6ZaOxLGQtt6uhP3a3uegWZIH5ZqMm2cAo9GgbnvAdhQ0sOuKU3I2XHbakpE9Ay6L+cmjWQakXKWSk8f2DgUIeAYUgFRTxQmoiW7B2E5AAUg1VZyAmugWjO0EFIBUU8UJqIluwdhOQAFINVWcgJroFoztBBSAVFPFCaiJbsHYTkABSDVVnICa6BaM7QQUgFRTxQmoiW7B2E5AAUg1VZyAmugWjO0EFIBUU8UJqIluwdhOQAFINVWcgJroFoztBBSAVFPFCaiJbsHYTkABSDVVnICa6BaM7QQUgFRTxQmoiW7B2E5AAUg1VZyAmugWjO0EFIBUU8UJqIluwdhOQAFINVWcgJroFoztBBSAVFPFCaiJbsHYTkABSDVVsr8p/zP/3uSPAZ7+LnEJhp4BP4b2xXo5AWdBGWI8DfyTRnQCToLr8srZPeD8qdLfNE4l589ynRFylrMcdU4Ibs3Z6RmQQ+ZK8ooZUHIGuJKXZ09T4gvHPiYs6eUZcDY55w1w9Jv0ThuyhOfyiDht7tba5b7nLPUMyCFzJflqD+B9vGQHh43lmlfy6YrTnO+7Z8AV6To21SoDWKFkdYMOep0fCzz7z1P3DGjM9Rv/ayLHeElOwBvu1di/gunL/UoHO6cvRvMMSFG9qiT7HHDOmp7rm5Nf1eNkshKrcjo5OSbZbkX2eAYkhFxXYHuATUpLGqpsDDUeeQmOb9Wzb4rjbkl9Oy4S9SqCbV/YQsYBcpawcdwL8pwm9/IMYDQa1PUbnuP/Hb3NMPiM35P+2taU/9ca9/U5je4ceiznXpB7BjSOC8sAWHEsuoUz8NbrN+FBB/opn6k395EZ7Gnqo31F4BtKh36s6BlwwKVJzZ4DOE65HncHsQ1yzhJImE+WrMdp4t3FJl35Qk7Cd27FkrIwKGRFKvYMIHhaVMPb0AOLxpsS2tNZf1FaV9sFbD10tWFYJ2lc+cdRs2q4+ge2M50crSt5Ilp9s7i2JiovA6BFvEfNM+AFmjaVkAHCBuJx0QfZXhPBoh5W4QGXcsKM5TDmujazgOvW993ctm1D6yqiVcSr/CGqY1zbmLQhLIrxal1RJc+AxoEQ3gUd+LO6CiBF2ekiNyhjs4kOdkNgnNvt0HqHNXU4gSFdH4LrpLXaKBQVoHqHAN2US1On65QF7qx3ZYbj2pYydcy2A3ISfW0E3Ih4k5P+LVbJobhhrkTkkzrM4Q5FQAu9WfeDnS4mvgcQdi2qU69L+6Kr1aLExbc9QmXP2aDcYg/g876ZDaIzcZGK22ZGag+8SK1iTavrLacZT0fQFS1ga4mDE6ZONi+CtGcAkG9W9qNODb6eOuFkDxKNQr3RJoBGnIhwOsJb0hgRoo+u8CnK8UlKG/ggaFZjO2FEai3k0DTLVSk+J4kMvdY4SD+cKtE6K7a7WSJ+VtCxm3oGCFINr/VPRSgpQy+sGKu6B9gqpgEQ5WIz6ohp2/ej6MUlRM3Lx5dKTv6i0KRSZJUqIbpZPz5RqeHQwQ5qB0dBddhJ6/CgzXv5sUTPAEGk4TV907VpGIWa/V4WJ3ACbrHi2+OBBf/BWuiYpoq5ftC7z5rAYadH8h3Lwqy7Jt6qDcgAhfK77gHLJHvtb0/PofQMUPDaFYEk4eFx+RjK5/5bKPFO1FZ4PSXt9cQ6K5N2CmpncfOZKaBX571Bo95akQi6IIyLRPmiwA39PtS/7iR7PujC4hnQmNDpj0fJgL+/Suw/LFJ/6GR37vfC0jBK2SutKrDECEK5lGHRiBf2gPjp7u7kKh6NLH7V7QGLgzqNPAA+gmaASm84YQ764c/53yD3DFB42hX/AzWDLdN89MoDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Autoencoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(init_tensor)\n",
    "    loss = (y-init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "result = autoencoder(init_tensor)\n",
    "tensor_to_image(result.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(init_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "\n",
    "def generate_image(size): # , num_images):\n",
    "    # for i in range(num_images):\n",
    "    # Create a new image with a random background color\n",
    "    img = Image.new(\"RGB\", size, color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "    # Get a drawing context\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Choose a random shape (circle, square, or triangle)\n",
    "    # shape = random.choice([\"circle\", \"square\", \"triangle\"])\n",
    "    shape = \"circle\"\n",
    "\n",
    "    # Choose a random position\n",
    "    position = (random.randint(20, size[0]-20), random.randint(20, size[1]-20))\n",
    "\n",
    "    # Choose a random color for the shape\n",
    "    shape_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "    # Draw the shape on the image\n",
    "    if shape == \"circle\":\n",
    "        draw.ellipse([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "    elif shape == \"square\":\n",
    "        draw.rectangle([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "    elif shape == \"triangle\":\n",
    "        draw.polygon([(position[0], position[1]-20), (position[0]-20, position[1]+20), (position[0]+20, position[1]+20)], fill=shape_color)\n",
    "\n",
    "    # Show the image\n",
    "    # img.show()\n",
    "    # print(type(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDnKKKK4D5MKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK0dG0a51q8EEA2ovMkpHCD+p9B/9c1RhhkuJ44Il3SSMEUZxkk4FevaXp0OlafFawquFHzsBje3dj9f/AK1VGNyZSsZmneENKsVRpIvtUw5LzcjOMH5emPrn61oPoelSRsh020AYEHbCoP4EDIq/RW1kY3Zx2r+BoXjebS3aOQDIgc5VunAJ5Hfrnk9q4V0eORo5FZXUkMrDBBHYiva647xzpCPbLqkKKJEIWcjjcp4BPPY4Hrz7VE4dUaQn0ZwdFFFZGgUUUUAFFFFABRRRQAUUUUAbXhNEk8T2QdVYAscEZ5CEg/gQDXqdePaVenTtVtrsFgI3BbaASV6MBn1BNevo6SRrJGysjAFWU5BB7g1rT2Mqm46iiitCAqhriJJoOoB1VgLdzgjPIUkH8CAav1geL9RFjoUsavtmuf3SAYJx/Fwe2MjPuKT2BbnmNFFFc50BRRRQAUUUUAFFFFABRRRQAV2HhbxT9m2afqEn7j7sUzH/AFf+yf8AZ9D2+nTj6KadhNXPa0dJI1kjZWRgCrKcgg9wadXj1lqt/pxBtLqWIAk7AcqSRjJU8H8qvP4s1ySNkN+wDAg7Y0B/AgZFae0Rn7Nno+o6paaVbGa6lVRglUz8z47KO/Uf1ry/WdZudavDPOdqLxHEDwg/qfU//WFUZppbiVpZ5Xlkbq7sWJ/E0yolK5cY2CiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAC9klEQVR4Ae2cwU3DQBQFAVEE1dAKLVACJ860QKcYIjlI1oN4d/2e9XdyQNEn9jgz/hEH4P7p/fmOR87AQw4N+dsAAcL3AQEIEDYQxrMBBAgbCOPZAAKEDYTxbAABwgbCeDaAAGEDYTwbQICwgTCeDSBA2EAYzwYQIGwgjGcDCBA2EMazAQQIGwjj2QAChA2E8WwAAcIGwng2gABhA2E8G0CAsIEwng0gQNhAGM8GECBsIIxnAwgQNhDGswEECBsI49kAAoQNhPFsAAHCBsJ4NoAAYQNhPBtAgLCBMJ4NIEDYQBj/GOafAP/y+qau4vNDfksdsnd+P+1/zPrD+1bicSVmDLBL/e8YR2SYK0Cz+uMyTPRT0BD7S4lR57lEnSXAWGsDzzZFgIG+1s+iUeesH2CUqVX9+mTImYsHGOJoNb590n/+ygH67WyNbyedlMoBtrJOOCkboPPG3JWqh1U2wC6DwRfXDNBzS7bFaCbWDNAmMXIUASLar9CCAZo/Da5Wmp61cQsGaLIXO4gAMfUXMAEIEDYQxrMBBAgbCOPZAAKEDYTxBTfgiF8euaVSG7dggFtknec1BAi3qBmg7dOgJ0UzsWaAHpXmY8sGaL4lGwL0sMoGaPAYOaRygJ4b8/YYnZTKARaJnXb+zdB//uIBDm3Qb3+5vPoBDmowxP4sAYY3GGV/ogADGwy0v1zVXH+itLzh5dH26wvLgWPV/1zLlAEu73xXhiPUzx7g8v6Xr3+UOM77Sp/xI2h982d4MsWPoWcQra6BAMqMaU4Ak2iFIYAyY5oTwCRaYQigzJjmBDCJVhgCKDOmOQFMohWGAMqMaU4Ak2iFIYAyY5oTwCRaYQigzJjmBDCJVhgCKDOmOQFMohWGAMqMaU4Ak2iFIYAyY5oTwCRaYQigzJjmBDCJVhgCKDOmOQFMohWGAMqMaU4Ak2iFIYAyY5oTwCRaYQigzJjmBDCJVhgCKDOmOQFMohWGAMqMaU4Ak2iFIYAyY5oTwCRaYb4AxqJS25rveFkAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size = (128, 128)\n",
    "num_images = 5  # Change this to the number of images you want to generate\n",
    "\n",
    "new_image = generate_image(image_size)\n",
    "new_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 83.34357452392578\n",
      "step 1 : loss 82.53295135498047\n",
      "step 2 : loss 81.6451187133789\n",
      "step 3 : loss 80.62083435058594\n",
      "step 4 : loss 79.39480590820312\n",
      "step 5 : loss 77.8953628540039\n",
      "step 6 : loss 76.04495239257812\n",
      "step 7 : loss 73.77063751220703\n",
      "step 8 : loss 71.00028991699219\n",
      "step 9 : loss 67.67150115966797\n",
      "step 10 : loss 63.763824462890625\n",
      "step 11 : loss 59.359371185302734\n",
      "step 12 : loss 54.781227111816406\n",
      "step 13 : loss 50.88703536987305\n",
      "step 14 : loss 49.23735809326172\n",
      "step 15 : loss 50.02930450439453\n",
      "step 16 : loss 50.100303649902344\n",
      "step 17 : loss 48.01298904418945\n",
      "step 18 : loss 44.690284729003906\n",
      "step 19 : loss 41.461368560791016\n",
      "step 20 : loss 39.198734283447266\n",
      "step 21 : loss 38.00797653198242\n",
      "step 22 : loss 37.43558120727539\n",
      "step 23 : loss 36.97111511230469\n",
      "step 24 : loss 36.3299560546875\n",
      "step 25 : loss 35.461849212646484\n",
      "step 26 : loss 34.484989166259766\n",
      "step 27 : loss 33.62495422363281\n",
      "step 28 : loss 33.12342834472656\n",
      "step 29 : loss 33.0842399597168\n",
      "step 30 : loss 33.32954025268555\n",
      "step 31 : loss 33.48836898803711\n",
      "step 32 : loss 33.31997299194336\n",
      "step 33 : loss 32.886287689208984\n",
      "step 34 : loss 32.42291259765625\n",
      "step 35 : loss 32.11537551879883\n",
      "step 36 : loss 31.983041763305664\n",
      "step 37 : loss 31.919971466064453\n",
      "step 38 : loss 31.802175521850586\n",
      "step 39 : loss 31.561914443969727\n",
      "step 40 : loss 31.206666946411133\n",
      "step 41 : loss 30.80586814880371\n",
      "step 42 : loss 30.459331512451172\n",
      "step 43 : loss 30.252784729003906\n",
      "step 44 : loss 30.207927703857422\n",
      "step 45 : loss 30.26093101501465\n",
      "step 46 : loss 30.29997444152832\n",
      "step 47 : loss 30.243513107299805\n",
      "step 48 : loss 30.089447021484375\n",
      "step 49 : loss 29.896848678588867\n",
      "step 50 : loss 29.729618072509766\n",
      "step 51 : loss 29.613582611083984\n",
      "step 52 : loss 29.532432556152344\n",
      "step 53 : loss 29.452903747558594\n",
      "step 54 : loss 29.352476119995117\n",
      "step 55 : loss 29.232440948486328\n",
      "step 56 : loss 29.11271858215332\n",
      "step 57 : loss 29.01457405090332\n",
      "step 58 : loss 28.943077087402344\n",
      "step 59 : loss 28.882539749145508\n",
      "step 60 : loss 28.810819625854492\n",
      "step 61 : loss 28.719640731811523\n",
      "step 62 : loss 28.62155532836914\n",
      "step 63 : loss 28.5363712310791\n",
      "step 64 : loss 28.47031593322754\n",
      "step 65 : loss 28.40825080871582\n",
      "step 66 : loss 28.326208114624023\n",
      "step 67 : loss 28.211254119873047\n",
      "step 68 : loss 28.07073402404785\n",
      "step 69 : loss 27.923839569091797\n",
      "step 70 : loss 27.781932830810547\n",
      "step 71 : loss 27.634143829345703\n",
      "step 72 : loss 27.45286750793457\n",
      "step 73 : loss 27.215417861938477\n",
      "step 74 : loss 26.918458938598633\n",
      "step 75 : loss 26.569307327270508\n",
      "step 76 : loss 26.166179656982422\n",
      "step 77 : loss 25.69800567626953\n",
      "step 78 : loss 25.16464614868164\n",
      "step 79 : loss 24.562286376953125\n",
      "step 80 : loss 23.850378036499023\n",
      "step 81 : loss 23.106721878051758\n",
      "step 82 : loss 22.457332611083984\n",
      "step 83 : loss 21.895587921142578\n",
      "step 84 : loss 21.466365814208984\n",
      "step 85 : loss 21.118515014648438\n",
      "step 86 : loss 20.780981063842773\n",
      "step 87 : loss 20.44090461730957\n",
      "step 88 : loss 20.07487678527832\n",
      "step 89 : loss 19.64530372619629\n",
      "step 90 : loss 19.17889404296875\n",
      "step 91 : loss 18.775630950927734\n",
      "step 92 : loss 18.48146629333496\n",
      "step 93 : loss 18.273780822753906\n",
      "step 94 : loss 18.091346740722656\n",
      "step 95 : loss 17.926904678344727\n",
      "step 96 : loss 17.75922203063965\n",
      "step 97 : loss 17.539945602416992\n",
      "step 98 : loss 17.266286849975586\n",
      "step 99 : loss 16.972187042236328\n",
      "step 100 : loss 16.667978286743164\n",
      "step 101 : loss 16.396852493286133\n",
      "step 102 : loss 16.18494415283203\n",
      "step 103 : loss 16.0095272064209\n",
      "step 104 : loss 15.855692863464355\n",
      "step 105 : loss 15.702821731567383\n",
      "step 106 : loss 15.553167343139648\n",
      "step 107 : loss 15.397661209106445\n",
      "step 108 : loss 15.248250007629395\n",
      "step 109 : loss 15.097899436950684\n",
      "step 110 : loss 14.952743530273438\n",
      "step 111 : loss 14.810165405273438\n",
      "step 112 : loss 14.674224853515625\n",
      "step 113 : loss 14.560422897338867\n",
      "step 114 : loss 14.473822593688965\n",
      "step 115 : loss 14.414910316467285\n",
      "step 116 : loss 14.345856666564941\n",
      "step 117 : loss 14.288716316223145\n",
      "step 118 : loss 14.188895225524902\n",
      "step 119 : loss 14.09097671508789\n",
      "step 120 : loss 13.978216171264648\n",
      "step 121 : loss 13.887605667114258\n",
      "step 122 : loss 13.819336891174316\n",
      "step 123 : loss 13.759382247924805\n",
      "step 124 : loss 13.707158088684082\n",
      "step 125 : loss 13.637896537780762\n",
      "step 126 : loss 13.568150520324707\n",
      "step 127 : loss 13.479024887084961\n",
      "step 128 : loss 13.390442848205566\n",
      "step 129 : loss 13.304757118225098\n",
      "step 130 : loss 13.230063438415527\n",
      "step 131 : loss 13.164346694946289\n",
      "step 132 : loss 13.101143836975098\n",
      "step 133 : loss 13.042518615722656\n",
      "step 134 : loss 13.006001472473145\n",
      "step 135 : loss 13.05137825012207\n",
      "step 136 : loss 13.19961929321289\n",
      "step 137 : loss 13.487944602966309\n",
      "step 138 : loss 12.933676719665527\n",
      "step 139 : loss 12.68087387084961\n",
      "step 140 : loss 12.8942232131958\n",
      "step 141 : loss 12.823543548583984\n",
      "step 142 : loss 12.5613374710083\n",
      "step 143 : loss 12.508358001708984\n",
      "step 144 : loss 12.617959976196289\n",
      "step 145 : loss 12.54850959777832\n",
      "step 146 : loss 12.329874038696289\n",
      "step 147 : loss 12.377945899963379\n",
      "step 148 : loss 12.453391075134277\n",
      "step 149 : loss 12.234424591064453\n",
      "step 150 : loss 12.163055419921875\n",
      "step 151 : loss 12.244938850402832\n",
      "step 152 : loss 12.138762474060059\n",
      "step 153 : loss 12.007682800292969\n",
      "step 154 : loss 12.007972717285156\n",
      "step 155 : loss 12.0139741897583\n",
      "step 156 : loss 11.934435844421387\n",
      "step 157 : loss 11.831345558166504\n",
      "step 158 : loss 11.813448905944824\n",
      "step 159 : loss 11.826499938964844\n",
      "step 160 : loss 11.766885757446289\n",
      "step 161 : loss 11.67879581451416\n",
      "step 162 : loss 11.613717079162598\n",
      "step 163 : loss 11.594684600830078\n",
      "step 164 : loss 11.59228229522705\n",
      "step 165 : loss 11.559020042419434\n",
      "step 166 : loss 11.50813102722168\n",
      "step 167 : loss 11.435446739196777\n",
      "step 168 : loss 11.371755599975586\n",
      "step 169 : loss 11.320274353027344\n",
      "step 170 : loss 11.28111743927002\n",
      "step 171 : loss 11.252086639404297\n",
      "step 172 : loss 11.234710693359375\n",
      "step 173 : loss 11.247681617736816\n",
      "step 174 : loss 11.298641204833984\n",
      "step 175 : loss 11.46639347076416\n",
      "step 176 : loss 11.487627983093262\n",
      "step 177 : loss 11.469045639038086\n",
      "step 178 : loss 11.094212532043457\n",
      "step 179 : loss 10.937950134277344\n",
      "step 180 : loss 11.030789375305176\n",
      "step 181 : loss 11.140968322753906\n",
      "step 182 : loss 11.136651992797852\n",
      "step 183 : loss 10.872623443603516\n",
      "step 184 : loss 10.773049354553223\n",
      "step 185 : loss 10.86593246459961\n",
      "step 186 : loss 10.906137466430664\n",
      "step 187 : loss 10.824584007263184\n",
      "step 188 : loss 10.657442092895508\n",
      "step 189 : loss 10.628504753112793\n",
      "step 190 : loss 10.699036598205566\n",
      "step 191 : loss 10.689347267150879\n",
      "step 192 : loss 10.609762191772461\n",
      "step 193 : loss 10.500213623046875\n",
      "step 194 : loss 10.463858604431152\n",
      "step 195 : loss 10.486861228942871\n",
      "step 196 : loss 10.497330665588379\n",
      "step 197 : loss 10.477965354919434\n",
      "step 198 : loss 10.392742156982422\n",
      "step 199 : loss 10.31754207611084\n",
      "step 200 : loss 10.273907661437988\n",
      "step 201 : loss 10.264769554138184\n",
      "step 202 : loss 10.27606201171875\n",
      "step 203 : loss 10.279834747314453\n",
      "step 204 : loss 10.289522171020508\n",
      "step 205 : loss 10.25861930847168\n",
      "step 206 : loss 10.237319946289062\n",
      "step 207 : loss 10.174540519714355\n",
      "step 208 : loss 10.121893882751465\n",
      "step 209 : loss 10.057662010192871\n",
      "step 210 : loss 10.007406234741211\n",
      "step 211 : loss 9.965866088867188\n",
      "step 212 : loss 9.932807922363281\n",
      "step 213 : loss 9.902978897094727\n",
      "step 214 : loss 9.873714447021484\n",
      "step 215 : loss 9.843535423278809\n",
      "step 216 : loss 9.812701225280762\n",
      "step 217 : loss 9.781889915466309\n",
      "step 218 : loss 9.752006530761719\n",
      "step 219 : loss 9.72486686706543\n",
      "step 220 : loss 9.706000328063965\n",
      "step 221 : loss 9.718557357788086\n",
      "step 222 : loss 9.831539154052734\n",
      "step 223 : loss 10.313862800598145\n",
      "step 224 : loss 10.726714134216309\n",
      "step 225 : loss 10.993477821350098\n",
      "step 226 : loss 9.582337379455566\n",
      "step 227 : loss 10.165828704833984\n",
      "step 228 : loss 11.15753173828125\n",
      "step 229 : loss 9.466541290283203\n",
      "step 230 : loss 11.001178741455078\n",
      "step 231 : loss 10.80096435546875\n",
      "step 232 : loss 9.875219345092773\n",
      "step 233 : loss 11.584320068359375\n",
      "step 234 : loss 9.446727752685547\n",
      "step 235 : loss 10.626041412353516\n",
      "step 236 : loss 9.407050132751465\n",
      "step 237 : loss 10.298002243041992\n",
      "step 238 : loss 9.664298057556152\n",
      "step 239 : loss 9.829581260681152\n",
      "step 240 : loss 9.656246185302734\n",
      "step 241 : loss 9.467206001281738\n",
      "step 242 : loss 9.704230308532715\n",
      "step 243 : loss 9.358981132507324\n",
      "step 244 : loss 9.697610855102539\n",
      "step 245 : loss 9.212577819824219\n",
      "step 246 : loss 9.572863578796387\n",
      "step 247 : loss 9.170426368713379\n",
      "step 248 : loss 9.515802383422852\n",
      "step 249 : loss 9.135990142822266\n",
      "step 250 : loss 9.368180274963379\n",
      "step 251 : loss 9.107099533081055\n",
      "step 252 : loss 9.266471862792969\n",
      "step 253 : loss 9.11419677734375\n",
      "step 254 : loss 9.155909538269043\n",
      "step 255 : loss 9.09572696685791\n",
      "step 256 : loss 9.056266784667969\n",
      "step 257 : loss 9.085776329040527\n",
      "step 258 : loss 8.98870849609375\n",
      "step 259 : loss 9.06108283996582\n",
      "step 260 : loss 8.934239387512207\n",
      "step 261 : loss 9.007012367248535\n",
      "step 262 : loss 8.904054641723633\n",
      "step 263 : loss 8.950296401977539\n",
      "step 264 : loss 8.891517639160156\n",
      "step 265 : loss 8.883907318115234\n",
      "step 266 : loss 8.876594543457031\n",
      "step 267 : loss 8.82610034942627\n",
      "step 268 : loss 8.849435806274414\n",
      "step 269 : loss 8.788955688476562\n",
      "step 270 : loss 8.809673309326172\n",
      "step 271 : loss 8.768667221069336\n",
      "step 272 : loss 8.757892608642578\n",
      "step 273 : loss 8.750615119934082\n",
      "step 274 : loss 8.713090896606445\n",
      "step 275 : loss 8.72046947479248\n",
      "step 276 : loss 8.684508323669434\n",
      "step 277 : loss 8.67929458618164\n",
      "step 278 : loss 8.664789199829102\n",
      "step 279 : loss 8.638588905334473\n",
      "step 280 : loss 8.63805866241455\n",
      "step 281 : loss 8.610307693481445\n",
      "step 282 : loss 8.599632263183594\n",
      "step 283 : loss 8.588296890258789\n",
      "step 284 : loss 8.564070701599121\n",
      "step 285 : loss 8.557762145996094\n",
      "step 286 : loss 8.539011001586914\n",
      "step 287 : loss 8.522058486938477\n",
      "step 288 : loss 8.513381004333496\n",
      "step 289 : loss 8.493257522583008\n",
      "step 290 : loss 8.480423927307129\n",
      "step 291 : loss 8.468728065490723\n",
      "step 292 : loss 8.449831008911133\n",
      "step 293 : loss 8.438232421875\n",
      "step 294 : loss 8.424942970275879\n",
      "step 295 : loss 8.407448768615723\n",
      "step 296 : loss 8.395768165588379\n",
      "step 297 : loss 8.382084846496582\n",
      "step 298 : loss 8.365532875061035\n",
      "step 299 : loss 8.3533296585083\n",
      "step 300 : loss 8.339930534362793\n",
      "step 301 : loss 8.323986053466797\n",
      "step 302 : loss 8.3110933303833\n",
      "step 303 : loss 8.298208236694336\n",
      "step 304 : loss 8.282883644104004\n",
      "step 305 : loss 8.269214630126953\n",
      "step 306 : loss 8.256657600402832\n",
      "step 307 : loss 8.242283821105957\n",
      "step 308 : loss 8.228048324584961\n",
      "step 309 : loss 8.215497970581055\n",
      "step 310 : loss 8.202861785888672\n",
      "step 311 : loss 8.190572738647461\n",
      "step 312 : loss 8.182600975036621\n",
      "step 313 : loss 8.185547828674316\n",
      "step 314 : loss 8.213105201721191\n",
      "step 315 : loss 8.286117553710938\n",
      "step 316 : loss 8.352388381958008\n",
      "step 317 : loss 8.326601028442383\n",
      "step 318 : loss 8.157401084899902\n",
      "step 319 : loss 8.100146293640137\n",
      "step 320 : loss 8.155263900756836\n",
      "step 321 : loss 8.174880027770996\n",
      "step 322 : loss 8.13365364074707\n",
      "step 323 : loss 8.074074745178223\n",
      "step 324 : loss 8.05506420135498\n",
      "step 325 : loss 8.059002876281738\n",
      "step 326 : loss 8.068507194519043\n",
      "step 327 : loss 8.057127952575684\n",
      "step 328 : loss 7.995185852050781\n",
      "step 329 : loss 7.96790075302124\n",
      "step 330 : loss 7.993077278137207\n",
      "step 331 : loss 7.99797248840332\n",
      "step 332 : loss 7.9647979736328125\n",
      "step 333 : loss 7.926281452178955\n",
      "step 334 : loss 7.914879322052002\n",
      "step 335 : loss 7.913679122924805\n",
      "step 336 : loss 7.908760070800781\n",
      "step 337 : loss 7.904265880584717\n",
      "step 338 : loss 7.8857197761535645\n",
      "step 339 : loss 7.857588291168213\n",
      "step 340 : loss 7.835439205169678\n",
      "step 341 : loss 7.829648017883301\n",
      "step 342 : loss 7.828835487365723\n",
      "step 343 : loss 7.821058750152588\n",
      "step 344 : loss 7.81168270111084\n",
      "step 345 : loss 7.801571369171143\n",
      "step 346 : loss 7.790117263793945\n",
      "step 347 : loss 7.771444797515869\n",
      "step 348 : loss 7.75374174118042\n",
      "step 349 : loss 7.739726543426514\n",
      "step 350 : loss 7.727985858917236\n",
      "step 351 : loss 7.714619159698486\n",
      "step 352 : loss 7.70079231262207\n",
      "step 353 : loss 7.688905239105225\n",
      "step 354 : loss 7.678867340087891\n",
      "step 355 : loss 7.668415069580078\n",
      "step 356 : loss 7.656837463378906\n",
      "step 357 : loss 7.645510673522949\n",
      "step 358 : loss 7.636417388916016\n",
      "step 359 : loss 7.630762100219727\n",
      "step 360 : loss 7.632375717163086\n",
      "step 361 : loss 7.650045871734619\n",
      "step 362 : loss 7.719576358795166\n",
      "step 363 : loss 7.862032413482666\n",
      "step 364 : loss 8.191462516784668\n",
      "step 365 : loss 8.138628005981445\n",
      "step 366 : loss 8.036174774169922\n",
      "step 367 : loss 7.656171798706055\n",
      "step 368 : loss 7.532529830932617\n",
      "step 369 : loss 7.6360979080200195\n",
      "step 370 : loss 7.801272869110107\n",
      "step 371 : loss 7.899441242218018\n",
      "step 372 : loss 7.645378589630127\n",
      "step 373 : loss 7.494168281555176\n",
      "step 374 : loss 7.527054309844971\n",
      "step 375 : loss 7.644033908843994\n",
      "step 376 : loss 7.705667018890381\n",
      "step 377 : loss 7.554403781890869\n",
      "step 378 : loss 7.449435234069824\n",
      "step 379 : loss 7.463031768798828\n",
      "step 380 : loss 7.535747528076172\n",
      "step 381 : loss 7.57028341293335\n",
      "step 382 : loss 7.480528831481934\n",
      "step 383 : loss 7.407021999359131\n",
      "step 384 : loss 7.401451587677002\n",
      "step 385 : loss 7.441713333129883\n",
      "step 386 : loss 7.471215724945068\n",
      "step 387 : loss 7.4294328689575195\n",
      "step 388 : loss 7.37627649307251\n",
      "step 389 : loss 7.347043514251709\n",
      "step 390 : loss 7.354608535766602\n",
      "step 391 : loss 7.377658367156982\n",
      "step 392 : loss 7.378388404846191\n",
      "step 393 : loss 7.359241962432861\n",
      "step 394 : loss 7.323165416717529\n",
      "step 395 : loss 7.297471523284912\n",
      "step 396 : loss 7.288854122161865\n",
      "step 397 : loss 7.293415069580078\n",
      "step 398 : loss 7.301751613616943\n",
      "step 399 : loss 7.300995826721191\n",
      "step 400 : loss 7.294192790985107\n",
      "step 401 : loss 7.2750067710876465\n",
      "step 402 : loss 7.255712509155273\n",
      "step 403 : loss 7.236956596374512\n",
      "step 404 : loss 7.2225422859191895\n",
      "step 405 : loss 7.212256908416748\n",
      "step 406 : loss 7.2052998542785645\n",
      "step 407 : loss 7.200782299041748\n",
      "step 408 : loss 7.198312759399414\n",
      "step 409 : loss 7.198782444000244\n",
      "step 410 : loss 7.201596736907959\n",
      "step 411 : loss 7.212888717651367\n",
      "step 412 : loss 7.227893352508545\n",
      "step 413 : loss 7.265194892883301\n",
      "step 414 : loss 7.296352386474609\n",
      "step 415 : loss 7.363819122314453\n",
      "step 416 : loss 7.357901096343994\n",
      "step 417 : loss 7.368995189666748\n",
      "step 418 : loss 7.275435447692871\n",
      "step 419 : loss 7.207142353057861\n",
      "step 420 : loss 7.137388229370117\n",
      "step 421 : loss 7.099195957183838\n",
      "step 422 : loss 7.084579944610596\n",
      "step 423 : loss 7.088310241699219\n",
      "step 424 : loss 7.1059160232543945\n",
      "step 425 : loss 7.127996921539307\n",
      "step 426 : loss 7.160523414611816\n",
      "step 427 : loss 7.169650077819824\n",
      "step 428 : loss 7.184047698974609\n",
      "step 429 : loss 7.1523847579956055\n",
      "step 430 : loss 7.1265459060668945\n",
      "step 431 : loss 7.0812201499938965\n",
      "step 432 : loss 7.048558235168457\n",
      "step 433 : loss 7.021932125091553\n",
      "step 434 : loss 7.005523681640625\n",
      "step 435 : loss 6.996389865875244\n",
      "step 436 : loss 6.992648124694824\n",
      "step 437 : loss 6.993186950683594\n",
      "step 438 : loss 6.997197151184082\n",
      "step 439 : loss 7.007701396942139\n",
      "step 440 : loss 7.0212907791137695\n",
      "step 441 : loss 7.050408840179443\n",
      "step 442 : loss 7.074131965637207\n",
      "step 443 : loss 7.123166084289551\n",
      "step 444 : loss 7.125443458557129\n",
      "step 445 : loss 7.144822597503662\n",
      "step 446 : loss 7.085268497467041\n",
      "step 447 : loss 7.0419158935546875\n",
      "step 448 : loss 6.978019714355469\n",
      "step 449 : loss 6.936988830566406\n",
      "step 450 : loss 6.910195827484131\n",
      "step 451 : loss 6.898514270782471\n",
      "step 452 : loss 6.897690773010254\n",
      "step 453 : loss 6.904163837432861\n",
      "step 454 : loss 6.917477607727051\n",
      "step 455 : loss 6.930872917175293\n",
      "step 456 : loss 6.953088283538818\n",
      "step 457 : loss 6.96239709854126\n",
      "step 458 : loss 6.982721328735352\n",
      "step 459 : loss 6.9724955558776855\n",
      "step 460 : loss 6.972229480743408\n",
      "step 461 : loss 6.9401984214782715\n",
      "step 462 : loss 6.918241024017334\n",
      "step 463 : loss 6.884073734283447\n",
      "step 464 : loss 6.859477519989014\n",
      "step 465 : loss 6.836982727050781\n",
      "step 466 : loss 6.821257591247559\n",
      "step 467 : loss 6.810212135314941\n",
      "step 468 : loss 6.803299427032471\n",
      "step 469 : loss 6.79964017868042\n",
      "step 470 : loss 6.798728942871094\n",
      "step 471 : loss 6.801106929779053\n",
      "step 472 : loss 6.806675434112549\n",
      "step 473 : loss 6.82032585144043\n",
      "step 474 : loss 6.838845252990723\n",
      "step 475 : loss 6.879011154174805\n",
      "step 476 : loss 6.914151191711426\n",
      "step 477 : loss 6.984909534454346\n",
      "step 478 : loss 6.984393119812012\n",
      "step 479 : loss 7.0040483474731445\n",
      "step 480 : loss 6.916231155395508\n",
      "step 481 : loss 6.855491638183594\n",
      "step 482 : loss 6.785334587097168\n",
      "step 483 : loss 6.74864387512207\n",
      "step 484 : loss 6.735548496246338\n",
      "step 485 : loss 6.743169784545898\n",
      "step 486 : loss 6.766904830932617\n",
      "step 487 : loss 6.78972053527832\n",
      "step 488 : loss 6.817836761474609\n",
      "step 489 : loss 6.810568332672119\n",
      "step 490 : loss 6.802058696746826\n",
      "step 491 : loss 6.762329578399658\n",
      "step 492 : loss 6.730406284332275\n",
      "step 493 : loss 6.6999077796936035\n",
      "step 494 : loss 6.681487083435059\n",
      "step 495 : loss 6.673035144805908\n",
      "step 496 : loss 6.672843933105469\n",
      "step 497 : loss 6.678796768188477\n",
      "step 498 : loss 6.6875786781311035\n",
      "step 499 : loss 6.701019763946533\n",
      "step 500 : loss 6.709987640380859\n",
      "step 501 : loss 6.724806308746338\n",
      "step 502 : loss 6.725214958190918\n",
      "step 503 : loss 6.732418060302734\n",
      "step 504 : loss 6.719137668609619\n",
      "step 505 : loss 6.713206768035889\n",
      "step 506 : loss 6.691519260406494\n",
      "step 507 : loss 6.677486896514893\n",
      "step 508 : loss 6.6573405265808105\n",
      "step 509 : loss 6.6436333656311035\n",
      "step 510 : loss 6.629467487335205\n",
      "step 511 : loss 6.619497299194336\n",
      "step 512 : loss 6.61044979095459\n",
      "step 513 : loss 6.603747367858887\n",
      "step 514 : loss 6.597849369049072\n",
      "step 515 : loss 6.593436241149902\n",
      "step 516 : loss 6.589784145355225\n",
      "step 517 : loss 6.587928295135498\n",
      "step 518 : loss 6.58760929107666\n",
      "step 519 : loss 6.591675758361816\n",
      "step 520 : loss 6.600037097930908\n",
      "step 521 : loss 6.621901512145996\n",
      "step 522 : loss 6.651978492736816\n",
      "step 523 : loss 6.717160701751709\n",
      "step 524 : loss 6.762566089630127\n",
      "step 525 : loss 6.846105575561523\n",
      "step 526 : loss 6.792787551879883\n",
      "step 527 : loss 6.752262115478516\n",
      "step 528 : loss 6.6404266357421875\n",
      "step 529 : loss 6.570970058441162\n",
      "step 530 : loss 6.532199859619141\n",
      "step 531 : loss 6.526876926422119\n",
      "step 532 : loss 6.545979976654053\n",
      "step 533 : loss 6.57507848739624\n",
      "step 534 : loss 6.610164165496826\n",
      "step 535 : loss 6.611476421356201\n",
      "step 536 : loss 6.605191707611084\n",
      "step 537 : loss 6.5638298988342285\n",
      "step 538 : loss 6.529391288757324\n",
      "step 539 : loss 6.503050327301025\n",
      "step 540 : loss 6.494160175323486\n",
      "step 541 : loss 6.500384330749512\n",
      "step 542 : loss 6.515625476837158\n",
      "step 543 : loss 6.536443710327148\n",
      "step 544 : loss 6.546572208404541\n",
      "step 545 : loss 6.554390907287598\n",
      "step 546 : loss 6.543884754180908\n",
      "step 547 : loss 6.533510208129883\n",
      "step 548 : loss 6.532189846038818\n",
      "step 549 : loss 6.542905807495117\n",
      "step 550 : loss 6.582931041717529\n",
      "step 551 : loss 6.6013970375061035\n",
      "step 552 : loss 6.625077247619629\n",
      "step 553 : loss 6.576554298400879\n",
      "step 554 : loss 6.534590721130371\n",
      "step 555 : loss 6.4841461181640625\n",
      "step 556 : loss 6.457489013671875\n",
      "step 557 : loss 6.444912433624268\n",
      "step 558 : loss 6.440467834472656\n",
      "step 559 : loss 6.439685821533203\n",
      "step 560 : loss 6.441038608551025\n",
      "step 561 : loss 6.449090003967285\n",
      "step 562 : loss 6.460330009460449\n",
      "step 563 : loss 6.478724956512451\n",
      "step 564 : loss 6.48584508895874\n",
      "step 565 : loss 6.492777347564697\n",
      "step 566 : loss 6.477811813354492\n",
      "step 567 : loss 6.4669976234436035\n",
      "step 568 : loss 6.448431968688965\n",
      "step 569 : loss 6.439048767089844\n",
      "step 570 : loss 6.426298141479492\n",
      "step 571 : loss 6.416362762451172\n",
      "step 572 : loss 6.4028401374816895\n",
      "step 573 : loss 6.391806125640869\n",
      "step 574 : loss 6.383147716522217\n",
      "step 575 : loss 6.378219127655029\n",
      "step 576 : loss 6.3754563331604\n",
      "step 577 : loss 6.3733344078063965\n",
      "step 578 : loss 6.370431900024414\n",
      "step 579 : loss 6.36648416519165\n",
      "step 580 : loss 6.361901760101318\n",
      "step 581 : loss 6.357532501220703\n",
      "step 582 : loss 6.353975296020508\n",
      "step 583 : loss 6.351536273956299\n",
      "step 584 : loss 6.350231647491455\n",
      "step 585 : loss 6.350400924682617\n",
      "step 586 : loss 6.35245943069458\n",
      "step 587 : loss 6.3594841957092285\n",
      "step 588 : loss 6.373758316040039\n",
      "step 589 : loss 6.409584999084473\n",
      "step 590 : loss 6.465351581573486\n",
      "step 591 : loss 6.586760997772217\n",
      "step 592 : loss 6.655237197875977\n",
      "step 593 : loss 6.759428024291992\n",
      "step 594 : loss 6.595210552215576\n",
      "step 595 : loss 6.471513271331787\n",
      "step 596 : loss 6.355627059936523\n",
      "step 597 : loss 6.311923027038574\n",
      "step 598 : loss 6.322113990783691\n",
      "step 599 : loss 6.367770195007324\n",
      "step 600 : loss 6.434365272521973\n",
      "step 601 : loss 6.4539594650268555\n",
      "step 602 : loss 6.455646991729736\n",
      "step 603 : loss 6.3884453773498535\n",
      "step 604 : loss 6.335336208343506\n",
      "step 605 : loss 6.299694061279297\n",
      "step 606 : loss 6.293181419372559\n",
      "step 607 : loss 6.307781219482422\n",
      "step 608 : loss 6.32851505279541\n",
      "step 609 : loss 6.348582744598389\n",
      "step 610 : loss 6.344418525695801\n",
      "step 611 : loss 6.333652496337891\n",
      "step 612 : loss 6.3075456619262695\n",
      "step 613 : loss 6.28676700592041\n",
      "step 614 : loss 6.271938323974609\n",
      "step 615 : loss 6.266651630401611\n",
      "step 616 : loss 6.269200325012207\n",
      "step 617 : loss 6.275901794433594\n",
      "step 618 : loss 6.284598350524902\n",
      "step 619 : loss 6.288078308105469\n",
      "step 620 : loss 6.289759159088135\n",
      "step 621 : loss 6.282113075256348\n",
      "step 622 : loss 6.27379035949707\n",
      "step 623 : loss 6.261446952819824\n",
      "step 624 : loss 6.251209735870361\n",
      "step 625 : loss 6.2424397468566895\n",
      "step 626 : loss 6.236469745635986\n",
      "step 627 : loss 6.2327494621276855\n",
      "step 628 : loss 6.230685234069824\n",
      "step 629 : loss 6.229666709899902\n",
      "step 630 : loss 6.229179382324219\n",
      "step 631 : loss 6.229287147521973\n",
      "step 632 : loss 6.229563236236572\n",
      "step 633 : loss 6.231135845184326\n",
      "step 634 : loss 6.233251571655273\n",
      "step 635 : loss 6.239001750946045\n",
      "step 636 : loss 6.246376037597656\n",
      "step 637 : loss 6.262893199920654\n",
      "step 638 : loss 6.280765533447266\n",
      "step 639 : loss 6.317569255828857\n",
      "step 640 : loss 6.341501235961914\n",
      "step 641 : loss 6.388491630554199\n",
      "step 642 : loss 6.377689838409424\n",
      "step 643 : loss 6.3787360191345215\n",
      "step 644 : loss 6.318384647369385\n",
      "step 645 : loss 6.274747371673584\n",
      "step 646 : loss 6.226679801940918\n",
      "step 647 : loss 6.198023796081543\n",
      "step 648 : loss 6.183435916900635\n",
      "step 649 : loss 6.181347370147705\n",
      "step 650 : loss 6.188506603240967\n",
      "step 651 : loss 6.20100736618042\n",
      "step 652 : loss 6.218209743499756\n",
      "step 653 : loss 6.229772567749023\n",
      "step 654 : loss 6.243268013000488\n",
      "step 655 : loss 6.239694118499756\n",
      "step 656 : loss 6.238156318664551\n",
      "step 657 : loss 6.221716403961182\n",
      "step 658 : loss 6.2107439041137695\n",
      "step 659 : loss 6.195173263549805\n",
      "step 660 : loss 6.18605899810791\n",
      "step 661 : loss 6.177065372467041\n",
      "step 662 : loss 6.172420024871826\n",
      "step 663 : loss 6.167604923248291\n",
      "step 664 : loss 6.164584636688232\n",
      "step 665 : loss 6.160543441772461\n",
      "step 666 : loss 6.156737804412842\n",
      "step 667 : loss 6.152161598205566\n",
      "step 668 : loss 6.147582054138184\n",
      "step 669 : loss 6.143165111541748\n",
      "step 670 : loss 6.1392340660095215\n",
      "step 671 : loss 6.136327266693115\n",
      "step 672 : loss 6.134457588195801\n",
      "step 673 : loss 6.134253025054932\n",
      "step 674 : loss 6.135745048522949\n",
      "step 675 : loss 6.140625953674316\n",
      "step 676 : loss 6.148650169372559\n",
      "step 677 : loss 6.165606498718262\n",
      "step 678 : loss 6.187002658843994\n",
      "step 679 : loss 6.229501724243164\n",
      "step 680 : loss 6.261880397796631\n",
      "step 681 : loss 6.3214592933654785\n",
      "step 682 : loss 6.311033248901367\n",
      "step 683 : loss 6.314217567443848\n",
      "step 684 : loss 6.24538516998291\n",
      "step 685 : loss 6.202141761779785\n",
      "step 686 : loss 6.15755033493042\n",
      "step 687 : loss 6.137635231018066\n",
      "step 688 : loss 6.129076957702637\n",
      "step 689 : loss 6.127158164978027\n",
      "step 690 : loss 6.128152370452881\n",
      "step 691 : loss 6.124624729156494\n",
      "step 692 : loss 6.123913764953613\n",
      "step 693 : loss 6.121975421905518\n",
      "step 694 : loss 6.12725830078125\n",
      "step 695 : loss 6.131742000579834\n",
      "step 696 : loss 6.140700340270996\n",
      "step 697 : loss 6.138530731201172\n",
      "step 698 : loss 6.133321762084961\n",
      "step 699 : loss 6.1159162521362305\n",
      "step 700 : loss 6.099445819854736\n",
      "step 701 : loss 6.084785461425781\n",
      "step 702 : loss 6.077261447906494\n",
      "step 703 : loss 6.075033664703369\n",
      "step 704 : loss 6.07583475112915\n",
      "step 705 : loss 6.075778961181641\n",
      "step 706 : loss 6.07390022277832\n",
      "step 707 : loss 6.069345951080322\n",
      "step 708 : loss 6.063999652862549\n",
      "step 709 : loss 6.0593647956848145\n",
      "step 710 : loss 6.05664587020874\n",
      "step 711 : loss 6.056201457977295\n",
      "step 712 : loss 6.057548999786377\n",
      "step 713 : loss 6.060514450073242\n",
      "step 714 : loss 6.064002513885498\n",
      "step 715 : loss 6.070315361022949\n",
      "step 716 : loss 6.077935695648193\n",
      "step 717 : loss 6.0948486328125\n",
      "step 718 : loss 6.1154327392578125\n",
      "step 719 : loss 6.1588850021362305\n",
      "step 720 : loss 6.192673206329346\n",
      "step 721 : loss 6.255265235900879\n",
      "step 722 : loss 6.243520736694336\n",
      "step 723 : loss 6.242110252380371\n",
      "step 724 : loss 6.16441011428833\n",
      "step 725 : loss 6.109411239624023\n",
      "step 726 : loss 6.058274745941162\n",
      "step 727 : loss 6.031984806060791\n",
      "step 728 : loss 6.023946762084961\n",
      "step 729 : loss 6.029782295227051\n",
      "step 730 : loss 6.045018196105957\n",
      "step 731 : loss 6.062230587005615\n",
      "step 732 : loss 6.082458972930908\n",
      "step 733 : loss 6.0881171226501465\n",
      "step 734 : loss 6.093562602996826\n",
      "step 735 : loss 6.07852840423584\n",
      "step 736 : loss 6.0659356117248535\n",
      "step 737 : loss 6.044900894165039\n",
      "step 738 : loss 6.029362678527832\n",
      "step 739 : loss 6.015692710876465\n",
      "step 740 : loss 6.007124423980713\n",
      "step 741 : loss 6.002538681030273\n",
      "step 742 : loss 6.001542091369629\n",
      "step 743 : loss 6.00338888168335\n",
      "step 744 : loss 6.007034778594971\n",
      "step 745 : loss 6.012509822845459\n",
      "step 746 : loss 6.017439365386963\n",
      "step 747 : loss 6.024281978607178\n",
      "step 748 : loss 6.027765274047852\n",
      "step 749 : loss 6.034034729003906\n",
      "step 750 : loss 6.034420967102051\n",
      "step 751 : loss 6.03873348236084\n",
      "step 752 : loss 6.0358500480651855\n",
      "step 753 : loss 6.037782192230225\n",
      "step 754 : loss 6.032520294189453\n",
      "step 755 : loss 6.032366752624512\n",
      "step 756 : loss 6.025786399841309\n",
      "step 757 : loss 6.023941993713379\n",
      "step 758 : loss 6.016780376434326\n",
      "step 759 : loss 6.013678073883057\n",
      "step 760 : loss 6.006674289703369\n",
      "step 761 : loss 6.003152847290039\n",
      "step 762 : loss 5.997190952301025\n",
      "step 763 : loss 5.994357109069824\n",
      "step 764 : loss 5.990135669708252\n",
      "step 765 : loss 5.988893032073975\n",
      "step 766 : loss 5.9867262840271\n",
      "step 767 : loss 5.9876909255981445\n",
      "step 768 : loss 5.987620830535889\n",
      "step 769 : loss 5.991304397583008\n",
      "step 770 : loss 5.993205547332764\n",
      "step 771 : loss 6.000123023986816\n",
      "step 772 : loss 6.003391742706299\n",
      "step 773 : loss 6.013261795043945\n",
      "step 774 : loss 6.015737056732178\n",
      "step 775 : loss 6.025909900665283\n",
      "step 776 : loss 6.0235419273376465\n",
      "step 777 : loss 6.028818607330322\n",
      "step 778 : loss 6.018944263458252\n",
      "step 779 : loss 6.01624059677124\n",
      "step 780 : loss 6.001725196838379\n",
      "step 781 : loss 5.993862152099609\n",
      "step 782 : loss 5.980258464813232\n",
      "step 783 : loss 5.972042560577393\n",
      "step 784 : loss 5.96211051940918\n",
      "step 785 : loss 5.955737113952637\n",
      "step 786 : loss 5.949149131774902\n",
      "step 787 : loss 5.944467544555664\n",
      "step 788 : loss 5.939939975738525\n",
      "step 789 : loss 5.936313629150391\n",
      "step 790 : loss 5.932910442352295\n",
      "step 791 : loss 5.929943561553955\n",
      "step 792 : loss 5.927233695983887\n",
      "step 793 : loss 5.924790382385254\n",
      "step 794 : loss 5.922607421875\n",
      "step 795 : loss 5.920613765716553\n",
      "step 796 : loss 5.918863296508789\n",
      "step 797 : loss 5.917296409606934\n",
      "step 798 : loss 5.916014194488525\n",
      "step 799 : loss 5.915010452270508\n",
      "step 800 : loss 5.914573669433594\n",
      "step 801 : loss 5.914791107177734\n",
      "step 802 : loss 5.916575908660889\n",
      "step 803 : loss 5.920292854309082\n",
      "step 804 : loss 5.929012775421143\n",
      "step 805 : loss 5.942917823791504\n",
      "step 806 : loss 5.972370624542236\n",
      "step 807 : loss 6.008423805236816\n",
      "step 808 : loss 6.078920364379883\n",
      "step 809 : loss 6.113329887390137\n",
      "step 810 : loss 6.173351764678955\n",
      "step 811 : loss 6.111913204193115\n",
      "step 812 : loss 6.071922779083252\n",
      "step 813 : loss 5.996526718139648\n",
      "step 814 : loss 5.962381839752197\n",
      "step 815 : loss 5.9453206062316895\n",
      "step 816 : loss 5.944143295288086\n",
      "step 817 : loss 5.945876598358154\n",
      "step 818 : loss 5.935573101043701\n",
      "step 819 : loss 5.9262847900390625\n",
      "step 820 : loss 5.916355609893799\n",
      "step 821 : loss 5.920003890991211\n",
      "step 822 : loss 5.929386615753174\n",
      "step 823 : loss 5.942995548248291\n",
      "step 824 : loss 5.940018653869629\n",
      "step 825 : loss 5.9259796142578125\n",
      "step 826 : loss 5.901456832885742\n",
      "step 827 : loss 5.883345603942871\n",
      "step 828 : loss 5.876838684082031\n",
      "step 829 : loss 5.8803815841674805\n",
      "step 830 : loss 5.886106491088867\n",
      "step 831 : loss 5.887905597686768\n",
      "step 832 : loss 5.8838372230529785\n",
      "step 833 : loss 5.877640724182129\n",
      "step 834 : loss 5.87477445602417\n",
      "step 835 : loss 5.876567363739014\n",
      "step 836 : loss 5.882384300231934\n",
      "step 837 : loss 5.886474609375\n",
      "step 838 : loss 5.889511585235596\n",
      "step 839 : loss 5.886984348297119\n",
      "step 840 : loss 5.886237621307373\n",
      "step 841 : loss 5.885242462158203\n",
      "step 842 : loss 5.889925479888916\n",
      "step 843 : loss 5.894148349761963\n",
      "step 844 : loss 5.902718544006348\n",
      "step 845 : loss 5.905246257781982\n",
      "step 846 : loss 5.912046909332275\n",
      "step 847 : loss 5.9111714363098145\n",
      "step 848 : loss 5.918039798736572\n",
      "step 849 : loss 5.9164137840271\n",
      "step 850 : loss 5.922828197479248\n",
      "step 851 : loss 5.915937423706055\n",
      "step 852 : loss 5.914840221405029\n",
      "step 853 : loss 5.901172161102295\n",
      "step 854 : loss 5.8935227394104\n",
      "step 855 : loss 5.880528450012207\n",
      "step 856 : loss 5.873234748840332\n",
      "step 857 : loss 5.864321708679199\n",
      "step 858 : loss 5.858634948730469\n",
      "step 859 : loss 5.8519392013549805\n",
      "step 860 : loss 5.846947193145752\n",
      "step 861 : loss 5.842001914978027\n",
      "step 862 : loss 5.838632583618164\n",
      "step 863 : loss 5.835989475250244\n",
      "step 864 : loss 5.834517478942871\n",
      "step 865 : loss 5.833357334136963\n",
      "step 866 : loss 5.832807540893555\n",
      "step 867 : loss 5.83221435546875\n",
      "step 868 : loss 5.832423686981201\n",
      "step 869 : loss 5.833035945892334\n",
      "step 870 : loss 5.835674285888672\n",
      "step 871 : loss 5.839642524719238\n",
      "step 872 : loss 5.848255634307861\n",
      "step 873 : loss 5.858913421630859\n",
      "step 874 : loss 5.879512310028076\n",
      "step 875 : loss 5.898817539215088\n",
      "step 876 : loss 5.934536933898926\n",
      "step 877 : loss 5.948971271514893\n",
      "step 878 : loss 5.977850437164307\n",
      "step 879 : loss 5.955610752105713\n",
      "step 880 : loss 5.9432902336120605\n",
      "step 881 : loss 5.897059917449951\n",
      "step 882 : loss 5.8655314445495605\n",
      "step 883 : loss 5.834291458129883\n",
      "step 884 : loss 5.815600395202637\n",
      "step 885 : loss 5.805185317993164\n",
      "step 886 : loss 5.8021559715271\n",
      "step 887 : loss 5.804598808288574\n",
      "step 888 : loss 5.810760021209717\n",
      "step 889 : loss 5.819952487945557\n",
      "step 890 : loss 5.828568935394287\n",
      "step 891 : loss 5.839322566986084\n",
      "step 892 : loss 5.843699932098389\n",
      "step 893 : loss 5.849979877471924\n",
      "step 894 : loss 5.84599494934082\n",
      "step 895 : loss 5.844318389892578\n",
      "step 896 : loss 5.8341522216796875\n",
      "step 897 : loss 5.827075481414795\n",
      "step 898 : loss 5.816335201263428\n",
      "step 899 : loss 5.808718204498291\n",
      "step 900 : loss 5.80079460144043\n",
      "step 901 : loss 5.795165538787842\n",
      "step 902 : loss 5.790264129638672\n",
      "step 903 : loss 5.786683082580566\n",
      "step 904 : loss 5.783758640289307\n",
      "step 905 : loss 5.7815022468566895\n",
      "step 906 : loss 5.779613971710205\n",
      "step 907 : loss 5.778020858764648\n",
      "step 908 : loss 5.776603698730469\n",
      "step 909 : loss 5.775293827056885\n",
      "step 910 : loss 5.774056434631348\n",
      "step 911 : loss 5.772855758666992\n",
      "step 912 : loss 5.771675109863281\n",
      "step 913 : loss 5.770513534545898\n",
      "step 914 : loss 5.76936149597168\n",
      "step 915 : loss 5.768216133117676\n",
      "step 916 : loss 5.767082214355469\n",
      "step 917 : loss 5.76596736907959\n",
      "step 918 : loss 5.76486873626709\n",
      "step 919 : loss 5.763801574707031\n",
      "step 920 : loss 5.762789249420166\n",
      "step 921 : loss 5.761898994445801\n",
      "step 922 : loss 5.761194229125977\n",
      "step 923 : loss 5.76088285446167\n",
      "step 924 : loss 5.761263847351074\n",
      "step 925 : loss 5.763236999511719\n",
      "step 926 : loss 5.767894268035889\n",
      "step 927 : loss 5.779297828674316\n",
      "step 928 : loss 5.800422668457031\n",
      "step 929 : loss 5.848604679107666\n",
      "step 930 : loss 5.913864612579346\n",
      "step 931 : loss 6.043447494506836\n",
      "step 932 : loss 6.074655055999756\n",
      "step 933 : loss 6.125924110412598\n",
      "step 934 : loss 5.977413177490234\n",
      "step 935 : loss 5.891968727111816\n",
      "step 936 : loss 5.828047275543213\n",
      "step 937 : loss 5.819142818450928\n",
      "step 938 : loss 5.8297438621521\n",
      "step 939 : loss 5.824324607849121\n",
      "step 940 : loss 5.81522274017334\n",
      "step 941 : loss 5.795019149780273\n",
      "step 942 : loss 5.799897193908691\n",
      "step 943 : loss 5.81479549407959\n",
      "step 944 : loss 5.832463264465332\n",
      "step 945 : loss 5.813483715057373\n",
      "step 946 : loss 5.775677680969238\n",
      "step 947 : loss 5.742094993591309\n",
      "step 948 : loss 5.735688209533691\n",
      "step 949 : loss 5.751442909240723\n",
      "step 950 : loss 5.768010139465332\n",
      "step 951 : loss 5.769608020782471\n",
      "step 952 : loss 5.756759166717529\n",
      "step 953 : loss 5.7483930587768555\n",
      "step 954 : loss 5.748862266540527\n",
      "step 955 : loss 5.7542643547058105\n",
      "step 956 : loss 5.7495646476745605\n",
      "step 957 : loss 5.738285064697266\n",
      "step 958 : loss 5.726237773895264\n",
      "step 959 : loss 5.7224884033203125\n",
      "step 960 : loss 5.726058483123779\n",
      "step 961 : loss 5.730064392089844\n",
      "step 962 : loss 5.730142116546631\n",
      "step 963 : loss 5.72611141204834\n",
      "step 964 : loss 5.723572731018066\n",
      "step 965 : loss 5.724522590637207\n",
      "step 966 : loss 5.728368282318115\n",
      "step 967 : loss 5.730628967285156\n",
      "step 968 : loss 5.731010913848877\n",
      "step 969 : loss 5.728753089904785\n",
      "step 970 : loss 5.728648662567139\n",
      "step 971 : loss 5.730070114135742\n",
      "step 972 : loss 5.7353057861328125\n",
      "step 973 : loss 5.73917293548584\n",
      "step 974 : loss 5.745819091796875\n",
      "step 975 : loss 5.748991012573242\n",
      "step 976 : loss 5.758502006530762\n",
      "step 977 : loss 5.764474391937256\n",
      "step 978 : loss 5.779338359832764\n",
      "step 979 : loss 5.783142566680908\n",
      "step 980 : loss 5.794264793395996\n",
      "step 981 : loss 5.786398410797119\n",
      "step 982 : loss 5.78591775894165\n",
      "step 983 : loss 5.769301414489746\n",
      "step 984 : loss 5.7601423263549805\n",
      "step 985 : loss 5.742166042327881\n",
      "step 986 : loss 5.72982120513916\n",
      "step 987 : loss 5.715820789337158\n",
      "step 988 : loss 5.706323146820068\n",
      "step 989 : loss 5.698858261108398\n",
      "step 990 : loss 5.694216251373291\n",
      "step 991 : loss 5.6910481452941895\n",
      "step 992 : loss 5.688854217529297\n",
      "step 993 : loss 5.687211990356445\n",
      "step 994 : loss 5.686092376708984\n",
      "step 995 : loss 5.685586452484131\n",
      "step 996 : loss 5.685689449310303\n",
      "step 997 : loss 5.686411380767822\n",
      "step 998 : loss 5.687582015991211\n",
      "step 999 : loss 5.689662456512451\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_init_tensor = image_to_tensor(new_image)\n",
    "# print(new_init_tensor.shape)\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(new_init_tensor)\n",
    "    # print(y.shape)\n",
    "    loss = (y-new_init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "result = autoencoder(new_init_tensor)\n",
    "tensor_to_image(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_train = 20\n",
    "num_images_test = 5\n",
    "dataset_train = [None] * num_images_train\n",
    "dataset_test = [None] * num_images_test\n",
    "\n",
    "for i in range(num_images_train):\n",
    "    dataset_train[i] = image_to_tensor(generate_image(image_size))\n",
    "\n",
    "for i in range(num_images_test):\n",
    "    dataset_test[i] = image_to_tensor(generate_image(image_size))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 132.61376953125\n",
      "step 1 : loss 131.64112854003906\n",
      "step 2 : loss 130.5806121826172\n",
      "step 3 : loss 129.34437561035156\n",
      "step 4 : loss 127.84871673583984\n",
      "step 5 : loss 126.00630950927734\n",
      "step 6 : loss 123.7280502319336\n",
      "step 7 : loss 120.91514587402344\n",
      "step 8 : loss 117.4564437866211\n",
      "step 9 : loss 113.2298355102539\n",
      "step 10 : loss 108.11158752441406\n",
      "step 11 : loss 102.01224517822266\n",
      "step 12 : loss 94.96006774902344\n",
      "step 13 : loss 87.30363464355469\n",
      "step 14 : loss 80.12794494628906\n",
      "step 15 : loss 75.85962677001953\n",
      "step 16 : loss 77.81075286865234\n",
      "step 17 : loss 83.12570190429688\n",
      "step 18 : loss 84.24937438964844\n",
      "step 19 : loss 81.36972045898438\n",
      "step 20 : loss 77.34535217285156\n",
      "step 21 : loss 74.3086929321289\n",
      "step 22 : loss 73.00001525878906\n",
      "step 23 : loss 73.07252502441406\n",
      "step 24 : loss 73.79420471191406\n",
      "step 25 : loss 74.55781555175781\n",
      "step 26 : loss 75.02159118652344\n",
      "step 27 : loss 75.05724334716797\n",
      "step 28 : loss 74.6686019897461\n",
      "step 29 : loss 73.93714904785156\n",
      "step 30 : loss 72.9920654296875\n",
      "step 31 : loss 71.99345397949219\n",
      "step 32 : loss 71.1106948852539\n",
      "step 33 : loss 70.48686218261719\n",
      "step 34 : loss 70.18807220458984\n",
      "step 35 : loss 70.15864562988281\n",
      "step 36 : loss 70.228515625\n",
      "step 37 : loss 70.19652557373047\n",
      "step 38 : loss 69.94200897216797\n",
      "step 39 : loss 69.4799575805664\n",
      "step 40 : loss 68.92622375488281\n",
      "step 41 : loss 68.4155044555664\n",
      "step 42 : loss 68.03297424316406\n",
      "step 43 : loss 67.79144287109375\n",
      "step 44 : loss 67.64751434326172\n",
      "step 45 : loss 67.5347671508789\n",
      "step 46 : loss 67.39158630371094\n",
      "step 47 : loss 67.1783447265625\n",
      "step 48 : loss 66.88221740722656\n",
      "step 49 : loss 66.51512145996094\n",
      "step 50 : loss 66.10787200927734\n",
      "step 51 : loss 65.70085144042969\n",
      "step 52 : loss 65.33126068115234\n",
      "step 53 : loss 65.01766967773438\n",
      "step 54 : loss 64.74591827392578\n",
      "step 55 : loss 64.46614074707031\n",
      "step 56 : loss 64.11184692382812\n",
      "step 57 : loss 63.64076614379883\n",
      "step 58 : loss 63.06865692138672\n",
      "step 59 : loss 62.4543342590332\n",
      "step 60 : loss 61.84149932861328\n",
      "step 61 : loss 61.219207763671875\n",
      "step 62 : loss 60.54290771484375\n",
      "step 63 : loss 59.80757522583008\n",
      "step 64 : loss 59.12335205078125\n",
      "step 65 : loss 58.577125549316406\n",
      "step 66 : loss 57.898162841796875\n",
      "step 67 : loss 57.32262420654297\n",
      "step 68 : loss 56.94213104248047\n",
      "step 69 : loss 56.475746154785156\n",
      "step 70 : loss 56.21580123901367\n",
      "step 71 : loss 55.76920700073242\n",
      "step 72 : loss 55.510223388671875\n",
      "step 73 : loss 55.09564971923828\n",
      "step 74 : loss 54.836669921875\n",
      "step 75 : loss 54.364906311035156\n",
      "step 76 : loss 53.98732376098633\n",
      "step 77 : loss 53.47003173828125\n",
      "step 78 : loss 52.97400665283203\n",
      "step 79 : loss 52.4420051574707\n",
      "step 80 : loss 51.918983459472656\n",
      "step 81 : loss 51.4506950378418\n",
      "step 82 : loss 50.922828674316406\n",
      "step 83 : loss 50.48322296142578\n",
      "step 84 : loss 49.99754333496094\n",
      "step 85 : loss 49.5792236328125\n",
      "step 86 : loss 49.12331008911133\n",
      "step 87 : loss 48.69404983520508\n",
      "step 88 : loss 48.24376678466797\n",
      "step 89 : loss 47.772911071777344\n",
      "step 90 : loss 47.34123992919922\n",
      "step 91 : loss 46.889320373535156\n",
      "step 92 : loss 46.46847152709961\n",
      "step 93 : loss 46.10321807861328\n",
      "step 94 : loss 45.78439712524414\n",
      "step 95 : loss 45.52052307128906\n",
      "step 96 : loss 45.290374755859375\n",
      "step 97 : loss 45.100425720214844\n",
      "step 98 : loss 44.934017181396484\n",
      "step 99 : loss 44.806556701660156\n",
      "step 100 : loss 44.63168716430664\n",
      "step 101 : loss 44.40302276611328\n",
      "step 102 : loss 44.109397888183594\n",
      "step 103 : loss 43.90567398071289\n",
      "step 104 : loss 43.793617248535156\n",
      "step 105 : loss 43.658912658691406\n",
      "step 106 : loss 43.45970916748047\n",
      "step 107 : loss 43.252174377441406\n",
      "step 108 : loss 43.11396789550781\n",
      "step 109 : loss 43.0147705078125\n",
      "step 110 : loss 42.85386276245117\n",
      "step 111 : loss 42.67409896850586\n",
      "step 112 : loss 42.507850646972656\n",
      "step 113 : loss 42.3914909362793\n",
      "step 114 : loss 42.31462097167969\n",
      "step 115 : loss 42.258174896240234\n",
      "step 116 : loss 42.233543395996094\n",
      "step 117 : loss 42.16053771972656\n",
      "step 118 : loss 42.05333709716797\n",
      "step 119 : loss 41.8367805480957\n",
      "step 120 : loss 41.671241760253906\n",
      "step 121 : loss 41.594505310058594\n",
      "step 122 : loss 41.57792282104492\n",
      "step 123 : loss 41.59882354736328\n",
      "step 124 : loss 41.53071212768555\n",
      "step 125 : loss 41.418270111083984\n",
      "step 126 : loss 41.210174560546875\n",
      "step 127 : loss 41.071292877197266\n",
      "step 128 : loss 41.02290725708008\n",
      "step 129 : loss 41.034210205078125\n",
      "step 130 : loss 41.09723663330078\n",
      "step 131 : loss 41.05967330932617\n",
      "step 132 : loss 40.972843170166016\n",
      "step 133 : loss 40.728851318359375\n",
      "step 134 : loss 40.58629608154297\n",
      "step 135 : loss 40.581565856933594\n",
      "step 136 : loss 40.626373291015625\n",
      "step 137 : loss 40.64672088623047\n",
      "step 138 : loss 40.48676300048828\n",
      "step 139 : loss 40.330360412597656\n",
      "step 140 : loss 40.249839782714844\n",
      "step 141 : loss 40.254371643066406\n",
      "step 142 : loss 40.295143127441406\n",
      "step 143 : loss 40.25672912597656\n",
      "step 144 : loss 40.18915557861328\n",
      "step 145 : loss 40.04480743408203\n",
      "step 146 : loss 39.932621002197266\n",
      "step 147 : loss 39.85447692871094\n",
      "step 148 : loss 39.8105583190918\n",
      "step 149 : loss 39.82085418701172\n",
      "step 150 : loss 39.991065979003906\n",
      "step 151 : loss 40.519657135009766\n",
      "step 152 : loss 40.47570037841797\n",
      "step 153 : loss 40.03902053833008\n",
      "step 154 : loss 39.49938201904297\n",
      "step 155 : loss 39.864959716796875\n",
      "step 156 : loss 40.1328125\n",
      "step 157 : loss 39.40216064453125\n",
      "step 158 : loss 39.62154006958008\n",
      "step 159 : loss 39.88861846923828\n",
      "step 160 : loss 39.22236633300781\n",
      "step 161 : loss 39.57280731201172\n",
      "step 162 : loss 39.59258270263672\n",
      "step 163 : loss 39.060543060302734\n",
      "step 164 : loss 39.531707763671875\n",
      "step 165 : loss 39.20954895019531\n",
      "step 166 : loss 38.95558547973633\n",
      "step 167 : loss 39.2987060546875\n",
      "step 168 : loss 38.779945373535156\n",
      "step 169 : loss 38.82722473144531\n",
      "step 170 : loss 38.814910888671875\n",
      "step 171 : loss 38.43670654296875\n",
      "step 172 : loss 38.545692443847656\n",
      "step 173 : loss 38.31355667114258\n",
      "step 174 : loss 38.10506820678711\n",
      "step 175 : loss 38.11565399169922\n",
      "step 176 : loss 37.809288024902344\n",
      "step 177 : loss 37.63468933105469\n",
      "step 178 : loss 37.540462493896484\n",
      "step 179 : loss 37.193912506103516\n",
      "step 180 : loss 36.90486526489258\n",
      "step 181 : loss 36.705772399902344\n",
      "step 182 : loss 36.390995025634766\n",
      "step 183 : loss 36.01115417480469\n",
      "step 184 : loss 35.544376373291016\n",
      "step 185 : loss 35.068111419677734\n",
      "step 186 : loss 34.6142578125\n",
      "step 187 : loss 34.46137237548828\n",
      "step 188 : loss 35.511940002441406\n",
      "step 189 : loss 38.42498779296875\n",
      "step 190 : loss 32.961585998535156\n",
      "step 191 : loss 41.31243133544922\n",
      "step 192 : loss 34.224632263183594\n",
      "step 193 : loss 36.69475555419922\n",
      "step 194 : loss 31.945709228515625\n",
      "step 195 : loss 35.97864532470703\n",
      "step 196 : loss 31.65680503845215\n",
      "step 197 : loss 34.40910720825195\n",
      "step 198 : loss 32.10654067993164\n",
      "step 199 : loss 33.12275695800781\n",
      "step 200 : loss 32.40339279174805\n",
      "step 201 : loss 31.88979148864746\n",
      "step 202 : loss 32.69249725341797\n",
      "step 203 : loss 30.962194442749023\n",
      "step 204 : loss 32.40564727783203\n",
      "step 205 : loss 30.9290828704834\n",
      "step 206 : loss 31.568012237548828\n",
      "step 207 : loss 31.334407806396484\n",
      "step 208 : loss 30.728954315185547\n",
      "step 209 : loss 31.317485809326172\n",
      "step 210 : loss 30.44708251953125\n",
      "step 211 : loss 31.03385353088379\n",
      "step 212 : loss 30.43180274963379\n",
      "step 213 : loss 30.61774253845215\n",
      "step 214 : loss 30.470117568969727\n",
      "step 215 : loss 30.2240047454834\n",
      "step 216 : loss 30.408193588256836\n",
      "step 217 : loss 29.953760147094727\n",
      "step 218 : loss 30.220577239990234\n",
      "step 219 : loss 29.81814956665039\n",
      "step 220 : loss 29.96775245666504\n",
      "step 221 : loss 29.777828216552734\n",
      "step 222 : loss 29.70798110961914\n",
      "step 223 : loss 29.699054718017578\n",
      "step 224 : loss 29.511301040649414\n",
      "step 225 : loss 29.589330673217773\n",
      "step 226 : loss 29.343887329101562\n",
      "step 227 : loss 29.43987464904785\n",
      "step 228 : loss 29.21775245666504\n",
      "step 229 : loss 29.28030776977539\n",
      "step 230 : loss 29.112707138061523\n",
      "step 231 : loss 29.123363494873047\n",
      "step 232 : loss 29.001134872436523\n",
      "step 233 : loss 28.99529457092285\n",
      "step 234 : loss 28.88458251953125\n",
      "step 235 : loss 28.870702743530273\n",
      "step 236 : loss 28.770503997802734\n",
      "step 237 : loss 28.749492645263672\n",
      "step 238 : loss 28.648944854736328\n",
      "step 239 : loss 28.640045166015625\n",
      "step 240 : loss 28.535690307617188\n",
      "step 241 : loss 28.532413482666016\n",
      "step 242 : loss 28.4327449798584\n",
      "step 243 : loss 28.42440414428711\n",
      "step 244 : loss 28.342065811157227\n",
      "step 245 : loss 28.30918312072754\n",
      "step 246 : loss 28.256006240844727\n",
      "step 247 : loss 28.196208953857422\n",
      "step 248 : loss 28.174068450927734\n",
      "step 249 : loss 28.10321617126465\n",
      "step 250 : loss 28.0819034576416\n",
      "step 251 : loss 28.039419174194336\n",
      "step 252 : loss 28.021596908569336\n",
      "step 253 : loss 28.093530654907227\n",
      "step 254 : loss 28.321735382080078\n",
      "step 255 : loss 28.814722061157227\n",
      "step 256 : loss 28.62312889099121\n",
      "step 257 : loss 28.076236724853516\n",
      "step 258 : loss 27.76383399963379\n",
      "step 259 : loss 28.06340980529785\n",
      "step 260 : loss 28.293249130249023\n",
      "step 261 : loss 27.817852020263672\n",
      "step 262 : loss 27.673486709594727\n",
      "step 263 : loss 27.940753936767578\n",
      "step 264 : loss 27.87784194946289\n",
      "step 265 : loss 27.569869995117188\n",
      "step 266 : loss 27.5799503326416\n",
      "step 267 : loss 27.711162567138672\n",
      "step 268 : loss 27.6339168548584\n",
      "step 269 : loss 27.416728973388672\n",
      "step 270 : loss 27.478923797607422\n",
      "step 271 : loss 27.560047149658203\n",
      "step 272 : loss 27.418914794921875\n",
      "step 273 : loss 27.295230865478516\n",
      "step 274 : loss 27.348560333251953\n",
      "step 275 : loss 27.37448501586914\n",
      "step 276 : loss 27.272380828857422\n",
      "step 277 : loss 27.178686141967773\n",
      "step 278 : loss 27.18528175354004\n",
      "step 279 : loss 27.225753784179688\n",
      "step 280 : loss 27.16310691833496\n",
      "step 281 : loss 27.078577041625977\n",
      "step 282 : loss 27.027664184570312\n",
      "step 283 : loss 27.0373592376709\n",
      "step 284 : loss 27.046422958374023\n",
      "step 285 : loss 27.012731552124023\n",
      "step 286 : loss 26.9530029296875\n",
      "step 287 : loss 26.88832664489746\n",
      "step 288 : loss 26.8536434173584\n",
      "step 289 : loss 26.836044311523438\n",
      "step 290 : loss 26.8347110748291\n",
      "step 291 : loss 26.831783294677734\n",
      "step 292 : loss 26.819183349609375\n",
      "step 293 : loss 26.803945541381836\n",
      "step 294 : loss 26.76987075805664\n",
      "step 295 : loss 26.741384506225586\n",
      "step 296 : loss 26.701892852783203\n",
      "step 297 : loss 26.672489166259766\n",
      "step 298 : loss 26.63893699645996\n",
      "step 299 : loss 26.6165828704834\n",
      "step 300 : loss 26.597482681274414\n",
      "step 301 : loss 26.59304428100586\n",
      "step 302 : loss 26.592565536499023\n",
      "step 303 : loss 26.615142822265625\n",
      "step 304 : loss 26.617176055908203\n",
      "step 305 : loss 26.628582000732422\n",
      "step 306 : loss 26.56973648071289\n",
      "step 307 : loss 26.50193214416504\n",
      "step 308 : loss 26.39068603515625\n",
      "step 309 : loss 26.300106048583984\n",
      "step 310 : loss 26.229476928710938\n",
      "step 311 : loss 26.183486938476562\n",
      "step 312 : loss 26.157756805419922\n",
      "step 313 : loss 26.145893096923828\n",
      "step 314 : loss 26.148380279541016\n",
      "step 315 : loss 26.162887573242188\n",
      "step 316 : loss 26.20127296447754\n",
      "step 317 : loss 26.236736297607422\n",
      "step 318 : loss 26.29031753540039\n",
      "step 319 : loss 26.258432388305664\n",
      "step 320 : loss 26.20419692993164\n",
      "step 321 : loss 26.067296981811523\n",
      "step 322 : loss 25.95148277282715\n",
      "step 323 : loss 25.858591079711914\n",
      "step 324 : loss 25.801589965820312\n",
      "step 325 : loss 25.77217674255371\n",
      "step 326 : loss 25.763381958007812\n",
      "step 327 : loss 25.77468490600586\n",
      "step 328 : loss 25.802942276000977\n",
      "step 329 : loss 25.864181518554688\n",
      "step 330 : loss 25.90866470336914\n",
      "step 331 : loss 25.964014053344727\n",
      "step 332 : loss 25.88922691345215\n",
      "step 333 : loss 25.790740966796875\n",
      "step 334 : loss 25.63384437561035\n",
      "step 335 : loss 25.51693344116211\n",
      "step 336 : loss 25.437442779541016\n",
      "step 337 : loss 25.39506721496582\n",
      "step 338 : loss 25.3803653717041\n",
      "step 339 : loss 25.38723373413086\n",
      "step 340 : loss 25.420181274414062\n",
      "step 341 : loss 25.463336944580078\n",
      "step 342 : loss 25.537593841552734\n",
      "step 343 : loss 25.541162490844727\n",
      "step 344 : loss 25.52750015258789\n",
      "step 345 : loss 25.385108947753906\n",
      "step 346 : loss 25.25139808654785\n",
      "step 347 : loss 25.121822357177734\n",
      "step 348 : loss 25.036182403564453\n",
      "step 349 : loss 24.984350204467773\n",
      "step 350 : loss 24.958354949951172\n",
      "step 351 : loss 24.954051971435547\n",
      "step 352 : loss 24.96997833251953\n",
      "step 353 : loss 25.022228240966797\n",
      "step 354 : loss 25.08440399169922\n",
      "step 355 : loss 25.193134307861328\n",
      "step 356 : loss 25.176300048828125\n",
      "step 357 : loss 25.125329971313477\n",
      "step 358 : loss 24.908742904663086\n",
      "step 359 : loss 24.730165481567383\n",
      "step 360 : loss 24.600021362304688\n",
      "step 361 : loss 24.537599563598633\n",
      "step 362 : loss 24.524662017822266\n",
      "step 363 : loss 24.546321868896484\n",
      "step 364 : loss 24.60529136657715\n",
      "step 365 : loss 24.65418243408203\n",
      "step 366 : loss 24.719806671142578\n",
      "step 367 : loss 24.652240753173828\n",
      "step 368 : loss 24.56361961364746\n",
      "step 369 : loss 24.389528274536133\n",
      "step 370 : loss 24.25310516357422\n",
      "step 371 : loss 24.145435333251953\n",
      "step 372 : loss 24.07613182067871\n",
      "step 373 : loss 24.03292465209961\n",
      "step 374 : loss 24.0078182220459\n",
      "step 375 : loss 24.00261878967285\n",
      "step 376 : loss 24.004467010498047\n",
      "step 377 : loss 24.035537719726562\n",
      "step 378 : loss 24.024307250976562\n",
      "step 379 : loss 24.028799057006836\n",
      "step 380 : loss 23.96147918701172\n",
      "step 381 : loss 24.055166244506836\n",
      "step 382 : loss 24.494705200195312\n",
      "step 383 : loss 25.614025115966797\n",
      "step 384 : loss 24.421672821044922\n",
      "step 385 : loss 23.43051528930664\n",
      "step 386 : loss 23.609439849853516\n",
      "step 387 : loss 24.26970100402832\n",
      "step 388 : loss 24.041549682617188\n",
      "step 389 : loss 23.212146759033203\n",
      "step 390 : loss 23.496768951416016\n",
      "step 391 : loss 23.71630096435547\n",
      "step 392 : loss 23.138973236083984\n",
      "step 393 : loss 23.174631118774414\n",
      "step 394 : loss 23.207557678222656\n",
      "step 395 : loss 22.8232364654541\n",
      "step 396 : loss 22.708032608032227\n",
      "step 397 : loss 22.894861221313477\n",
      "step 398 : loss 22.76251792907715\n",
      "step 399 : loss 22.344266891479492\n",
      "step 400 : loss 22.34901237487793\n",
      "step 401 : loss 22.464008331298828\n",
      "step 402 : loss 22.15627670288086\n",
      "step 403 : loss 21.869747161865234\n",
      "step 404 : loss 21.84092140197754\n",
      "step 405 : loss 21.85626220703125\n",
      "step 406 : loss 21.72675132751465\n",
      "step 407 : loss 21.587589263916016\n",
      "step 408 : loss 22.038297653198242\n",
      "step 409 : loss 24.251697540283203\n",
      "step 410 : loss 23.022647857666016\n",
      "step 411 : loss 22.440715789794922\n",
      "step 412 : loss 21.12277603149414\n",
      "step 413 : loss 20.651512145996094\n",
      "step 414 : loss 20.740657806396484\n",
      "step 415 : loss 21.441965103149414\n",
      "step 416 : loss 22.996562957763672\n",
      "step 417 : loss 20.520496368408203\n",
      "step 418 : loss 19.85349464416504\n",
      "step 419 : loss 20.486730575561523\n",
      "step 420 : loss 21.341121673583984\n",
      "step 421 : loss 21.854684829711914\n",
      "step 422 : loss 19.42348861694336\n",
      "step 423 : loss 20.207218170166016\n",
      "step 424 : loss 22.397178649902344\n",
      "step 425 : loss 19.002626419067383\n",
      "step 426 : loss 21.09307098388672\n",
      "step 427 : loss 22.90894317626953\n",
      "step 428 : loss 19.354137420654297\n",
      "step 429 : loss 26.27083396911621\n",
      "step 430 : loss 19.734569549560547\n",
      "step 431 : loss 22.31393051147461\n",
      "step 432 : loss 18.890979766845703\n",
      "step 433 : loss 20.78081703186035\n",
      "step 434 : loss 18.93918228149414\n",
      "step 435 : loss 20.5150203704834\n",
      "step 436 : loss 18.670629501342773\n",
      "step 437 : loss 19.17330551147461\n",
      "step 438 : loss 18.571863174438477\n",
      "step 439 : loss 18.095584869384766\n",
      "step 440 : loss 18.82407569885254\n",
      "step 441 : loss 17.71091079711914\n",
      "step 442 : loss 18.83015251159668\n",
      "step 443 : loss 17.598941802978516\n",
      "step 444 : loss 18.21225929260254\n",
      "step 445 : loss 17.277122497558594\n",
      "step 446 : loss 17.721960067749023\n",
      "step 447 : loss 17.28776741027832\n",
      "step 448 : loss 17.049509048461914\n",
      "step 449 : loss 17.382715225219727\n",
      "step 450 : loss 16.751728057861328\n",
      "step 451 : loss 17.0241756439209\n",
      "step 452 : loss 16.694568634033203\n",
      "step 453 : loss 16.68198013305664\n",
      "step 454 : loss 16.80002212524414\n",
      "step 455 : loss 16.293041229248047\n",
      "step 456 : loss 16.485715866088867\n",
      "step 457 : loss 16.345378875732422\n",
      "step 458 : loss 16.104351043701172\n",
      "step 459 : loss 16.419370651245117\n",
      "step 460 : loss 16.258899688720703\n",
      "step 461 : loss 16.287303924560547\n",
      "step 462 : loss 16.586942672729492\n",
      "step 463 : loss 16.648500442504883\n",
      "step 464 : loss 16.42397689819336\n",
      "step 465 : loss 16.504520416259766\n",
      "step 466 : loss 15.912240982055664\n",
      "step 467 : loss 15.78569221496582\n",
      "step 468 : loss 15.731698989868164\n",
      "step 469 : loss 15.522109985351562\n",
      "step 470 : loss 15.482373237609863\n",
      "step 471 : loss 15.59660530090332\n",
      "step 472 : loss 15.588040351867676\n",
      "step 473 : loss 15.819465637207031\n",
      "step 474 : loss 16.171215057373047\n",
      "step 475 : loss 16.53095054626465\n",
      "step 476 : loss 15.815587043762207\n",
      "step 477 : loss 15.48662281036377\n",
      "step 478 : loss 15.171536445617676\n",
      "step 479 : loss 15.025588989257812\n",
      "step 480 : loss 15.056028366088867\n",
      "step 481 : loss 15.107154846191406\n",
      "step 482 : loss 15.201820373535156\n",
      "step 483 : loss 15.449658393859863\n",
      "step 484 : loss 15.904245376586914\n",
      "step 485 : loss 15.562344551086426\n",
      "step 486 : loss 15.334324836730957\n",
      "step 487 : loss 15.018186569213867\n",
      "step 488 : loss 14.822896003723145\n",
      "step 489 : loss 14.723347663879395\n",
      "step 490 : loss 14.706341743469238\n",
      "step 491 : loss 14.69822883605957\n",
      "step 492 : loss 14.693142890930176\n",
      "step 493 : loss 14.761240005493164\n",
      "step 494 : loss 14.922225952148438\n",
      "step 495 : loss 15.243463516235352\n",
      "step 496 : loss 15.268588066101074\n",
      "step 497 : loss 15.308178901672363\n",
      "step 498 : loss 14.9139404296875\n",
      "step 499 : loss 14.674247741699219\n",
      "step 500 : loss 14.506922721862793\n",
      "step 501 : loss 14.434405326843262\n",
      "step 502 : loss 14.396890640258789\n",
      "step 503 : loss 14.368425369262695\n",
      "step 504 : loss 14.341113090515137\n",
      "step 505 : loss 14.322421073913574\n",
      "step 506 : loss 14.322566032409668\n",
      "step 507 : loss 14.353307723999023\n",
      "step 508 : loss 14.462007522583008\n",
      "step 509 : loss 14.66380500793457\n",
      "step 510 : loss 15.101239204406738\n",
      "step 511 : loss 15.097986221313477\n",
      "step 512 : loss 15.055755615234375\n",
      "step 513 : loss 14.557574272155762\n",
      "step 514 : loss 14.29973316192627\n",
      "step 515 : loss 14.157037734985352\n",
      "step 516 : loss 14.096125602722168\n",
      "step 517 : loss 14.06920337677002\n",
      "step 518 : loss 14.060876846313477\n",
      "step 519 : loss 14.074427604675293\n",
      "step 520 : loss 14.126733779907227\n",
      "step 521 : loss 14.279150009155273\n",
      "step 522 : loss 14.50024127960205\n",
      "step 523 : loss 14.878646850585938\n",
      "step 524 : loss 14.704917907714844\n",
      "step 525 : loss 14.537487983703613\n",
      "step 526 : loss 14.19780445098877\n",
      "step 527 : loss 14.036331176757812\n",
      "step 528 : loss 13.947285652160645\n",
      "step 529 : loss 13.915738105773926\n",
      "step 530 : loss 13.908025741577148\n",
      "step 531 : loss 13.929780960083008\n",
      "step 532 : loss 13.968470573425293\n",
      "step 533 : loss 14.048373222351074\n",
      "step 534 : loss 14.100398063659668\n",
      "step 535 : loss 14.149024963378906\n",
      "step 536 : loss 14.087061882019043\n",
      "step 537 : loss 14.019515991210938\n",
      "step 538 : loss 14.007890701293945\n",
      "step 539 : loss 14.064035415649414\n",
      "step 540 : loss 14.3095703125\n",
      "step 541 : loss 14.377885818481445\n",
      "step 542 : loss 14.430974960327148\n",
      "step 543 : loss 14.078245162963867\n",
      "step 544 : loss 13.848974227905273\n",
      "step 545 : loss 13.690913200378418\n",
      "step 546 : loss 13.62584400177002\n",
      "step 547 : loss 13.60789966583252\n",
      "step 548 : loss 13.624653816223145\n",
      "step 549 : loss 13.679128646850586\n",
      "step 550 : loss 13.759404182434082\n",
      "step 551 : loss 13.878156661987305\n",
      "step 552 : loss 13.889300346374512\n",
      "step 553 : loss 13.866511344909668\n",
      "step 554 : loss 13.74169921875\n",
      "step 555 : loss 13.668912887573242\n",
      "step 556 : loss 13.705225944519043\n",
      "step 557 : loss 13.837142944335938\n",
      "step 558 : loss 14.151552200317383\n",
      "step 559 : loss 14.161981582641602\n",
      "step 560 : loss 14.119502067565918\n",
      "step 561 : loss 13.756795883178711\n",
      "step 562 : loss 13.55583667755127\n",
      "step 563 : loss 13.441920280456543\n",
      "step 564 : loss 13.415698051452637\n",
      "step 565 : loss 13.445879936218262\n",
      "step 566 : loss 13.52851390838623\n",
      "step 567 : loss 13.66981029510498\n",
      "step 568 : loss 13.731355667114258\n",
      "step 569 : loss 13.739692687988281\n",
      "step 570 : loss 13.5719633102417\n",
      "step 571 : loss 13.444684028625488\n",
      "step 572 : loss 13.401582717895508\n",
      "step 573 : loss 13.452001571655273\n",
      "step 574 : loss 13.64777946472168\n",
      "step 575 : loss 13.800933837890625\n",
      "step 576 : loss 13.957189559936523\n",
      "step 577 : loss 13.69526481628418\n",
      "step 578 : loss 13.501194953918457\n",
      "step 579 : loss 13.33305835723877\n",
      "step 580 : loss 13.291208267211914\n",
      "step 581 : loss 13.324167251586914\n",
      "step 582 : loss 13.424867630004883\n",
      "step 583 : loss 13.553601264953613\n",
      "step 584 : loss 13.544717788696289\n",
      "step 585 : loss 13.46294116973877\n",
      "step 586 : loss 13.30237865447998\n",
      "step 587 : loss 13.19865894317627\n",
      "step 588 : loss 13.176676750183105\n",
      "step 589 : loss 13.223104476928711\n",
      "step 590 : loss 13.36810302734375\n",
      "step 591 : loss 13.491289138793945\n",
      "step 592 : loss 13.629997253417969\n",
      "step 593 : loss 13.481376647949219\n",
      "step 594 : loss 13.356389045715332\n",
      "step 595 : loss 13.200843811035156\n",
      "step 596 : loss 13.155102729797363\n",
      "step 597 : loss 13.168027877807617\n",
      "step 598 : loss 13.264132499694824\n",
      "step 599 : loss 13.383169174194336\n",
      "step 600 : loss 13.407499313354492\n",
      "step 601 : loss 13.334157943725586\n",
      "step 602 : loss 13.164637565612793\n",
      "step 603 : loss 13.03478717803955\n",
      "step 604 : loss 12.97016429901123\n",
      "step 605 : loss 12.959304809570312\n",
      "step 606 : loss 13.010000228881836\n",
      "step 607 : loss 13.100709915161133\n",
      "step 608 : loss 13.260297775268555\n",
      "step 609 : loss 13.316202163696289\n",
      "step 610 : loss 13.34801197052002\n",
      "step 611 : loss 13.181938171386719\n",
      "step 612 : loss 13.067448616027832\n",
      "step 613 : loss 12.966997146606445\n",
      "step 614 : loss 12.955909729003906\n",
      "step 615 : loss 12.998332023620605\n",
      "step 616 : loss 13.127543449401855\n",
      "step 617 : loss 13.26417064666748\n",
      "step 618 : loss 13.296564102172852\n",
      "step 619 : loss 13.187273979187012\n",
      "step 620 : loss 13.00049114227295\n",
      "step 621 : loss 12.856603622436523\n",
      "step 622 : loss 12.78414535522461\n",
      "step 623 : loss 12.759992599487305\n",
      "step 624 : loss 12.782974243164062\n",
      "step 625 : loss 12.843015670776367\n",
      "step 626 : loss 12.967997550964355\n",
      "step 627 : loss 13.072232246398926\n",
      "step 628 : loss 13.18467903137207\n",
      "step 629 : loss 13.080766677856445\n",
      "step 630 : loss 12.974759101867676\n",
      "step 631 : loss 12.826095581054688\n",
      "step 632 : loss 12.751684188842773\n",
      "step 633 : loss 12.716657638549805\n",
      "step 634 : loss 12.748224258422852\n",
      "step 635 : loss 12.834510803222656\n",
      "step 636 : loss 13.014851570129395\n",
      "step 637 : loss 13.172823905944824\n",
      "step 638 : loss 13.201501846313477\n",
      "step 639 : loss 13.023323059082031\n",
      "step 640 : loss 12.817265510559082\n",
      "step 641 : loss 12.664867401123047\n",
      "step 642 : loss 12.589582443237305\n",
      "step 643 : loss 12.556058883666992\n",
      "step 644 : loss 12.55500602722168\n",
      "step 645 : loss 12.579987525939941\n",
      "step 646 : loss 12.650616645812988\n",
      "step 647 : loss 12.757430076599121\n",
      "step 648 : loss 12.9344482421875\n",
      "step 649 : loss 12.983428955078125\n",
      "step 650 : loss 12.98865032196045\n",
      "step 651 : loss 12.791021347045898\n",
      "step 652 : loss 12.649717330932617\n",
      "step 653 : loss 12.539155960083008\n",
      "step 654 : loss 12.492376327514648\n",
      "step 655 : loss 12.484254837036133\n",
      "step 656 : loss 12.533082962036133\n",
      "step 657 : loss 12.651812553405762\n",
      "step 658 : loss 12.886985778808594\n",
      "step 659 : loss 13.097297668457031\n",
      "step 660 : loss 13.112820625305176\n",
      "step 661 : loss 12.853208541870117\n",
      "step 662 : loss 12.610238075256348\n",
      "step 663 : loss 12.454195022583008\n",
      "step 664 : loss 12.394105911254883\n",
      "step 665 : loss 12.38536262512207\n",
      "step 666 : loss 12.42493724822998\n",
      "step 667 : loss 12.506200790405273\n",
      "step 668 : loss 12.656525611877441\n",
      "step 669 : loss 12.743782997131348\n",
      "step 670 : loss 12.805570602416992\n",
      "step 671 : loss 12.640565872192383\n",
      "step 672 : loss 12.505439758300781\n",
      "step 673 : loss 12.385583877563477\n",
      "step 674 : loss 12.332879066467285\n",
      "step 675 : loss 12.323790550231934\n",
      "step 676 : loss 12.37368106842041\n",
      "step 677 : loss 12.486320495605469\n",
      "step 678 : loss 12.709466934204102\n",
      "step 679 : loss 12.875805854797363\n",
      "step 680 : loss 12.93529224395752\n",
      "step 681 : loss 12.687311172485352\n",
      "step 682 : loss 12.47799015045166\n",
      "step 683 : loss 12.322074890136719\n",
      "step 684 : loss 12.260259628295898\n",
      "step 685 : loss 12.245892524719238\n",
      "step 686 : loss 12.279439926147461\n",
      "step 687 : loss 12.349645614624023\n",
      "step 688 : loss 12.483799934387207\n",
      "step 689 : loss 12.569046974182129\n",
      "step 690 : loss 12.641467094421387\n",
      "step 691 : loss 12.50480842590332\n",
      "step 692 : loss 12.386248588562012\n",
      "step 693 : loss 12.263310432434082\n",
      "step 694 : loss 12.204948425292969\n",
      "step 695 : loss 12.1845064163208\n",
      "step 696 : loss 12.2177734375\n",
      "step 697 : loss 12.302486419677734\n",
      "step 698 : loss 12.492039680480957\n",
      "step 699 : loss 12.664824485778809\n",
      "step 700 : loss 12.791956901550293\n",
      "step 701 : loss 12.600973129272461\n",
      "step 702 : loss 12.388522148132324\n",
      "step 703 : loss 12.203028678894043\n",
      "step 704 : loss 12.107338905334473\n",
      "step 705 : loss 12.057558059692383\n",
      "step 706 : loss 12.034403800964355\n",
      "step 707 : loss 12.024091720581055\n",
      "step 708 : loss 12.023587226867676\n",
      "step 709 : loss 12.035165786743164\n",
      "step 710 : loss 12.07341480255127\n",
      "step 711 : loss 12.155233383178711\n",
      "step 712 : loss 12.32926082611084\n",
      "step 713 : loss 12.494303703308105\n",
      "step 714 : loss 12.64918041229248\n",
      "step 715 : loss 12.439080238342285\n",
      "step 716 : loss 12.252765655517578\n",
      "step 717 : loss 12.087970733642578\n",
      "step 718 : loss 12.021599769592285\n",
      "step 719 : loss 12.016852378845215\n",
      "step 720 : loss 12.093259811401367\n",
      "step 721 : loss 12.260123252868652\n",
      "step 722 : loss 12.556974411010742\n",
      "step 723 : loss 12.695953369140625\n",
      "step 724 : loss 12.606880187988281\n",
      "step 725 : loss 12.278421401977539\n",
      "step 726 : loss 12.065377235412598\n",
      "step 727 : loss 11.954636573791504\n",
      "step 728 : loss 11.931702613830566\n",
      "step 729 : loss 11.96191120147705\n",
      "step 730 : loss 12.053382873535156\n",
      "step 731 : loss 12.163908004760742\n",
      "step 732 : loss 12.304156303405762\n",
      "step 733 : loss 12.25342082977295\n",
      "step 734 : loss 12.174165725708008\n",
      "step 735 : loss 12.021953582763672\n",
      "step 736 : loss 11.934354782104492\n",
      "step 737 : loss 11.888500213623047\n",
      "step 738 : loss 11.894816398620605\n",
      "step 739 : loss 11.950037002563477\n",
      "step 740 : loss 12.09785270690918\n",
      "step 741 : loss 12.281068801879883\n",
      "step 742 : loss 12.509278297424316\n",
      "step 743 : loss 12.402949333190918\n",
      "step 744 : loss 12.239587783813477\n",
      "step 745 : loss 12.0054292678833\n",
      "step 746 : loss 11.881908416748047\n",
      "step 747 : loss 11.815618515014648\n",
      "step 748 : loss 11.796285629272461\n",
      "step 749 : loss 11.805651664733887\n",
      "step 750 : loss 11.853672981262207\n",
      "step 751 : loss 11.937139511108398\n",
      "step 752 : loss 12.088052749633789\n",
      "step 753 : loss 12.172493934631348\n",
      "step 754 : loss 12.229203224182129\n",
      "step 755 : loss 12.068568229675293\n",
      "step 756 : loss 11.939836502075195\n",
      "step 757 : loss 11.824935913085938\n",
      "step 758 : loss 11.77619743347168\n",
      "step 759 : loss 11.768610000610352\n",
      "step 760 : loss 11.819746971130371\n",
      "step 761 : loss 11.928603172302246\n",
      "step 762 : loss 12.148515701293945\n",
      "step 763 : loss 12.302145004272461\n",
      "step 764 : loss 12.363683700561523\n",
      "step 765 : loss 12.124506950378418\n",
      "step 766 : loss 11.908218383789062\n",
      "step 767 : loss 11.749887466430664\n",
      "step 768 : loss 11.671960830688477\n",
      "step 769 : loss 11.634288787841797\n",
      "step 770 : loss 11.617220878601074\n",
      "step 771 : loss 11.609601020812988\n",
      "step 772 : loss 11.607721328735352\n",
      "step 773 : loss 11.613245010375977\n",
      "step 774 : loss 11.63381576538086\n",
      "step 775 : loss 11.683760643005371\n",
      "step 776 : loss 11.790238380432129\n",
      "step 777 : loss 11.954319953918457\n",
      "step 778 : loss 12.147628784179688\n",
      "step 779 : loss 12.130318641662598\n",
      "step 780 : loss 12.003071784973145\n",
      "step 781 : loss 11.79180908203125\n",
      "step 782 : loss 11.658891677856445\n",
      "step 783 : loss 11.584354400634766\n",
      "step 784 : loss 11.548406600952148\n",
      "step 785 : loss 11.53040599822998\n",
      "step 786 : loss 11.521175384521484\n",
      "step 787 : loss 11.5172700881958\n",
      "step 788 : loss 11.518538475036621\n",
      "step 789 : loss 11.529592514038086\n",
      "step 790 : loss 11.562494277954102\n",
      "step 791 : loss 11.650498390197754\n",
      "step 792 : loss 11.814094543457031\n",
      "step 793 : loss 12.101304054260254\n",
      "step 794 : loss 12.149895668029785\n",
      "step 795 : loss 12.136066436767578\n",
      "step 796 : loss 11.912212371826172\n",
      "step 797 : loss 11.918943405151367\n",
      "step 798 : loss 12.050971984863281\n",
      "step 799 : loss 12.351137161254883\n",
      "step 800 : loss 12.207235336303711\n",
      "step 801 : loss 11.953304290771484\n",
      "step 802 : loss 11.659794807434082\n",
      "step 803 : loss 11.505546569824219\n",
      "step 804 : loss 11.439432144165039\n",
      "step 805 : loss 11.422447204589844\n",
      "step 806 : loss 11.436346054077148\n",
      "step 807 : loss 11.481037139892578\n",
      "step 808 : loss 11.572992324829102\n",
      "step 809 : loss 11.70378303527832\n",
      "step 810 : loss 11.881460189819336\n",
      "step 811 : loss 11.921414375305176\n",
      "step 812 : loss 11.922222137451172\n",
      "step 813 : loss 11.769972801208496\n",
      "step 814 : loss 11.72937297821045\n",
      "step 815 : loss 11.732877731323242\n",
      "step 816 : loss 11.842206954956055\n",
      "step 817 : loss 11.831369400024414\n",
      "step 818 : loss 11.801241874694824\n",
      "step 819 : loss 11.62273120880127\n",
      "step 820 : loss 11.502786636352539\n",
      "step 821 : loss 11.414384841918945\n",
      "step 822 : loss 11.369436264038086\n",
      "step 823 : loss 11.343353271484375\n",
      "step 824 : loss 11.328789710998535\n",
      "step 825 : loss 11.319668769836426\n",
      "step 826 : loss 11.312943458557129\n",
      "step 827 : loss 11.307286262512207\n",
      "step 828 : loss 11.303013801574707\n",
      "step 829 : loss 11.301634788513184\n",
      "step 830 : loss 11.306135177612305\n",
      "step 831 : loss 11.323586463928223\n",
      "step 832 : loss 11.375628471374512\n",
      "step 833 : loss 11.493906021118164\n",
      "step 834 : loss 11.752899169921875\n",
      "step 835 : loss 12.012701034545898\n",
      "step 836 : loss 12.206585884094238\n",
      "step 837 : loss 11.856813430786133\n",
      "step 838 : loss 11.58505916595459\n",
      "step 839 : loss 11.40638256072998\n",
      "step 840 : loss 11.379722595214844\n",
      "step 841 : loss 11.462379455566406\n",
      "step 842 : loss 11.692505836486816\n",
      "step 843 : loss 11.84842586517334\n",
      "step 844 : loss 11.925838470458984\n",
      "step 845 : loss 11.599447250366211\n",
      "step 846 : loss 11.382109642028809\n",
      "step 847 : loss 11.2569580078125\n",
      "step 848 : loss 11.2120943069458\n",
      "step 849 : loss 11.208638191223145\n",
      "step 850 : loss 11.237104415893555\n",
      "step 851 : loss 11.305110931396484\n",
      "step 852 : loss 11.413569450378418\n",
      "step 853 : loss 11.562193870544434\n",
      "step 854 : loss 11.622350692749023\n",
      "step 855 : loss 11.601795196533203\n",
      "step 856 : loss 11.475489616394043\n",
      "step 857 : loss 11.374667167663574\n",
      "step 858 : loss 11.359966278076172\n",
      "step 859 : loss 11.429384231567383\n",
      "step 860 : loss 11.642374992370605\n",
      "step 861 : loss 11.79456615447998\n",
      "step 862 : loss 11.878557205200195\n",
      "step 863 : loss 11.602136611938477\n",
      "step 864 : loss 11.389698028564453\n",
      "step 865 : loss 11.239758491516113\n",
      "step 866 : loss 11.178627967834473\n",
      "step 867 : loss 11.16016674041748\n",
      "step 868 : loss 11.172479629516602\n",
      "step 869 : loss 11.211055755615234\n",
      "step 870 : loss 11.277846336364746\n",
      "step 871 : loss 11.36985969543457\n",
      "step 872 : loss 11.43991756439209\n",
      "step 873 : loss 11.468757629394531\n",
      "step 874 : loss 11.42231559753418\n",
      "step 875 : loss 11.348249435424805\n",
      "step 876 : loss 11.310798645019531\n",
      "step 877 : loss 11.306912422180176\n",
      "step 878 : loss 11.39633846282959\n",
      "step 879 : loss 11.497353553771973\n",
      "step 880 : loss 11.641210556030273\n",
      "step 881 : loss 11.570222854614258\n",
      "step 882 : loss 11.466928482055664\n",
      "step 883 : loss 11.287382125854492\n",
      "step 884 : loss 11.179774284362793\n",
      "step 885 : loss 11.109247207641602\n",
      "step 886 : loss 11.074850082397461\n",
      "step 887 : loss 11.05547046661377\n",
      "step 888 : loss 11.046777725219727\n",
      "step 889 : loss 11.043867111206055\n",
      "step 890 : loss 11.049527168273926\n",
      "step 891 : loss 11.064371109008789\n",
      "step 892 : loss 11.10123348236084\n",
      "step 893 : loss 11.157256126403809\n",
      "step 894 : loss 11.259430885314941\n",
      "step 895 : loss 11.329263687133789\n",
      "step 896 : loss 11.404167175292969\n",
      "step 897 : loss 11.319426536560059\n",
      "step 898 : loss 11.243884086608887\n",
      "step 899 : loss 11.134374618530273\n",
      "step 900 : loss 11.073348999023438\n",
      "step 901 : loss 11.03091049194336\n",
      "step 902 : loss 11.014535903930664\n",
      "step 903 : loss 11.01386833190918\n",
      "step 904 : loss 11.043325424194336\n",
      "step 905 : loss 11.125496864318848\n",
      "step 906 : loss 11.313491821289062\n",
      "step 907 : loss 11.63806438446045\n",
      "step 908 : loss 11.837239265441895\n",
      "step 909 : loss 11.822171211242676\n",
      "step 910 : loss 11.426040649414062\n",
      "step 911 : loss 11.22951602935791\n",
      "step 912 : loss 11.167314529418945\n",
      "step 913 : loss 11.23136043548584\n",
      "step 914 : loss 11.248741149902344\n",
      "step 915 : loss 11.255605697631836\n",
      "step 916 : loss 11.145562171936035\n",
      "step 917 : loss 11.065631866455078\n",
      "step 918 : loss 11.01910400390625\n",
      "step 919 : loss 11.034631729125977\n",
      "step 920 : loss 11.134589195251465\n",
      "step 921 : loss 11.27785587310791\n",
      "step 922 : loss 11.473963737487793\n",
      "step 923 : loss 11.425761222839355\n",
      "step 924 : loss 11.322759628295898\n",
      "step 925 : loss 11.120546340942383\n",
      "step 926 : loss 11.004087448120117\n",
      "step 927 : loss 10.93330192565918\n",
      "step 928 : loss 10.905793190002441\n",
      "step 929 : loss 10.899660110473633\n",
      "step 930 : loss 10.913938522338867\n",
      "step 931 : loss 10.949541091918945\n",
      "step 932 : loss 11.015593528747559\n",
      "step 933 : loss 11.109842300415039\n",
      "step 934 : loss 11.210932731628418\n",
      "step 935 : loss 11.258182525634766\n",
      "step 936 : loss 11.235782623291016\n",
      "step 937 : loss 11.137411117553711\n",
      "step 938 : loss 11.068955421447754\n",
      "step 939 : loss 11.021705627441406\n",
      "step 940 : loss 11.055253028869629\n",
      "step 941 : loss 11.119230270385742\n",
      "step 942 : loss 11.240395545959473\n",
      "step 943 : loss 11.249589920043945\n",
      "step 944 : loss 11.224630355834961\n",
      "step 945 : loss 11.075321197509766\n",
      "step 946 : loss 10.967931747436523\n",
      "step 947 : loss 10.88056755065918\n",
      "step 948 : loss 10.834834098815918\n",
      "step 949 : loss 10.807169914245605\n",
      "step 950 : loss 10.793231964111328\n",
      "step 951 : loss 10.785371780395508\n",
      "step 952 : loss 10.783862113952637\n",
      "step 953 : loss 10.787901878356934\n",
      "step 954 : loss 10.803842544555664\n",
      "step 955 : loss 10.834772109985352\n",
      "step 956 : loss 10.900238037109375\n",
      "step 957 : loss 10.987967491149902\n",
      "step 958 : loss 11.12272834777832\n",
      "step 959 : loss 11.17442512512207\n",
      "step 960 : loss 11.207815170288086\n",
      "step 961 : loss 11.112421035766602\n",
      "step 962 : loss 11.080903053283691\n",
      "step 963 : loss 11.07489013671875\n",
      "step 964 : loss 11.20759105682373\n",
      "step 965 : loss 11.334470748901367\n",
      "step 966 : loss 11.37868881225586\n",
      "step 967 : loss 11.232763290405273\n",
      "step 968 : loss 10.991262435913086\n",
      "step 969 : loss 10.830361366271973\n",
      "step 970 : loss 10.741631507873535\n",
      "step 971 : loss 10.708666801452637\n",
      "step 972 : loss 10.706689834594727\n",
      "step 973 : loss 10.732854843139648\n",
      "step 974 : loss 10.794341087341309\n",
      "step 975 : loss 10.919473648071289\n",
      "step 976 : loss 11.069234848022461\n",
      "step 977 : loss 11.235251426696777\n",
      "step 978 : loss 11.17912483215332\n",
      "step 979 : loss 11.063931465148926\n",
      "step 980 : loss 10.879904747009277\n",
      "step 981 : loss 10.769491195678711\n",
      "step 982 : loss 10.703508377075195\n",
      "step 983 : loss 10.673416137695312\n",
      "step 984 : loss 10.660051345825195\n",
      "step 985 : loss 10.659228324890137\n",
      "step 986 : loss 10.672430038452148\n",
      "step 987 : loss 10.712248802185059\n",
      "step 988 : loss 10.79155445098877\n",
      "step 989 : loss 10.949699401855469\n",
      "step 990 : loss 11.089728355407715\n",
      "step 991 : loss 11.209151268005371\n",
      "step 992 : loss 11.033449172973633\n",
      "step 993 : loss 10.881680488586426\n",
      "step 994 : loss 10.746256828308105\n",
      "step 995 : loss 10.691522598266602\n",
      "step 996 : loss 10.686128616333008\n",
      "step 997 : loss 10.743413925170898\n",
      "step 998 : loss 10.866841316223145\n",
      "step 999 : loss 11.089405059814453\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder()\n",
    "autoencoder.train()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    loss_iter = 0\n",
    "    for image in dataset_train:\n",
    "        y = autoencoder(image)\n",
    "        loss = (y-image).norm()\n",
    "        loss_iter += loss\n",
    "    loss = loss_iter/num_images_train\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "# result = autoencoder(new_init_tensor)\n",
    "# tensor_to_image(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDOooor4k+2CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACitnTfD814iTzP5ULcgY+Zh/QH1/StuPw7pqIFaFpCP4mc5P5YFUoNnFVx9Gm+Xd+RxdFdfc+GbOUZgZ4GxgYO5fyPP61zV/p9xp0wjnA5GVZeVb6UOLRpQxdKs7RevYq0UUVJ0hRRRQAUUUUAFFFFABRRRQAVs+H9NW8uGmnTdDF0B6M3p7gf4VjV2nh2NU0aJlGC7Mze5yR/ICqgrs4sfVdOj7u70NWiiitz5sKgu7SG9t2gnXKnoR1B9R71PRSHGTi7rc87uIHtbiSCQYdGwff3+lRVs+Jo1TVQyjBeIM3uckfyArGrBqzsfV0KntKcZ9wooopGoUUUUAFFFFABRRRQAV1/hm5EunGAkboWxgDseR+ufyrkKtaffyaddieMBuNrKf4h6e1VF2ZzYug61JxW/Q7+ioLS7hvbdZ4Gyp6g9QfQ+9T1sfMSi4uz3CiisfWtaWxUwQENckfUIPU+/t/kjdi6VKVWXJDcwdfuRc6tIAQViAjBA9Ov6k1mUUVzt3Z9VTgqcFBdAooooLCiiigAooooAKKKKACiiigCWC4mtZRJBI0bjup6+x9RWpH4mv0QKwhkI/iZTk/kQKxqKabWxlUoU6nxxuadzr+oXI2iURKRgiIY/Xr+tZlFFJtsqFOFNWgrBRRRQWFFFFABRRRQAUUUUAf//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACr0lEQVR4Ae2dW0okQRQFbR97FFyDq3ENLsVV+WM2BU2DdEJ33XMDssIPEQsykog6znw4zOnn/eXJD87AM4eWfDZgAPg9MIABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBcABXln+7+fHrQu8fX3ferTS90/If2U48f5f7tolugPcpf46xqoZ+gI8rH7tDE1/CJfYHyWqzrmOyn7dEaDWWu1prP1BjwdI+EqcSZXIBsiZyp3cXCIYIO0ofX5PiVSAHjs9lGiJVIDopVc6PBKg88XsZCXCRwIkLrrqmfUB+l/JfmLh21AfoPByRzjKAHDl4gDUTwOKu79ecYD9FzraCQaAixvAALABGO8CDAAbgPEuwACwARhfvADql0co7v56xQH2X+hoJxgALl4foP+nQT+xMFp9gMLLHeGoSIDOV7KTlXghIgESF131zFSAnhezhxJtnwowLp22kz4/6v1yeDBAtMEa9oeibIBQg2XsdwQob7CS/aYAhQ0Wsz/M9P0TpQEbHw//+sJ66jch3QE26l0ZVlVPBtjY4/OkxNreLwaYBVzwfhH/a6iK5wYMMPcTf2qAuOI5wABzP/GnBogrngMMMPcTf2qAuOI5wABzP/GnBogrngMMMPcTf2qAuOI5wABzP/Gnf2EtVPvAuvARAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAdxklEQVR4Ae2d2W4kOXaGY0upqqd6ZgyPL8b3fgC/mV/DT+g739gXBgzMtDHdLSkzI8I833dYyaiUujZVpQGHADGY3Jf/bCSD0f/rv/xz13X/1P+puD8+/Udxh3fn4v4wn4q7PIX/MM/F7e+n4o7DUtyh7yMkvN06rOHOY4TM4S/PxsX7SY65Pinpd0u00qF1HUqNA/1aB/p+CvfUxcgMT4finun78RzhT3cRPnUxQP1dFDGW3F33b7/eF/fv+vD/+8+/FDd8+98NR2D68X9+LtW/Gx+Kez//pbhnJmV+COyvy1Nxu6dA9wQFDCOzGqFdH0m6w12461NkE8MDKBhASomJ6Gf+TPtS7DMZXjWI2l+o/Dr4tEZ6e9TTL/u4BFPohsdjcc9wgtMpco+k74MMuvExeMn9Q1DJXxmsP0/h/8O7N8XdKaAMwi3/pj/dvy31/3F5LO74a/jPcP/115ibtY/57B+Cc01vImSYImRYYP9zhIxvAh3LMfxLOMVPmvG3MW5spH+ZSoz9dm7bBmuJlifRNs0X+8C6OyP/RpC+3keH14B4dz5Ehhl5Oa7BM8YlSptJ0y0R++Yh3KcxMvy8/q64DFh57n83GoHp9+fg/ucuJMEkf5/g+8ju4RBs/m6Mmfzxd8G5pkMwthHlYIACDlMwwvkcNDFPMcMLiDgECLZ/wCsFhf5t/E1+fUqjEGop4mY60Afb7/oDnAAesNL3p0fLi+j7mdj74B+HMsZd949/iNF7OIf752OM804BZRBu+TfNzhL4PaLbLueA7nqKWVKRwds9RkDOWI9sGE8xwxNzezYv8bDBbpJlRqYX//7PkMGLLTTiRH/1L/aL0dAS6uHvfRmJrjvOwQOkmAnLYF2CGlZG+KcnBzGo4XwX47xTQBmEW/5N/RBzcsa9g4v9Ae5/Qh5MbyP2QPiPbwKvK8rtgKIwDDF/I9L/BLGcsfHkkhPaQknwwd816rU2Aznf+Y8qt+3hl0E2CP+APJMMZpprkkWMMxrjGjLyDmq4Z0wc1V+m4BOY0t3DQ/jvp6CJAS1xp4AyFLf8m1Z09vUcM4pw7hAB3YxKP/KjV71H5xEWA3rR4LTC1hZkQNfD10BB6s5N7xpIFUhEhDgyvEn4Xb1Ze9uIJEmCaKIUb7NYDCtrBPyCAhbk6AA/X+y7Y5VmUaB8pcwRnrGS5m/E7hTwXSf7urJpZsVOi1dON9yFjO7h4Hfwsjdovm+xgcWs6hGMsYiIQMo55zyqkPm7Yhq/n/vLcp6L+s5hSYUNBajmZIB2L0AV9Gfo1j5i6hZ0R1rJZmWN4EjEEwbVfB+yoSfXHZJyxH0Dz9gp4DtP94fVTVPL74hVx28VVK2BBiIubKTbsQq4wBrVkaWetHg/rDF/i7tE3wtpvk/wBu/g9LpVK4BfiJUOlAr6VwTCwjDB3kvDY7SkEhTDMpyEAHhlatDFbgcwCLd0pgW9flqC72u7Dli2I9Pn6tDIzB9c+VNmqBfL95nKmalf0IhcBx/G3+JvyTHpe0tbNxsMGyH46W+lYCLoykx4Skq4/AL1G64fc7jYVZEh19bQDCf31LCTx1MM35kh+K0xutlY/H+qeDrcxRyMruTgrnI6sLAyz6K18rur4ZHtqQlsIgM75k00Jb4iUQu1JniT/5v/aEnvuhEZEolyzQeu0NMlXTk7ZF8WgaO9FqlW2WE6u2eu/Jg1H7C6tKt3Cvjms/zbFUzTXczZAH7Pp5iPmd0c9ZncHwbjc9nZL8gFBZrAvXwNebCwspGx4LtXBoijBMalMRvAtWkuSb6l7yM1blpHr6Mx8gD1wwX5B0vvZtbNViJmd4YBtnrRCTXoBG2cGUO5/7IG5ewUECN7w7+JRZ2yexkQPbM6oZbqOk+1CZgnsc9MVgoA2EaiBacFQJpemmix9iGwbtjxj1ct0Zou+6VBpPkrf8c/QAgL6HaVrGwZlowrlOHJCUeMpaOC+ohd4Bw7BXx8Jr5pikn5PjO3Z7n2FFMD3Mscxi5P6j+kWUG3ig/eSBtpEh3FW+REuJkmvFU50I+7IQZ/tJBrUn6Vd1MNJVlLU2Pj3VRlTw2qxcRTLp9JU5WJ8B6e7phMAHtgdWAewsZakAQ9Byj6mdU2ku4UkCN5q8fkno5q+YL9tiCZldQnIjwD8cgML2jBTvbgijYoOAJ4d0c7OGPyzRZgFUilt4239v2ZoBr1is+2ltb/TBWXaGlaJV6/KzyOQ9oBykipH6ahsHCHwD2AkWwjtvGRlDsFPDPw3zNoyp0d+NGkiySYRTFt0TZWR8odUWbY9Q3R7D5wFReRLffLgJH2cMt+7WRizIfRRtzAjUa83ISIURbKAzYLXWTrya3sBOJlHzgiPEtajpUXv3hni727gxp2CijDcsu/ouQkCptWXELEvlGX0JrU2Da89ddchgUW2tjWX8v79Ocn575UXgoHqFaS4R+W0/5uUpeWX2L0XYe0rb+kvvS6Dbuk3SngMhY38cn2L9P0YSOYtlzRzKn3AXfDm0jJRPzKXD4oUn35kvW5Go39sAXP/W7BaXxbcptjk7KpYBPeZvgS/28Xdh3rYKQJ8SUV7nlebwTC0C1/17Nk+NZtEARP1Fb0xEstwzSXlGrNWc5VNZd025peSt+magvb1NImanrWpm+T2Aa1l4bVpz3fhrS5Wn+W0AZd+WUQ1+3cZcDVUH3fAFXSCl/qbpGSWm22yZjLfPtbi7fNJewy1rwfZv0NYF3Kd0epHZAmrgRTKEFt8W36ragx1SXe0i6/a5GbxmWmeGR6mXfS1jNlvC/QuJqirf3i3yng/XDdxsNpt/eTy2S1Ok9yrrZtzbRuUmYa5jYnXdRYaFPEVUATV7wXdMiGc3/VROZtkmzyXodnSyJV423ryAKMzQKuy9lU0/4waVP2dV5DrhJayk4B7WjewJ9aUEIi54pHOvFIppfTbCLbqp8IBLzrITXO2Da9MQ1erDiTtOFtyog2ybUWcQ1mS8n0FpNuG7OJeF/+R8gk20ne9F+3+VKySZ5rySXNTgGXsbiJL2WA87jh6QRlOMBotX7PAydeaqLSgVxbZdnQXVB5r0kyfQOJfA85oxkBYrMlpiQ4KzGiKcFRk/KSPkia6a8g7SnlrDDLj0fWaHHk6htySzlkyU2rMnltxOZn/ZF18bjOulNAHacbPSchUmcJLNgUZztdZq71i/F0jb2kSdi5fJ4mZtO/BmxVZmT9l0QUdvn53kf4dZHSmZk8m2SILXmpsFqqtbepwn8dWtPHs5Jik8vxqdLqfQltrq0/8u4UsB2T7/6ryABnOtzqj1YY2ranDWnYY5ukKCsNIlp/m8okbXHXqozpm8KyRVS8ydqUvEn+DBLNF6nalDX0utRIdR3aVPgK3p0CXmEQv6aIYgeABidafMm7N1w+qsgzARlLLk5C5AnqJldasMbaug3k2h9N4w2mJdkc8KfeYjvrymvkyvA2fVNClmuPMGRqrZEhz/xYDUkbbw5JTZ8lXR4m3URvcl9SfoJvp4BPGKRvmaS8Ihazl4qJE9mEeAKgujFbC2cDtAO0kHXzvJDCgRMVeVaO1kse71WHqx4Bp4QR7SGFJ+s9id+w9MrBs7WRNOEonLIci8CVk9s2AvLMK1JKangG0LkAsImJ3FcB1vFl7k4BXzZur5ZrawlfyXwnW5uw5bnXIEiuSsNaTAm7dFtsZheipAyuqvWHIaSUDpRDHOCr2a7pwPS4nk1r33NPANcqSRVOQx4V4gY919XIQPgzHYq4z/jbKeAzButbJK0UwFQKwXStzfD0x6QLiMrThcEFB72WXWodpo3M6UuhYXG4cOGW2barLloVKQPMlEkvNWawheFu4pRnTaK2Lttl+gy3hEvDS8szvsRsSrbMr3Z3CvjqIfy6Ajb7AcnHtWAFd2vNtriQDftOgPvKcsyRdwt4T2og3DXRKgMCQ7VIirMEKQQQGqttUS5njN61p1S9rdGWNJZHvrMInOppV+oStRSamluO16Uzz6xHXUBf9UNzXTK9LyWSmjyDPvOxU8BnDthrJ08ZkJPIVCoDNiGbWgMGznlKgow1HJBYDriWqqp0iQjz1iIbBDSKiNZJrsgnpMnrW2wt0im6FQ2WX0OiPfLx2gZrjlR5F7QtouHZuMafXh+10Ty3/dhEfcaPpv+fkWtP+mojUGUABW6xH5OeE+9k55THI/k40RnMVMrSvTUx1+VFK65aeXJ5ys43Kak96+LhWyXeZycdJH6BaJ7NFzy+ngIxeip/sBopA78CK9eRTGntkMnGaqEl1aFntCfbViPK8zqkRuZ41J8fedqJjyTao7/dCFQZQA1VH4jZFae97/YxTa3C4o1ZPReHeP+Tb8uMfZCU78pOaERqQd4QmPeDUFrqJIAzMSioQKVvWFYKALRgeeZ2x8kbikQ6frUgXrwtNq2yKZBYjdkLlj3ikRIie81j44jiD1H+edjeFPjij50CXhya7xNRZUCjgVT9gQYInZx6KAMuLH/Pm/S9/xKS6UF0QpmbYeXLCy/O9ugz3rfjzQpdhlNyEl1U1ie6CedOQi2DkXtGpEW/1rHy8vnAvbTyek0FrQflgW80GpLWNU30VGtKvs1424NNULTqw4BX+L1TwCsM4tcUkW9Juhss0NPlITf1RijVDWOTR0Ms4tFrQ1MLAipp/RLEFVHljqho6gjFJJy0oiku3ys2L33K9wut0nBSLtSbehG0Ivf3iIf3MWjC+k7nAOVZjJo+gqO8vxjVpP3/DCFElU3ltKk6L4XX+E997hTwqSP1jdIVLYh5Trhe/AkIwp3t1l3IpY50gnf7XQllANcOldsjYnbVgp7wxw0JhZNKJoQMaEreoyAW6mpo0F7dt4qa65potNC77fL2KWzjCTmh4PDd/7wDA7L1RhhjDW/hbr9KseXP8JQZhER9JZxHm5LgpA/9X+buFPBl4/ZquQoFxLw6t9VPCLh25lNzoFJVFd8BFxjeod7nbTnBjf2G0KgdAJefDlAD9Zi+fJKopBz4+sbinROKBZUw10G5v7HnW109Iat4F6h5YUW0dsA+GMjbe2kjDV0AvPaNLacTH3GSPgQ/aW1UDlMT/pGCPiF6p4BPGKRvmaS8JxwTmpIg97OY4gxnhkQWXi6UK/cIGR7uyr2LYvN8CMNi4SbR5R7dBOzrF4/lOs2SRmkx810tJUpSmwV7kbuSBpHSo973TyEbvJ105UK79RghQ3wEqvxBu4oaFLj84h+CyEVVZUmSUPbxfdaSP0rYQDwCUgaQ7pUduvvKZe7FfcYI5Ono7Zz76xIGCBIYqa2r4UAx6jALJu8K3/cuwRH+vkIB50MQxQwf1youC0YlZL2Lz/BJfxRZwByt7/k6jbcy5ooQS6yp0YPxMxKiozK/75dnk5Qfqb/Ri0tXqp6T6k7UlX8mzK4Spr/JW5O+5nOngNcczS8oK++KcB7qOyFOerj1TECUXEETwOi9FZCEq7cjqt1zC6n3SM98SKznxunlh6hhRnL0hK9SDN+nXHPNleKkgCMKPBgcNLL5MJEazgGT+mQbxCmmtre2eTeVeotrVvYuv/pKJWkNwPF1gg793/Y6fkWE1eCP3xs5YRzBn+3Yts/Otmd4rREoWnrMar7tjlZgiPul6uyunDSKTyjwJdfMVeET/hV/P8ZX49Tx76fg7z06z4J2lDtW+FdzIRvSElYIWBmKTl++1xtY86N10AQXrU0ueNKgeYxw2zZiJeSJUgxf74ZXckii6lp0tKJYWCeM43HBefnx0t/XwL4pc6eAZjBu4c39gFwRpAXOf4sRoZlUwpSlHQCoevi+sStfXPV8XP8Gff8tGd5R0QHYuE9goaRxWTLvmnaHK7/ICHXypeKOr5e6h9x74SCywXUbDOFOd3miB1IzX7VMLo9l4MVvyjatBFdP7XXC2s6385GD0gRtMjQDl/RzIZCURk3WKnUiaKeAdmBu4K+roVbdwp6ZdJqzXQ0uXAuSbrw3XT5eFvtL4hV0y/cH8L7wJaH1DjtWP3fydm+xAxAOlsOSUjESAhmplfGVpjX1Ir7L6LqQd5a7yYC1ke302lqZvXsPiA+/dZOQkxpaFDd+7WFlQa7IbnAdSdNmbsckq7882tFrinfxIZPtFHAZr5v40hL2omf5lucVVHzzq0gwrVxPp5nyTW+cVov3lnyxr77vWr8KyhFUnsByjxZUPkxcSlqhhiVPV4AGtHuN2Vz/yeWnaN2gDQFnP3tnOdRmS7y7VHs4+SxdsuV+NjZ3hglX4bLXavqp7tNHwX1lACT2SZLmgSWI5dZyMk3r5rmTrDJidgpox+cG/nouaMOuhEcEqWOkZaDOw5TVNf3g+CP4FcWuBSkJxjEw3oP9AzbBiirUT9gKrhThd+9sTGU+ah9Y6czv9Lq0BB0MI/eO4y7IgNxX6CN8YCegT3ODXtAB35BJOwDS6FGGUuQ1eJTVt4NRiv3Mv5bbkzWrwd/WRcBOAZ85vK+d3FMJdanDSWKW5H15MDkn1Yj44URmmia9fHYOwih2ckQ4w+4YC2X3hJUWalP5DTqT4nraLqGjFoRWA9kkXfq1gwXl351nDOGyFx21Z17LpP2556FaniGRMnehw1s7Zn8N0f9SiDslbaxj0+SqkfF03HL1gR7ZQKva3RuMQF0Lsuo6WeVXen00P9pVo8510ARbJEr7oFkfda3UPQA+Od2tajK5RhTEkl9gUvdHC/LUm59ZGdSCQM/iCiuSawX2C+VomeepUG3slFg03RYmNpsu1SVTeg86s6cEtH4CriVE2v+JbHOZTaybLdwPf9eYnQLqSNzoWfSSmJsGFe8bch1+mVvtw3oKgSxEptaEDFAZSZVEXCtxtE7x97qEcLA6VJlSXOpLnINzfT8/SYMelXRmStC96LZaNZCrUirKlPPWkGhzSgU1ogjYyEJ7S7CDlK4mdg2/QjYVtHmtURLIGinCnDsF5Eje6gGiovKrmUyqaMPxCwC9uGn7baAVEfUWCvwao7keGQ+/PN3lW4/gwBUbW5LpyZB+Kmv9bY2WLPpImB1q/dHN939iNKKrdar/fYIXPeZMjLdQzxyXkq8H9TrXTgEvDvT3iZhSEUDTyFUUJq6uCDYYgSaU+9k4qUHkOrmuv8vlLfoICDm9k6uklu9pTtIsEzQhT0daTA9RaK4FcUKio4SZr7T3T1HmwjrozHbZwm7BipuHsGmVJ4h8S4B21FuP+JEhzUgbktIiw6NjyffpYz0ZHtHtalKim97lfgN+FxuyLmhUv6bKTgE5zLd6FApwhpkVT1KyojK4M6U+A/YzBH1j9FiBi5bYou6C+TVuxceAAuQ6zMgyf35vlPM8FWuBABE3QAEqMpNnflZWfvTjDuwSl69eRi7Xf/hOXVk8ihEkpMsVoQhZ2TFezuH3rYLFvQSlhXcfNZitxBytqxZP8eYZ79betp2egtX2TpEEpP168Kp+5Ts8vh0UhaXds2Dh7xTAkNzOmU6cPe5OYIpv5C6cy8yb4Nh3zbObWJgid4UL91Mgxp3VhRn2ZI6ccWTdX1hPIpRTnsMpBMUCr1+QGWtjDbgjNj645B8Mvj/jP0YLDV9/jXpX2H+PtBiJHexFuqDZV5bRnfKUdaNHKedS2tH+7B0PbR2N6M07z2hcpqyFxa/RhTMjkG2wjzJAkUEL6QC1qbIdIKKdAsrg3PJvWkUis7Sgmczwd3eXnE51gCWBHWGec5bH+cF131eZPbMvMPxyKIrIzEJougHlsjEVCJU7ax/mDjBVrqL+3FAA5axLqEErsmFeItYaz54gUno1EsK29fRIvqwkSFSW/KU0ZUDqJ0hEJQTglwLSxGYEHA3lgTuDnkRy9Vda8VvBmVedUI5PZvWombN+OwUwCbdzph+YJuch94m0TmFUE5N1B/f33JnbUNKKmBrQ0D2tNoL6ZQ4De8ACGADJBC9eHwJfvWcj0E9WTgrlcqcyBpSNT8qkoJJ+Cf/qm8CP8NOfDcf9Jdzp1wgfOTnRmxJXO8C11RG+nO8HlNQhvYLc6ltpEaIu3+o/orhP6ieN/D3hHfW6mjtIQ3AUKcajGvPA7jdD7Mic1L7gEzsFxJje8G9iMqrNKfcEca7CO/EDiF5BcfJQ+NfquQQoRq1J8yBX831ZEq6nfTD4XjwU476xb8rn6qaYosrhiOYzhzvA8Vfk0/CIKfxL0MSADFgfINUHBAv6jy/QrOhySppBvY6SFVKuUynVpAApQ43eRVGtk6QJpkh/voXAj8HeAeOhMZH9lnbIl/IPrc8SBWOltBgxkXcKYGhv50yLL7Eze8FNu+4R3rSA9yPYz2VyV/PhkyopUoMrNmcRqtWHfdBrhYIU31jPL3a7pu/JOPl+AzO1iw5boePb60XhL62SqoZT+JdfkATalg+BMleHIGBhVzJFoepXM2csUoujhWnZyLXh6aXUkr7qfpE3DXPaliumyowSF9pXuNK9J8nHZBcRcS8dk7fyibAKHmnJmVWsc57YiHL2v5uNwHRkVg/Ywyfg96Q1ADmwTNn5xgsLKqWhMcMzBx9yDxbefUbhGMjbq9a4koMtfQLpsy7bXatHJzwLJFJwvc9nAukrFOAX7T3RNp1C91carcgq34uXVhY47JBaHIgG41Jw2vaIjLSBE9GAmSkQ1/ZRnci4ds8jpSDpVXnw5lqTueQlKWkoIq0N+EePKslQpYFhCbt7gxGY3jEHY/+2VI6CXlYBY/60985iGdT0vgMDTg1fOebgMs8ZaTFiAno2LXUndWvUC0+RlgOe0VGoIc8Ipf4dweoki2f/kUyrayxIJpd2+kckAfS68Obw+ci6C9JihZp71rVEYupF0pkSDv+Qa7qiPFyPWtuc1HBI6aqOTD51JOhe+htsCWOFIVSW1mLIXDFbObdxOoXom6DaIy30lNSuBZVhueXf1B1jZh5h8Gf06MNBHTym/qg2DbsaZ856IgPOnvoHcd6ANaetEKV5i8qoBgWcFuyGBWpbhijf3TFXUtVVILNQ+0vsggxwxzjXWUHu6vIplsSC0pN7YTDv1faAryK4SjknMa62o1aOXy0oKYBwtaBzQ4tqQRBAuZ+lFJZv4Mi7FS49JOMZ8jynBLAf4CKe5JjO8Q5Ex3nWu/6H4l1ZF3rijNNOATE4N/ybHh6Dv6kFFcQUvze7jfDQfFcAzr6eI6W6RPcU/kWlHczKc3veffSOxAG/6yoDuHC9c5ZkAJi0kvqGYNOq9HS0FoAIVUGBTDxRMUpbUIa0MqBZq4NJN2uuoUY2tRfbn34FXe6RRfVSYcoh2pN0wEP1SpvGt9V8h1kZkieasKu8FWxAap7gMXnfEe+v3cEJpu5NqXGngDIIt/yb5ruYv7ItEK14DG41wqMT3aKMeZrXiHXNr1/g9XmALbKuhKyFli44Mg3IAmtn7W3oRtpSBkT+kkuwgfdcUwKQqYOn6WlS3JYCKN8ko/oJdZ3cixbp4HtRliS1Rd8HF++VE4YD6azXEOryfYV824AylQ3uDGsPJ03zrtzEWxHDEBrmirL4wzHGUBv4dIyx2imgDMIt/6bfscIjW747xfy8uQtrc8RSfVQnAddyzzwXBL7yTACI874gUeNJTfUQ36XJPVU5OJMuPy3IKHUZm+UTC/OPpZnyl3ec4xeD3pDiUQUWTIu6FtEAV1O67B/EL/He3hEkm89Te25uRCVJ2UojSxKbkqXrPFWlipjcMU5ZFWQyMIhSwMASwoiNcsf4HKdYr10eY2x/fgre8Fd31Itv/7vhCEw//RRIefej/DpwNPwS7XmrMcBMzsDmgHXHtJXfMecj4b4B4Jqlm/95LzTW7witKFHUsoe0ikEN5eTCY/LZqD2tXwjB9Zb21EVqFBDIOXEfLXdz42CrsJ/LZ9JKeO9OBimlCa3ZemYpCFN0Q2ZVzpFeinc3Tb3efcCeXhywbya1r9T6oA/aMKCDzXcxlCcE1MztMAPS1BUh6awk2P9uMwLTT38JWfzu7/9Y3L+5hcp5mxFbdMTm9A33E3I/z48y566eqyZoEsiXYXoFPzG7ygnP+M8kYu+q2BDkg23nGgvZsjTwu6JleY90cvPU68E1KPNGdunP83pcXVHXJt2NohapCoIsLYsg94TVmqy3BJa/XBGSeojw1B7FpJjwdphHbAhvjl9o7cQa1x2dtL9PWFTH5fel5P/+W4zJ42P4/+sf/rO4OwWUQbjl3/8CQgqyLsugCHcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDz+uj0Xwq9/Ctzdu8MLcoqj5nHrz0HTtz+RqDwvpa6jqRkmQPbwDcynkMT90fzP4Y716FXv4iu4vljuehluAjVXtau3RGfBoWl26FEsYCCc/vF3n82yaZdeH9Luk2tZxxkAgNENhGe/HB/HNadFcXtJXvc914ek1y8qt6HnuueHJdKHnxMZrUnBbHKc8A/4+vpxWHXrrosiMjqGRhhlIyCPSvNNd07+zNVlgUYib54v909up6cjn0rvw9dz92W58/mWAVD95T+F/gZtFFFdR5IUUUUAFFFFABRRRQAUUUUAdz4KRRpU7hRvM5BbHJAUYH6n866WuN8FXwSSexd8b/3kYOOo4b3zjH5GuyrycQmqjufXZdNSw0bdAooorE7grj/ABwih7Fwo3kOC2OSBtwP1P512FcD4vvhdaqsCPujt12npjeevP5D6g10YVN1Ezzc1klhmn1t+Zz9FFFeofKhRRRQAUUUUAFFFFABRRRQBJb3EtrOk8DlJUOVYdq9D0XX4NVhVXZIrscNFn73uvqOPw/U+cUVjVoxqLXc7MJjZ4aWmqfQ9eorzSDxDq1uhRL2QgnP7wBz+bAmmXWuaneJsmvJCmCCq4QEHqDjGfxrk+pzvueu85pW0i7/ACOu1/xJHYRm3tHSS7OQSCCIu3Pv7fn78E7tI7O7FnY5Zickn1pKK7KVKNNWR42Kxc8TLmlt0QUUUVqcoUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADaElEQVR4Ae2bQXLTQBQFgeIu7NmQA3BFbsMBuAK3wVWqjG2NZD8r6j+y6WwyGr95X9Utu7JwPn//8euTP+MIfN08+vfbt7WzP//8XXvJ/RmBhwXc4N6qW0YTjcna4gEBDetaV78/HVFDT6btRAI2oG8DTgs1XNKYrb/MrvvLD9JvhXv1tMLXWNwRsC+1fdteXwDBi+h8ahOr7wCOFNf8jCaWBdCM6P4nMrEgoIZOzZTjm1gQcPybfqU7nAuofDArZx3W2VzAYW/0VW/sSkD9I1k/8WgirwQc7eb+h/tRwGDLZwGjPg1GzR0M/n38WcD7jr9LCSigFHc/TAE9k9IdBZTi7ocpoGdSuqOAUtz9MAX0TEp3FFCKux92FjDqyyOj5vYshuycBQwZ71AFDH4GrgTUfxrUTxzMuxt/JaB71Q2cwFxA5SNZOQsHuXXAXMDWHs9tJLAgoObBrJmykUrhsQUBp+k0Hbq/EOBHRy0LQB1I/1LaqgDIgfQv6Z/WtwTs7kD6M/r3BezoQPo9/dNO9C9KE7vNX18Q/SL6aTMSMEU3aBD9DfQPC7jUcFrfeEPI/S73FnjgHdDOTAspz4Bsu7zzV9C2Uk/lBBSQs0KSCkCw5qUKyFkhSQUgWPNSBeSskKQCEKx5qQJyVkhSAQjWvFQBOSskqQAEa16qgJwVklQAgjUvVUDOCkkqAMGalyogZ4UkFYBgzUsVkLNCkgpAsOalCshZIUkFIFjzUgXkrJCkAhCseakCclZIUgEI1rxUATkrJKkABGteqoCcFZJUAII1L1VAzgpJKgDBmpcqIGeFJBWAYM1LFZCzQpIKQLDmpQrIWSFJBSBY81IF5KyQpAIQrHmpAnJWSFIBCNa8VAE5KySpAARrXqqAnBWSVACCNS9VQM4KSSoAwZqXKiBnhSQVgGDNSxWQs0KSCkCw5qUKyFkhSQUgWPNSBeSskKQCEKx5qQJyVkhSAQjWvFQBOSskqQAEa16qgJwVklQAgjUvVUDOCkkqAMGalyogZ4UkFYBgzUsVkLNCkgpAsOalCshZIUkFIFjzUgXkrJCkAhCseakCclZIUgEI1rxUATkrJKkABGteqoCcFZJUAII1L1VAzgpJKgDBmpcqIGeFJBWAYM1LFZCzQpIKQLDmpQrIWSFJBSBY89J/TYVX82nSS0UAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAbFklEQVR4Ae2dS3IkOXKGMx6ZZE332NjINAuttdAJdCTdQsfRfbSSmQ6ghUzSjKTprmJmxkv+f7+DiWCSrOqaIrPaFEkjAgE4HgH87nAHEIjmn/7hH3e73d8e/z7cj//153D3938T7ocmnF03dOE+zEO4P9x/kHsXzm6+1+V+HuVfzuFO8yHchxPJ9m34j/MS7sez3KMId+1Bl+Mi99PQh/sfP4cTGU3h/Pv5U7h/PP4U7p8eFHK6g75V6ced8pzbfbjLTqXMi2goZNfMs/wUvltU4hxU4bZK1cgbyUTjnwkjRreOzZiri2MhzFTkn3SLwhryWajDjqdrGoW3rdyJZMuip/hJVd4dWrXzv33853BVv+13wxbod//511H8T7/7XbgPe3XWfi9ststHuYDnNAl3Hb3dgyMjeiJk3yt2mJXqQWyw6+jX5YO4Z2nU6U0rtyPnqbkP/9CJh+Y74aI5KPbQK6T9s7I4/ElcNX6QezoLuQ87lTLBkXOvujWDOGai3H4RkhbqtoBQoBYFAGBjPzlAzwgo46pf7X+JGxoiTNkkaskHpmrBe0MNG7cJ5XaLWqBtJDnmnWr0oZO7b/WMy49/F+7GAdEIt/z1y4ffR/lDo54YA7vRPx2Itv+gkHkQZne9MD7f02eN0S18DdBMM5T0vKXh6UCenTA+PMgdgdADOZ/vjG4Q2sj/c6f8z5MoT/EX7kjas1AzIFunyYj5OULaSamWmRA4YAe3mQOCNyLWvwZ/A2d4/GgttYm2BC+0XC3lqS1OFKNwl104wCkU34L6YPPwN9GKEcJw1DV6IoaqGA8Uvp/h8ju1Z9v8Vm78b78btkD/h6P6Yb4TXn74Uf0xzQ/hLq3w1TU/hnuHLnFPH94J6LsDPXfXCKcL6JvAdQjmCBnRB9pJvb2MynmY1P/TaIwopO9AyqgcBqR86ipHYWo+qZjzIPpxkKBdQNm4iL7tBMgJCTvjNh4DkMXmA2s8i2W3xyFQvIPGXst/J1pLf+M+E0RZHgPKfblSrXBS+0o+U8nmktEjX2hwolH4nqfoZsmYu/GHcDcOiEa45a8fgQqqdujL6plm/KAa7YXTEPnh9GjfHRDtDBj3JFBoJkm6lr70SNCC+m6ntOaACck+wAH7VuGhuodzj0zvkfXHURzzwX4KPzOiPMA9s2GG5dHAbRbUM2PAntLnVhUyJ83Uc4aocXiCHC5M2Ad5oDXlvG78qyBeQsiZG9PXfGOtPxPRMo15XQ0T9VGJrXmU511IbK7aOIBGvZ3TT+gzPw1CwQ8NI3WPdtHJHTvJL48HUyvMYsAG3uS3rO8ZIULhjxBruwv5jGB8quyABj0htRSw2TIGtKPKbdCjOrSdgyWmsUmqUBqCprVmDQfMxDaLYaYxo9mpzg0skDg19nHjOSI2XYPebobnzYUG39pRDtajXLUFzjM3e3SxrjXSPste9D1PZwbeU9ZE+7TYJRsHrFv43e96I+iAFbfvpcPsG8niPZrGffMbVakTOu46pDbWck9PQhKR6sWFKY9lhxZEDy/I+hltZ8IC3HWiPMRfYBmMfwCbJxDdLQrvsABQHyJTIWiGt0bwMo4KsbWZUy+2OcltZ80KTbzwoug7wi21F7BptBrF4pzP/5SPCV06lUo7P7mKoMVqvzOkznPOUymHhVmAHVJnZvjdOMBNdTO3d6/eYbPd2b51ZzPH8hvmNRuw/4Ex4B5ZfAA05gPPhFjGIY9jnkdZtKRy2gUOaMH7PWgd0Zof4CQKjHkScdhHuKRtMTesokEzgq9hsqyH50QeDKIQ52BthNDQuxVmvDegnopriFC49ZCE/qsckJHklkwkf3odi2ubQ+Ng5G8VrbdfI5xrAgNHYiwkxo+NA6JxbvnrO1DveR7LpgZucHCb8lp9fkCiMbBHqDrdeLfVZ3t4YeWgQdudPN/pOSJrL7gnZH0LB+zRYTyzf9jJ/mCI2fWMIp3ltY0USi+IdskYC9aRUjzDDUZf4YrIs0ETS9BCWeZz9FwFzPI6G3xrB4ynZlXFXNsQJWcTqYaFxgxLzWmHUOkiduOAqjlv4Q0lHomG9t3N6rHFegUIaq1XYMV1CFHZyoF9EIFyVLQgwISI3rmDTWNEu/dtLd9jNwzI9DvjlLR7SulJ7FJa2xCpt0Dk+UULYOv74N3S37ZlzoOijSyMN9ZMbD9T/ZTIPERyisMdYv/KpfCaP9ZpFZ1Ih1MzthqHViMEtSJy44BVM9/gpu97ad9zwywQ+ngDB9jtPI/BgNCBlQ4YtPitETdg1quytgObSYgwb7WeGWX4Zzo1uAcdBuQemDPBgN0dsMb3LBrsmV8yN2SJqFnWZ5pqDaClzrCxhWrUT6V79tQrYqhvYXlUMLboJcCWc7a9Sbgp3FCuGiwUnU9dA5hUiXFzpFmednBrFA6oskYGbGNAtvytLv1oeICaCRtySbtU44FnE7vUItBqAQSMETDTzZKpBIMZME/Yw3OukWl+ZmB0GeCDCezYjj2wqsV0Z6wWCQ39LHoj2qvQLfZ5SnZb1DaUmWNpkfVGveeCOqRwLkRRc8+6NDZVzLtRxuPPNsHj7VMP8OapE/vO4RIs8a9fHYLfqE/bm2iPSbZRWnhl44Cn7f3O9/2Y8ySa9Tx7n0/OPooDHlqtQBmze+OOLrO2kzgFADO4ewD7AzOpZ9B6ZE343Cu3gfFmALMjOtXEateMOrUQsnTYwFjCtlczHLthseLFap0XosyjnllaGKscYlimdWKGtQvec4SIOonOfAxoKzCnVlOhu0D9ElTnA6DJLxxkQ+5EynKVv+sT08vyx/9mB9AIt3RyMn+i5yfs0nHRilWDTTuM0pEaVrjOYKfHFLY87ZCqC1rQwojPctbu/KAeHtjlMDwILyOrvstRutbSM8MKRryiMHgmlb11M3byhJXtddQFHl3gy2Yv/uiwJGav36Vuo/mWmH4KJ/GJLpc8ZKnN2FbioTKpUpbfKmR1IwoHGN2Z4orGc0GQmiMXlL/EO21oJc5ri9sYUJr+Rtd+PEk656opiC7rw8gs+GCmD0d2PEhHCRQj/q3XWxNHFdqNo+KnB+lLluzzUflPJ4VMD+HsRrQaGCZuFBLMEg6bgzTghH9mLnZmRJmwFr3rYia2ZVeBDeQd+lXO51j7BmzWWHKB2PCzq/LKz2IbVGbQNU0N8cR+MoKS1PQu3blBmZFO5UT1zBX0GweUzrjRtWcbcmylUW9NIGKiZxChucN5Yl7emDUHHKwAnZHI7mgs2OOokOmoILy7s9So2N8p9yxVa3dklnxAENoCOA6if6CAI7axx4aRdQUzidfFcl80PJEWO4zg/Wi5z4CnsPbtteucIypqiirBr4Z+DfQSz4PhmNJ6UUK5EOm6GhWISCIuJbEiCPDIZP/GAbTX7Zz+Z3ZBW17fs9P/HDuRtQIMaNkn2qPLH9jb47WwvVe70Do8X+QZ0BN4nHqhdkLrP00aA7zDZ2B97dRr3n9EFx6RiUcw8gnO+MgO6oeztK8HLIkTg8NAWd5j4V0XZa6UMQyL1zuxjXpZ5RLRF7fxEkeE6idgA+6i2hBaO2kH1EErkV9H1DnV4Vd+E1bBGwdUjXELb79nhn2a2KdoTZzdcHMjPhjZ3RaGQPg7bFHvUJs9Y4rdkDuNQfSMHJ/RnbxLbgKDA2lP4H1g5nVEjp/A9Sds4E/so/9I7JHVtNFakOeOnH+uiwk3Mzs5yiyjhKt36awsYevjlJVDgKUz4WVsiKTll7HltlzrtDY8EsrX0HdI7ZZM4pp7iiqu2zigap5bePsJS3ViH7LX8hdWx2bP4dBBE/vjct6cRc3chelU4GvGfzYHnNkdhNV6xgYeJKh3A3ueR2tBHgNI6z2jZ9SmAVvkxKry4LlVZkxTC5pRqmwywGcL45b37kUJUUrOPnpFzDOjaS27gRO7okz86lKgX67Kx+GXkDr901DFXXI2ZdLXwVfJNg6o2+oG/v4j61ALs5LxClZU4YQUth3gedAWHd8kKCmxqgU6zurQe3qeidTY0S+tw3u+RhaFj6wBnOCM3O+fHKC0MyPEGdSfT8ozZ408d+Q1YbDsXXgtFgNDRgxMGplaox6afDOAslIXSoENCNNvEDokMgjgKiTN1ZWtQLj5IBP5Qqoa705Mlra9i6x3kOmvUhGwcUDdOjfw94MndJjnsa4yIGGLbBWiG6Msd7QJBXuv67OaltYptvHgXQjoPH4j7EwXDzDUAP0IEr1/2Lv4R4wIDIac2vH7lyo7sMn8TwrshJQuZS5eBXiePXcowVWuVaEhH3Bqs9SxqQUpMrLAqSBeeSF4zqkRbv/nUim+TrVxwHPt+o5h/YT87UHubtGexfPu9+HOO/kbNJNcyUQ+Go65DsXeBc96ezPQiAY1MkPpacoBicxkaNjD9L+tAbSgCStkJNOFt5EXViAG/BO8WHZCCDeTdxqzNtfw7vnsFWnKtYVcrwFYuUlsAs4inSOz/H0Zcgv1564rnnvUrdaodx6uycYBn2vRN47vP42aojywCuYXeY/oQnuPBKwiGSN+d2WQohTvFSusO0sP2bOfeWGZOOdNUfttM6PO7I5g3JudvbZ1JmfPvx7R1nOcUPZREXiFtd+WmnhfnlfHOmwUW+beP5o6O3DyRK3tc68T1LK+jAEUUyHU9y+6NEHRlKCCn56j/2UctXHAc234jmH9zyDaa7yHT5rN/19mQO9wW7+BRYUa75Smy/ZoGi3rAXvvZoADbAFMrCHPOe8vqBj7A+O/ETLAAbauP2ErDCdVxXbJiQFkZE+GzxlxDVveTG79jhUjgXdZx2vz1FF6k3cTWep6N1z93gBkK+S/CGWTrtwrdF8FrMi/4GbjgC9opLck6f80/U/kv6C53zE3+d+95lvuuo/hNoQUHRzLE7z33nEGivvkAE0SeVVgZBeQmcfz/pPHgKCIH3safC6F+/8BM3faa8/ET+yKODLTeexUk5H5n4lS5ty5JF7xekDZd2Z7Bcsh112D5NlfDdra/yzxmwduHPDmTfx6Af0wae1pWn4r1+u67Y/h94jfYhl4Xsj7Pq1daKwIDLKu681qs3fy4Pr9L1sPnvef2O9m69fy2ic9eOrljDVu6/cTOZz8DkG+SaCyXIfgB92QLHe9wYsKXP9SslcCvnjLVfS1f53+ve42Dnivln6hnP7jSStfyyyJ36O0fwQWw/QpQppFVoLtT8+2+1QcvyvQsoPBw8Rs25XdaqPn4tle4XkeS/x8a56V3uQAMDgxAi3YyUc44Mz67cRc/8wJKV7tagjxHoh6ZPLuM7+xnraoZ6sS4lws8ON5Lr9tDLi0xf9TX+yKkDz3OQ07pmwegMXkeU04gIn3IELXRsfwHnyfCdIyCEze5wOJ087WbWhYzx35bWELXp9qmHOQ8IR3vY0I+zwR0btCsRL8pmPZUy9E2+LNvc3MLBnuOdvDIOYxJu0AG8HfWUdvY8CNO6QfmMnxjnWf5XDy3A6rta5dmcOxxFSfoZSnlWC1e4Y+d1kDckAZ+pXyyJn91DoUVLQa+XMtGlKX5VTW8WfGkhTmzs3yHb9L8fhR3iRQicX6hUgBz/4y12fj3idw44D3aecXS4mdyrJgzQaW2n6zJXAZ4Q1YG30eg3eWIUl9TqDPgnLwBCPM3BjvYSZEDrm2ZbRWc0EeA1JGww6Jd2aBJg8O1o4g8oymxxKfP1LmOJV1sYeNd3DtFeCIe/Z3e+hntTYOeLZ/3i+wP7Ma3LKbszlo7mU66A0Z6+zeW2D5nnvQcm+BsOaRwKcJ5cnJBj/rVgv7R4u2LsiZn+x62ihPFWF20+8o2AT3zmfzUOYc6ePHykFHzi7dbyPnGq+hb3TDqel1WrsOwl95a4p39W8c8K7NfV1YnBmnPvCcpWW6d+7n6QuW7NaIwL5XXPO8M48HuJix5ZSelL+WzrVctiXhEFXG6wGL6a3bUEefDFrOYhNbJcY9HmBhJLoNIdsB+c4zMdfwfiGYAqMqeX3ny8YB79zgT4vr/dZ2quWgIHcVgGvjIt/ooLM8S2odBsg+oh7xTyrPgyZ+s0QDzNiX670OjlzrMwpLHsmL09pVJXxShedlfTqFU5RTHJSDqeUrv+uQEsPVZX2GaJXim9xsHPBNmvHrMwkjQH1gG9Jah8/6LxoLaM3XIQEJGMm9OnTfSo6D3dRz0v515ZQstXUr/2Yl0z9ayhCRwIC0SwC8ZV7Js1qouUPSZSy5BvF1CDmunSTypSp3TfXN7zYO+OZN+ssy7AsqJcGb1Mfxo1EYa6kEMf3o8cAn8uc50oQ7pKxVmVfQXvAWRPkqN+csXdvUmnST80IZzqUk1g3Kf85EIec9VJmkBvF7I9kV/uXuxgG/vM2+aYq+7GU0hqyn4yKXLd/TTS1bfZYjhLsvtXjqBfBSw4HeAt91LlDW9TkOUGLTpGXgZJmYmBwJqIM1nSyxIjX4CSj6VRX7nXk3Drhxh4QdAGAu8Arkg0TCc86yqqQtXoOsglpM4TiLihQ0V8pOiYKy5owSkejXrb3V2FA0e0W4ys+kgr6uVaH5fq8bB9y4b3y4Z2BeyMqTFfxmVo4BGg/SEgZaKd9zfdhB2MDGLI9TuIEgOwnLisj8YU3GjbCiqSkV7XHIfOC4JF9xiTP6NbkbB9y4t/pEt6vh+Uhjiq6xtPU3EW0NFNVD+PNaWAlRFrlPv4YvQM15f5diAU/WucZAuMtaodv0OYyYKIPiUkaCujDH/prcjQNu3Fvx/QBB1CDzDgMjOt93NX6tXRQDVPTAzrguyK2QyKDgfFLbIZ98Vqe9evAqfcSRwLyYUv6SRfGtU1xl+KsI2Djgxt3EyQ+qQ0HVU7/rV8c6pA5/DYlOmRR5UdgqvM6y9l9J/5fqUSf6dfk3Drhxfz1yQF2Pp4hOfaOCn0PqmZakIZtVeo8fBBWr+BJfZVlXoPhfjH4xoqR8k+tblLpxwJt01ZdnuuKA13v4uVjCrKVcYF0E/FUtnEMR7M/l9zSJM/0Syqcpv+7+7Up6KeeNA76up75ZqkcOENYSxHSWe8xuDe7PqEtULHuby8oOIKOSp3NN2m/2QF+R0aoK9bNWNXyR5rXyypO+RrNxwGut8w5xjxygslbd7MK/ICjfUanpKxiV0zcVVIKV6TMZO4eV+2VUqyRfffMlZUGTCh9PUyWy12550royFWkVvHFA1Ri38K44YFWBusPqzs25TNEWEmOh3FURFyJDIklWN6L53n7Vo6yq5vAK3jVhBtdBq8SXm5pk44BLu9zE98gBda+8WpPa5HUiuxUunkn/JTTPJHvPIB6gfrovL/zq6a4CXsxr44AXm+Z9IpIDDN/nbFT35fOVKaCvaOqur/2rDCr6VfhNb74O+39xlTcO+Iub8C/L4HEMeJpN8sQq+HnkFqCX6yVJHVL7LxTfh891e70uF5qLTynWd6+FuEWf0m8c8Hq7v3lscoAFYC7B0km2YHM3jkOqaK8e+w0Zg6C846ga59pyZTHYCi7Tps9j4c2f9cUCylgWBDkSKKTY8EqW7x/ImzSJ5GwTwhPM5JYtZvoqxAE0WdmLRdDm3KoFggPcl3WXqTLPKAXZ6UYHFab/zSW529kh+TT0vP1MilrLymweZ4aS+Hu6GNfP1Ii6e4I3n11ERXt0AtqH5ivPq1TeLZit6iYgh20McKvdzO19+oLfNSz71NSHq3l8o9WAdt8SYvrcCepvVaxwzRsy+WgVNxDy9P5mLfCk4MKfRe7X0a5znslLA3mvuKXI6k0Lt0a6Sufx0ifLFAZTWRsH1C18A3+MAWxjy06hj/0uWO6OJjaPZVD9EvWmAQU5Bhg6hDtL0ZZf4ZJyv75e6NbhN7gr4KyK1oO5hn4vKM8mgrLeN15GAmhpjUyVeenOjZTZEb1xQNXUt/DGsbd0BN2Xur/PXbC4WtWJ/qs7EcGWPe8B3rHZrb7Q8xAlH6zy1E1meRX+XgGfL79YRVBaHgBjn6ixet8tdXydxJdvoKbg99OoTRpOHPYZqBsHvFcvv1BOfA6JPuGM2sYnKPpcc/fSChy6SU02pT+55gAPadL7IuwXzUqUuZMOliNlCkP7b+dmpVWB572Pwcavawpp0nOScAb48S6U+Q1LJI2xX1yNrxsHuDVv5sZ7wuqx+gy4OP5E1bG+b8BDY93G/euA1HaMAtSlhLT9+VCOVrqiKWfE93jx41GzWrL7GdwY/qafNSNrRH6Q4oeWNswQvgpYoG5egT/giY0DbgyDPCvCX+L1VwTyu0T57TC/Na9aWuctEFG4zxrye5PGi5HyOFBcHs4sQ3SOCkVtvtB8D758gEtV6jnRwhMi8nhWQkRf3rYT+5f5MbcWISKJHyG2IWiBjQPcLjdzcza0tZyatTzQo/+0rfwL/oWDbLPP6bLEu8+0TQ1Kz2CjoiwIuP9rVzQGgUO5v6FjLH++AmYMf6mvZhKf0eUT5QuT68lSn/RcEFpipuL7IA3f2/G3CjcO+HzrvylF7+8k9a2+o9E2OjH03OpbLi3fJfI56K1PAI3xIn7WjtCMrRXkqpm7Ev0ne7u+vAT4l8JV0hv+auUupXwN7KuSHZluPXplRoopUwdqiIYbGjLCSQdlWl0OgD82Drhq7PcN6P39ug8g1+djdT4HAk02j4TgDOeU7O49apkS3xdy8DKC9ZyiJ1QPlOZDFXIjb2E8PUzxP60KD7oK9DcA86lTnwHvfq5cCkfn8ahJFi1f6fEJvx5fB4wBPrhQzINVOdvNO7ZAv3TnKM7faPR5nwvnSC/+SimnyMVJ0kEzeyRAls2eB+VbMWkHIBkb5FqxCWps1f6nz1fon4a//f1rtapLNzcUnrbcdlozvuLzvGsYxE3lU7XLMCGalq/HNkTPfKl2GwPqdr6BPz4krK8oxafcw/G3TYsVpx7z9wDKqpl62yqAz/fP84WQhivxDgqMl0hS/a4RZ2wp/DquSnhrr/k7xT9SnvqmBpVLw3oCy4PW88r+ig5NY13IX0wb4IBh+mPQbxxw467td6ef1A+//UO48/ybcPtZ1oCx7zkifz+pQeLbDmg4EdES/4LhSOYfYi/PHDVqXoe3xWRJ/b1cr+pcV9NPnadV+xOauWog/ugGtCN/+btVe7ZeNWHuoJs0pi6TpM7GAdEIt/zFtyT/JcofZ/VGxxclm+av8Msezi/mMfHvbzfaGsgTsMCI9QAPDokLXyL9F/6S/pcm+8Lcv5as4oBSQYJgBIfYsg0wRxm5uk5bJa4xo1q+C77jC7Pd7hiU0/hzuIfuX5Uq/rffDVvg/wBQevzfJY4PmAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDgKKKK9s8MKKKKACiiigAooooAKKVVLMFUZJq9FbrGASAW9a6cNhZ137ui7ndg8BUxUvd0S6lIRuwyEYj1ApCpU4YEH3FalIQGGCAR6GvReUq2ktfQ9eWQR5dJ6+hl0VZnt9o3oPl7j0qtXlVqM6MuWZ4WJw1TDz5KiCiiisjnCiiigAooooAKKKKACiiigC3ZphS56ngVaqK2/wCPdfx/nUtfU4SChQil2v8AefdZfTjTw0FHqk/v1Ciiiug7ArOmTy5WUdO1aNUrz/XD/drzc0gnRUuqZ42eU4yw6m90/wAyvRRRXgHyQUUUUAFFFFABRRRQAUUUUAWrOTrGe/Iq3WUDg5HWrkV0pAEnDevY17WX4yKj7Ko7W2PpcpzGCgqFV2ts/wBCzRSAhhkEEeopHkRPvMBXrOUUuZvQ96U4xjzN2Q4nAyelZsr+ZIzetST3Bl+Vchf51BXgZhi1WahDZHyubY+OIap0/hXXuwooorzjxgooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADJElEQVR4Ae2bQU7DQBAEAfEFvsIveA8nTpz4Gq/CyFKECGkRdsprxpUDCul4e101toIUbh+enm98zCNwN6/a5k8CCpg8BwpQwGQCk+u9AhQwmcDk+vvJ/ZPqX98vfvh+eXzbclO3h/o7IHA/h76NiaMIuAr9Vxm0hv4C/ox+Gw3NPwWV0F9MVK3zVer6vLOAWmq1q51MtBVA8CLW7CmAILXObPnKDQWUMzrdLggH3QTQ9MsddBPwbVr3/2srAduMf+1F0ErA/uf9fId9BGw5/oUXQR8B58P1L15RwGRNTQRsf/+pugs1ETB5jAfqFTAAr+JQBVRQHFhDAQPwKg5VQAXFgTUUMACv4lAFVFAcWEMBA/AqDm0igP7yyCXU471NBFwCtP/XFTDZUR8B43eDa1WUNPYRcC2+nby/lYCSkfylmKquVgJ+yW5Xb+smoGows6TClm4CFnCFdH7UULt+QwGog1r6y1Z7CoAclNPvLKDcAUG/uYBCBxD9ZYf9/0VpOcnl8eevTXDo140dRcB6tldpoNEfUcB6zsvPYGIb7qedHOsKOJ32fp60/Ri6H8R5JwrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lyggMwHTxWAI84FCsh88FQBOOJcoIDMB08VgCPOBQrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lyggMwHTxWAI84FCsh88FQBOOJcoIDMB08VgCPOBQrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lyggMwHTxWAI84FCsh88FQBOOJcoIDMB08VgCPOBQrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lyggMwHTxWAI84FCsh88FQBOOJcoIDMB08VgCPOBQrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lyggMwHTxWAI84FCsh88FQBOOJcoIDMB08VgCPOBQrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lyggMwHTxWAI84FCsh88FQBOOJcoIDMB08VgCPOBQrIfPBUATjiXKCAzAdPFYAjzgUKyHzwVAE44lzwAehTUsMXcQVkAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwBlFFFe8IKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqlc6pa2rbWcu+cFU5x9aravqTQf6PCcSEZZgfuj0+tc/WFSrZ2RyVsTyvlibf8AwkP/AE6/+RP/AK1W7bWLWchWJibH8fT8/wDHFczRWSrSRzxxVRPXU7aisLSNSYOtrMcqeEYnp7f4Vu11QkpK6PQp1FUjdBRRRVFhRRRQAUUUUAFFFFABRRRQBxtxMbi4klOcuxOCc49qjp8sZileNsEoxU49qZXnvfU8V3vqFFFFIQqsVYMpIIOQR2rsoZPOgjlxjeobHpkVxldlbxmK2ijbBKIFOPYV0UN2duDvdklFFFdJ3BRRRQAUUUUAFFFFABRRRQBia1YOzm7iGRj94B1471iV21ZlzokEzbomMJJ5AGR+Vc9Sk27xOOthnJ80DnKK1P7Buv8AnpD+Z/wq3baFEhDXDmQ4+6vA/wAT+lZKlN9Dnjh6jexU0iweaZblxiJDlc/xH2+ldFSKoVQqgAAYAHalrrhBRVj0KVNU42QUUUVRoFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACrElEQVR4Ae2bzU3DUBAGAVEFlXGjB65Uw40aKIg6MLL0FMXyT8zuG3sznKzEeJIZf4EDPH68fz/4xRl44tCS/wwYAL4PDGAA2ACMdwEGgA3AeBdgANgAjHcBBoANwHgXYADYAIx3AQaADcB4F2AA2ACMdwEGgA3AeBcAB3iG+d3xnz9fc8y3l9e5p/Iev5cAC96b3HZOzxL1AzStTfTqwfgtfTJUDrBD/WWbPhnK/hD+p/1WIuo67YJXBzUDxFqLvVr9ABm+Mq45lqi2gDxTSVcuFSDJUfvQyLh+nQAZdpr6dhBOqROgOTrXQZEA4TfmQsVYVpEAC74O/lSFALG35JZggcQKAbYoO+w5BoDTnD5A4KfBTSmiuKcPcJO1A55sADiKAQwAG4DxLsAAsAEY7wIMABuA8adfQJ8/HplWiuKePsBUzbkeMQDcq0KAqE+D7SkCiRUCbBd3wDOLBAi8JVcjxbKKBFi1dtgT6gSIvTHngoVT6gQYlIXbucqQcf1SAVIbZNgfXnC1AEkNkuzXDBDeIM9+2QCBDVLtD6+z8r8oje52//lCtvrBfvEA4zvckaGP+vHlVV7A+A4vMwzHC4Po6b29tnsJ0N4wYrnRpwcFfw2dvskjP2IAuI4BDAAbgPEuwACwARjvAgwAG4DxLsAAsAEY7wIMABuA8S7AALABGO8CDAAbgPEuwACwARjvAgwAG4DxLsAAsAEY7wIMABuA8S7AALABGO8CDAAbgPEuwACwARjvAgwAG4DxLsAAsAEY7wIMABuA8S7AALABGO8CDAAbgPEuwACwARjvAgwAG4DxLsAAsAEY7wIMABuA8S7AALABGO8CDAAbgPEuwACwARjvAgwAG4DxLsAAsAEY7wIMABuA8S7AALABGO8CDAAbgPEuwACwARj/C8l3V4jyGkxoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDvaKKK4z1AooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiir9lbDAmcZP8I9PepnJRV2YYjERoQ55EMVlLJyRsH+11/Kpv7N/6a/8Ajv8A9er1FczrTZ4U8zxEndO3y/zMmW0liGSNw9V5xUFbtZ15bCP94g+UnkelaU6t3Znfg8x9rL2dTfoU6KKK3PVCiiigAooooAKKKKACiiigByLvkVc43EDNbYAAAAwB2rEjYJKjHoGBrbrmr9DxM3b5odtQooorA8YKbIgkjZD3GKdSOwRGY9AM0LfQqLakuXcw6KKK9A+yCiiigAooooAKKKKACiiigArSsrgOgiY4dRx7is2gEggg4I71M4KSsc+Kw0cRDlfyN2is6LUHXiRd3uODU39oRf3X/If41yulNdD5+eX4iLty39C3VG+uAQYVOefmP9Kilv5HGEGwfmaq1rTpNO8j0MFl0oSVSr02QUUUVuewFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB//9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACy0lEQVR4Ae2bQU4VURQFkbgnd+EymeMu2IQrYUCTTl5+wgWS772nzOty9POir5qqPugAf/x9fnrwF2fgkUNLfjdgAPg9MIABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA410AHOAnwn/5/foZ99cf5pE+e57p8+hX+4X39XWu33OREqEAS+sS/e2H849sn2E8wB3qb9tsn2H2L+F/tL9KdN2zLvx/PgwG6LXWe9v+ASZ8TdyJlxhZwJypuZupEv0Bph1N3x8u0RwgYydDyZRoDpB56J0onQGSL2aSNdq7M8Dog+56eVuA/CuZJ068BG0BJh7uCncaAK7cE4D6bkBxG6P1BGh8oKtdZQC4uAEMABuA8S7AALABGO8CDAAbgPE9C6B+eITiNkbrCdD4QFe7ygBw8bYA+e8GeeJEq7YAEw93hTs7AyRfySRr9D3oDDD6oLte3hwg82JmKJnkzQGOh562M31/xvui9AcYbbCZ/cPVSIChBvvZHwzQ3mBL+7MBGhvsav9QNP5flE53d//4wsbqD/uJACfmjgzbqz/NjC/gxNxmOD5/MYiLeF9aogEW9WqW1xf+8cPUP0M/kjwpDRig1JI7NEDOdUkyQKkld2iAnOuSZIBSS+7QADnXJckApZbcoQFyrkuSAUotuUMD5FyXJAOUWnKHBsi5LkkGKLXkDg2Qc12SDFBqyR0aIOe6JBmg1JI7fAMOQlgyfbmzOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAdTklEQVR4Ae2d244kSVKG45SV1T2nndnVroSExM1IiFskxPvxODwDT7EggZCQEMMN2mFmmOmurso44PZ/v1dadFZ1V3dXVQ4oUkpPT3PzQ7j/Zm7m7hHR/ts//F3TNF+3f1vCn7/71xIOX/95CS92hxJe/9CV8HX/Swn7IeLDfinh0u1K2C2T4jclnOd9Cfub4Gl2EbaN4vH/V/5p39u+sdW1t3Ht7RRXPV31JTwMcwkvl4ugj5F6PUa/7fdDCV/+7nUJhy+j/N1PEf7Td8H57ctXJfz7f/9jCf+v9FFp6v/PzzD+R4zk1V/8TwmvhxilMQapaYcfI/wscN3NMbZd/3lQ+usSXh9CAnYa82EfozjPQVmElFZysOsjV6H5W/IuwppwVP4pVQGMx/9ni7khqWm1KZGy6yLs5rjeaQqMt5L1faMu2weu5zbk41I98+J19MnPh8j16vvflfCri/8u4R8+j/h33/62hJsElE4452d48fUfSv2XFy9LeNP8voSfHb4o4Xj9poR986KEncaz7zTmU4znRYhBmQPGErY74QKkC/Wz6LORHpyLUiNWPsZXglmKwvKrCFetCl3PFY1dJEy7uPblZeiPqYm4KTeaO68KoWnb0B+/zCEfLy6D0vzwVQSXX5bwH+e/KeEmAaUTzvkZ/uwqNNGf5hjVF1/HeHTt6xLOMajNIq29tJG6NIECkR1n5Nsb2UIay7mL+DwJPCqz/L3nE3LDh5jyVNIz/h7b8ValuUGaBedFcqDrmtQR7RDX+0a20IBoHzSnyj48XIc0zEvMBO34cwlf/lY9uYS++asXUcQmAaUTzvkZflhCnd9cxUiWCb4E4xia66DZfNC8v+w0bjG0odhK0MlemmUJ9JGpyEqMpUEjykPG9rzYj3bftpk/94TYgcX9KelYQQsW4LWuWhgfZD4usy5e3kBojeIHyD6cJDev5ujElxdhL/00ROpDeqmwbZ+n6oHhsI85+k9SafsldNbXbYSNxnCZw8uVimv6NnRZcQRL0DPOmiEuJBmz4lgIEptm1woL2f7BD4hSVp9fgxzQIFrixlmc41+HfyMLcJal12lGnPCNlWFsw0qEvttFp4zqz+s+TKLxOvp57mJ+/aUPC/Nw81kJNwkonXDOz3Ati/5CMN5dBALmJkzWSTqufR3xeQ7UL5cBiclzfczgLT6hJGCRHmw0N1RX9x4JAFkJ9vaQS4nP/UmIV6sS6FdNWZbgXGQFMRO0snM0aRZ4Rxe0mh2LsxDxPpDdq7g9PbYPHnVP00/RM28kE5sElK4452coqrrU/1UXls+++MJl9GTJzofwgT/r0WWSDGnwqvWk7zTCfRs8i7SZUSDtP1gQhJ3CISZ+I1Re/ySf+chwlthJS4B1K+zjD03ENQc0g64dqDPbqYRe06CchyIZgfdeWuSqjR7eyeT8ZoiZYJOAs4zzsdIBtaaJwDqua0O/32iuv5hihCeNtu1fjTBeMfhdZBctgjR4iXmkpKmcpGWDVr53fe6j38X7+LTUxtOGiNIL3az12gAE79h1CltfXWC6nY8WEaLSy4JqpP3xIVpJzyYBjz+eH1RiGShpqEkyoJldpn+BfeCCuR7vtxjDKlojLLsYRHfSgLPGvOkY0ZChpQvpASjKWOJHgFXUHSnmecDPaR5Kg15LjoIezrlm1bXTklSo18S49lnrPH1Y99C7JVDfSh/Msg9h1DRq/7mRdTSN0eehWbY5QB1xzmAAv4Ps3GYSfvEApOo6zQFdE2O7LEI0PoHmcRpu41jrJHVFKMpZhIKMxyoNb+My4/chnbEqM2U4pTMzlbaI61hv5Yxfp+nHdE1lR+6Ca0GWTkJWFtn7zIWmI9/MiJTmPoySkAm86JG11THoaAw1bwvO0QPDFEqsqO7AOJsAvdb0q+Ure0bQZylolv7Ctm3x+mTtdCBJ2Fk0l7TScRX1UTxzgFEmAkG1HxLpGaNGun4cB9JQwLIo7I60jksopAloP+ugrZxjrnGRzcMakc9SaFlt1PzabX7AM47yvVV5G6cRZidcNyk5e33C/sRpAIGZfdEOi8haL6ACOqjH4085q6rFuaLwx8i7I+V8JJxiX6KNJLVTlwfqW82LngOwfJgu7EBH6xcKkmR0KmfWPtok63GbA843wqp5WLR/q6Mrzaj45U1ot1m7vwtzA14c2wICgQdV48kJuKrHwQgyEDWAbcuHsaOanUL8Q0NKfWguWnOax63UxRjvp00TE6uhRr2kHyuolak/ay5kHZQrbu0zRXGeTSUmB82LLX3r/YOHXsXG9yQ9MCxjTORew9F4ggvCWeM2Y9XIqrX9i42PPaC5wXrQ/NFWtJvpGEPJE/64q6FVrU2rI6ahv7vMO3gsnpHvjlSKO1ZSPJvgwr81P9iXD8Q+QZylKp/kA3VjWJizathh/+jclA4QbX5A9NY5P2XRPjR+r5HoZO3stFsPWC/k1014dKzzaOgHjSdrhFreYOmogCAAAzp6xMpQiYvMlpKZTniC72M+Cain2d+ZaPbMk+JEjXqbRPLzla0F74mJnYBykLakcxJi7EMC2B0Z9qz/RAltT1wFbcG5ekAO8N2VZ+hmjiOdGOc+T+M5T43DlQBWBaEyfPTvsVUPK4I2RK7cpho/luZ9jlyoE488LsI8iZ5zwXSSuPkBq056/j+3nnAMTbVk7m6GBy9bMsQVVkTpd8UTpd2Rl0oy593V3km9T4aoJ6cqu6s/FkW1mc/XTvO5opRcCxDJJmOUtpYPMpPtGBIrOl/VR1hLC8ImAeqW8wVlDtB4MKp1sN5uj4dMP3n4Et+p9KzuCdD4n1IyGvBFKXJdyWmz1umRJ1NyXOXdU4D5ciokNSXLBCxuv3hIrXRandoBj1plTrFSQq0wmDYJOPbdWWLVCtIw1RNqwEDtMQyIx8jlVR1PGrDnUEvldRmI64pkyq+l199INzzEKk5LjAgrdEN5d6jSKsyCdVXVMSFLJNTKGP+s6kWqJ+MSPbXQJTqz/6mJxBW6J4+psG8SoI46X1AlQC0AEQ6tuiKB8a9GkijWbjGK3heVNmNUTWFlPNhvMRjjb6CIXKVEuDiCo9YIzz1hXX/NyRShGo6BBcz86bqckwal2t9Hz7UonvWEM0PPbbs7vknA3f3ybNRqBVEh9m+2goUO7JMcduLptBboE3OKe62UVK8RqmhQBipX6z9HpIArNyR3wB0IFa9mGnKpwpInl5FLEp1rIeprpOhU2WkBqZi1zAVrZee3/kvlHaP3JG4ScOyis8Q8B7DC7nEEKQkv3ubMdFbGARBIxPIh5KyANKPVI5yEVENpKZ6vP5OJk0oBxDmT4VwwIaS5FrPGj8s5TXUR7/pxe3JexZG83MJ1KfenVL5NAmpPnOm3LOynoXQjjgPNCDoUJ7a8R1aMlh6nRhHso3F+xkYPlcBPPF+wKJlArmyJ3yK4sOXayQX07VZSGqFZxSWKy1xVdvIn5arR+rvmrbNCvoC7OWn1adomAeseffZ/Q7UfTmo2qBKd4TsdRLG4HPPoJ4HNqZohOHFUy72nuEx2XD+GWk6mJCgZibWG4+9prmPauWKbBJyr511vuW1PqAE6Rn38sQbPFOneeiIh8gNxzsDgGSw67eXC0tkY3xmg4fZ9ZDSAIshg3a4/pogpCalbBadPJLg2tScwnld4qOR21oi/qWRaUgnHclZMNbkW9ci/mwQ8cod+aHG+MWA15irDA89P/gMepU459bhaQRI2YccloEGcKLVPkNpoAVCGLFuJxXIBhbrq9BTZ8gpPbmYuYX110fQq2M6xZjZ7msLWBdyT6e1CHvZ/k4CH9dOTcZUzQBpQRtU+QaotGw5mPAGApSFy1buoFNcpI7hnS4b+wa9KOG+j6Arp3FloOq1SLqTE9714bogy3UyKV2hbC+/EggZXJNfUqCEj3eu+7hNXGUyeOoJyLEUJnxhsEvCJHfip2YsfACKioKrNQ3uzpj8LinM68+57ZUUxdnTyfRIefRsZANYpeG45sGToj+NquU8Ra7rIyKpbCUHz8xjgB9E6eQkoyUUIOH3fjpJ9Nl95CfKc4akgzWqJ8ZmimwQ8U0ffVw2P77BewzYwmsgBOtCtTuZPJFuD+0kJkW/hoCj3yvKcLZVTJSD+zPBQp8/aB716HhHnzksq9En8IJfMkk7uRdFdhsiB+cVS2x/tpASrefYnxFMDeHVFClYKPiVa82eeWsQn/m4S8Ikd+KnZvR8gg6UsjMagV4Ufce5pYuCpCh0KOEAfdIuKEnwfmeCHN2CnFYphGaXaF1Wuao2oNlPix3eniNylvMYOjSNUETzVp3q5kUCmOq+ovSo/Zrqo4IQiwoqOJWYBIflxwk0CHqcfP7qUuhoKIsCCBgWL296pIMTSzoywVJEpFS88IxqK7iNjP4BHaFHkqDKlwAt/tNawAwCYSukijFbhztOKREy3MPuhFYCypW3k1VUgi9SQwY1FZBESlutFB1elp0acRC02ouf4CeMHEDYJ+IDOegrWejqaAQX7hs1xjG07SwS8I4blA9hYAeVZEXrHAE+MaNKdNlhBlgxDWuUr4N6rgsJyhfZ1db8mLbAtr7qYXXimhf0D6/3onOqXqKO4CqJUZdFICUqlXq8IUKXp7/xxL52W9s5cJ4mbBJx0yfMS7AesVKC0Obq1GhAxzno0HHBpFuWzH6CnKaJDlxcBIazyZq+4MLXo/jJ72paAuFDfU8YZiyCUuSEy8Lx2rDI9fLN4AEe6Hk7h2xF5IEXrOYnKVI6gKYeknkBeqXmQS6hckYmJg9gDQ/JSzgOzrNg2CVh1x/P/kZ5eD7yxIbygNr1ehNeqZD0CWVgteWUFjTJKdEtseWJsXIhuBOeG2WYUQifZPjwzx5mVN/u9tAWLC1OJOzKxi1ib6oRpnllt9Pr+W9WLNOCAcBX0q1hXYgDdFpn/vOOHnllhnit9R573JW0S8L4eeuJ0nv+wxoCGmNF2KPPC3qzwhS1P25gPRkH6oPWfg+Kt3rfEQ7j0KFLfS6xHrnsu4TbbKgHUFqX2PLxChJ1Ai83D3bb4wzgPlgBJkimSBtNZO5Ig+61OKwBHXeZMfVApSrXtlDjVqpNiguEjPpsEfESnPWaW2/2AGFav/qt8LG78Xt4cgQPK3lZ5fcxtK2bNIzxTetyF6h3ZD+BJ1BriUVYTc8OE1KHmebCC9DWroSDLcwx0kfCB6wqPSAI8COKedE5r02boqzvcJcJ5DsAqo8bbyykRrs3eT064I37shzsSH0DaJOABnfSULKv9AE7v5IU/tKoNCrWjrmsCyyDx5KdR88RB5R00rMtF8HgO0Pu2sIJGPVsfD8CWkqrhqRXgsdcrDFr5w8w3neJ6fGMRxeDCu+7gZBqBXyHrTuAdyWD3DcR6HQn4eeqIa7n3cw/Qae29uR6QsEnAAzrpKVnqHTIaYfBiLZnGnDWWupQSCXmvlQ1aNO8ka2SSyeK5QStFB+078OR1LCXc01YS4+cNMnPoapG5Tv5zJ7jyqJZeSF80izAr+NGMcgqA8oLB5PkjXdiqH7m8dJFEM6RPKasSHufPJgGP048fXYo9Yc/8Br9goHhdEQo48Xw07B/vAQgj7PEiB9j4Xv0XuvXo3UavnmzYxLUEyP5hfckaOSGOjTrWfFoBm6cPAW49gMfrRdhL3pYWnOp+XPQJcWDt9U66iroc1x9jX/Fk+8PydOEmAU/Xtw8que4HZGawkEKfaTBG9KMAOto8PzbtjrggWn3pqIxz1B0eLPoa/1lW+HwTFaD3Z+9DKJcAA0Ch4xVDOQ1psgHNnygGc6/6wImuxOocZylxwuP/bBLw+H36QSX6VASbSashB2sqzKvtrIYKrfnZcN7jhV/lIQG8UKCGUTYaedLbV70KxK6ZJgH8AFrfem6KQmc9gdnPmZAHjsVV12gjB5YPz0D1roMKyleU48x59oRJ0JyXosr/VuDkt6jv/Ws5vJNvk4A7u+X5iL4/ADukLghqoLNpwsjTKobT9lKQKhKDCSTO+AFIjIZ4jVlxSvtjBbFXjL2PYkaqZi0eLbLxeRsB5bTMHAo9A4FfVqgIcV6Ma10AbT6BIxd3OhEgJXVC0MWnq9b/21Uj/j0kdPcVVmKbBDyk156Qp2w3xUj4eTiqCETUMH5tQUsmTjWsJgU/OJkFG6wdewaWgCiaVDwDkOuZQJVZ5IQyP1kcpMfrtso5VLWE0lgLMiXo1LW2iFToEXBRyPGTsQyP0mpUeY1R4plCMVCORT4sFrl82koZNgl4WL89GVf1hBl6G8yqTRTW3w0F4Q5rpB7BCU48ZK+eW/NSXApZmAd3WhmlBDztUyyRM86mqoIUHn1yl64f3ujBcW03IfKUD4gLJp9qFb+f+6X2VDxSHm0hrgIcnFKUkDOZM12NMyXKqrRI3iTAXXKuH/sBPoGjVqCLCXnmt0dQP6w48kIBGj2JFQr2DHHTNUXw3iAZ9M0sP8BPItdzVail7ohFNX4ekUqer4PiUxEqDU5yYYjUmSk4HbcgQImWIt6A0mtcZM44dXLkcoaIFXFVOS5CqTmXeN4XKNcJ0yYBJ13yvITqCbtWxlaDS2BEnDQq0UGH7/Yil+wT5g+slxpGOfZyWeFhTVTF4wdQMM9x98Shl/zhMThkaiCkBOpNrarzlmo8Tc0gVu05cDG5tARfyIkQ04uqWdFEOqXkeiJ1k4DcI2eIVwlgqDSSaH/UIy1igCslWH0/lxtMtviDmcOqvX0Lae1ehbJxy+pQeahQ8CuVd5DVOSDKoYSOnQDNAZyD8+6YSiOX3d7IlA+yuoW0B1tILAYrlwtlHaaOUIKv3ZlzPrXfmTN9Xd7d/478mwTc3UPPRi0SEGOcbeHVmB+HquJLTfPz2gQ/o1XZfHoHHlk4jHAXr1f1O9ZnGUn5uePYLVkC9HKJphVn5zBKIM4eGSefHSJPaq3LccvjBzmo0hDlnH646tW1nzIlCpype5wGxf15mpxKILpJwEmXPC9h6GXbtrqhq54Fi4Hz8xgEG0zqqj5j7Dupc+RgFLqNu5vIy2plJwW/PuOm1CsBQ3tkWDVlLSdy+R1bJVpe6B21tFfENVF4QzlRdCLIR7E5dMd7esXuIxaqKq90VTkQgglMOlKqn58pqpeSZb/VGVF09aHvMVWmukIcqdkDx0PynKR3KW0SoB48X1DepBeGNG+7LUo3WiLktoKQ332I1QGaJAicWWPtpdfhN97D1GvBc5B1O7H4qWvj7dr1jasx6sYLqZSpONpz0IIqz+TdEadkYbCTIdVL73sGkkR22orzDKHWIpf1xkyhTXTwa8mAIhKzET4wp6lJrDpd8Eao1Sd+i5KvgvKDB7qdcVqrxF7GnHtPKwIiK/8WnKUHhmuBfn+tN6SzhyX8slYDckFKK62NAowxLh8Fw3X8sII/yC/1uU+Bhzd2c34NeWrR5hzlIXRhkUHRcn+AitcBo53inQ6cDsw3N4GbQfcQtNL+nTg7yWLLHrI3H9QI0yOXr0tk6sI7ifrKJ013a50evFDcRjwbZbJkq0wFzUHvl29orm5ya3XIaadwaeKSRsncJgHqwvMFA6NkraQNqgW8gBHeKa9ZAQ3r1Qs2vYRXzh5zcm0QEpc+BKEXWusp5RhpzoZ22vDV6ymLvSRkCXdgwRIA0i0BQQPjQIo33Q8yKfAVBlXTSTtj1608ZBlbnW/qjM6uK6NxkQY9cwDV056aEBkS3XskJtFqdVbqE+6DY4e87nmEqukkAZNaznyzSUB07xk/g27yLSMTmGW9njPJCyuaUt6+c0ZalXMJM8AWChZpZFDP/cBYU2hnr2gK6cCok5vrU3WaM7Axsh+A58xWb3vADxBWNGMVFVtau9j2V1z+RzNF8sz9ZWota02ELk1gVVBwHb/Z+zGIC7WUz0qlGs1JJwSBq6i7DmqV5wOlzCG8o+YATjpN6ltyeUfkEP8WcW4SEH19xk+57TBGo0VL6naTneyNZRdo0o3vmDDFV9C4CU68M3SRzmXNspgIhR8bo0c/qmTOK6CXcSRBgWUFyIFEWiJdXNeCAh+DrKZWM5MaVQQsBAe9D7o53sRsxDY37i244xptg4lES0j1TrLqxXSyZOQ5QOczvH6FDBFKwC3Naj9mnSeIKaRh1sQ1zmH5THqXdqf7GwY9U2+TgNIt5/wMb6TNe+mjG8FpmWMJZhTls+aixHmCYt3jFXqwXjSbT3r2A1bt1O0Kv0/ly6+29hQ/o+1zpXJhW+Vl5aTjvd3qjZ5FE6HezwhStZY2vARbQcqAXYGMIlVCqGBN4LNPaHwkIK4s9PqRlSlGJZJYbSRYkFQkQ+IzKe67zwx+XaUC1gKweVp1xKiFs1ZvMd9x/48r237O1APDKE00Ce8HabFOMjFN8tYwYqTfZ8FjFiC5uxFbvrlhW02QYK0DHkKR0c6GEbvBYAfdCmhYYRUeQb39Z9Vb98jUT3i/SQLYJ4CfBS343atw6irQ+4Tc7VNPOgUv60jW4MiHWNUxkVx4vE6sXkKaWXnFy1nUKROrYUtsZ0Pv1aBeF9Zrrm3x3qPa7XO+Hhj2+32p/SKtp9fl7sDroY3UckS/BO0g/S4T3TyzUsewSWaOeconaPVG9eK8Rl40rHS9rRHhfZZsoT0RM69lgjvpcT8DVwrb1hpQ9NpOlI4VhA1mdFsv39ZtzLYcuQbesOIuB2MpiBApVTz5AZraCjGajn/QCtF49ZzhOCCpYJ+V2ib6hxXZvXrjRjzoGzYI1RlUt4Xn6IFhv3xR6j10Ye30zc8l3DWvS8gKaCONtrRxQJnzPIu0GHdJziACbQgU9VgU7NzFq4Ala0FBAMx2tNDNygz3vKOLu7SJZSseb1agxIrwIs6JnSOxiYrKByGRwi/3K4hCO9Oyp3V3kgCflVsBUniXZHjO8ypWzHltK1OMiVE7cbamJK+TJCD0QuGkDfLSh72S90HaXUZ8VWH5v32euQeGV69iHC7HwH578VMJR2m3qY0Z/LCEFmMlcpb/iR+7k8pcrmNWqKtG8k65hXe4KXTU8yyNbFQG1WBlpRAJ8Lk5W0HBg0Lm7FAvGGOfeDcCGRKY8hkIyw2VKTVbKVHu7Qfsc48N10ISaFXciFZTli5Qz5NiWNlfsIjEb0ufvFj3koxe4L+m31TXtWSCZ8d800bvbRJAx58tHPovQ5cVhzhGY3xRwn4OjX9g30p7N/ONpCGmibKkGBM5e7+zgA7gYp0jEuQTjJGw9AqDequXA062owGz8RU8reebiFtMhOJZFXBeGnliRkEWQf0KR+JnSuJeYmrEYkG2/NQjPFvkTXFNZ7e7BcdSF3a4ROjU8g49wVKXKtNyWrm3OTQBXr2f0sKqgVaE+ujgojOiK29+jL461hEp2+fZe2C4GF+VSvuvQg76q99EqOf87K4uSxwtf9MGltsu5IBJ/aBxZj9LSt4judMy5sh9jXJJSbVatrBg1gQWvXwiWHIOA35wCrtXMYVTn7gmLvD43EO0rFCVT3MGeb1OKbrvQhBntYLijyUSCaB6eCSGEFgHw6g6yPzpdaqH/W1mx56LkZ3TH8IH6sfosX37eQkPnW51e/OyxG9eR/zmKsJNAkonnPMzXP3nj6X+7q+jEVda/u9+DByxQjnLFpr2Gskxhh5PccTvVaqfRohm7GUXcZChkdwIQiDRuGZNJkEdS9n2TDSkfKIen0A1mEVm5qARImApVYJabjmIZOYAGu126o+fPwH8jPoog3IIkSdazpMt/NQ8SfmktYOBvLrqadD8dxFzQL9o/ht/KfH2BbNjUD5/Ef1z8UtU3Dbfl3CTgNIJ5/wMyz//S6n/99/+ZQl/lvZ8o7Ft9RDcVosg/ReB/espRo9n+HQa50UPwrqR1wdMWR8f5OlNrCOBR+Sg5A+UVYQFCgKz1Q+I1ApH4VGFYucgPZGjfpgbWAWiSNJcptgsPPxQV6Jb8lKZlb/WUX/Zv3M75RZPOg11kC20I5s8AFa3ej0r6VqnQzr5BJf9ZSns1XexyvD9f31Twt98+ccSbhJQOuGcn/8FsXjkNazNPXUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoencoder.eval()\n",
    "for image in dataset_test:\n",
    "    result = autoencoder(image)\n",
    "    tensor_to_image(image.data).show()\n",
    "    tensor_to_image(result.data).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
