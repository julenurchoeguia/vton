{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from refiners.fluxion import layers as fl\n",
    "from refiners.fluxion.adapters.adapter import Adapter\n",
    "from refiners.fluxion.context import Contexts\n",
    "from refiners.fluxion import utils\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHAIN) UNet()\n",
      "    ├── Conv2d(in_channels=3, out_channels=32, kernel_size=(1, 1)) #1\n",
      "    ├── (CHAIN) DownBlock()\n",
      "    │   ├── Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=(1, 1)) #1\n",
      "    │   ├── Lambda(append_residual(x: torch.Tensor)) #1\n",
      "    │   ├── (CHAIN) Downsample(channels=64, scale_factor=2) #1\n",
      "    │   │   ├── SetContext(context=sampling, key=shapes)\n",
      "    │   │   ├── Lambda(<lambda>(x))\n",
      "    │   │   └── Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    │   ├── Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=(1, 1)) #2\n",
      "    │   ├── Lambda(append_residual(x: torch.Tensor)) #2\n",
      "    │   ├── (CHAIN) Downsample(channels=128, scale_factor=2) #2\n",
      "    │   │   ├── SetContext(context=sampling, key=shapes)\n",
      "    │   │   ├── Lambda(<lambda>(x))\n",
      "    │   │   └── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    │   ├── Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=(1, 1)) #3\n",
      "    │   └── Lambda(append_residual(x: torch.Tensor)) #3\n",
      "    ├── (RES) MiddleBlock()\n",
      "    │   └── Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=(1, 1))\n",
      "    ├── (CHAIN) UpBlock()\n",
      "    │   ├── (CAT) #1\n",
      "    │   │   ├── Identity()\n",
      "    │   │   └── UseContext(context=unet, key=residuals)\n",
      "    │   ├── Conv2d(in_channels=512, out_channels=128, kernel_size=(3, 3), padding=(1, 1)) #1\n",
      "    │   ├── (CHAIN) Upsample(channels=128) #1\n",
      "    │   │   ├── (PAR)\n",
      "    │   │   │   ├── Identity()\n",
      "    │   │   │   └── UseContext(context=sampling, key=shapes)\n",
      "    │   │   ├── Interpolate()\n",
      "    │   │   └── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1))\n",
      "    │   ├── (CAT) #2\n",
      "    │   │   ├── Identity()\n",
      "    │   │   └── UseContext(context=unet, key=residuals)\n",
      "    │   ├── Conv2d(in_channels=256, out_channels=64, kernel_size=(3, 3), padding=(1, 1)) #2\n",
      "    │   ├── (CHAIN) Upsample(channels=64) #2\n",
      "    │   │   ├── (PAR)\n",
      "    │   │   │   ├── Identity()\n",
      "    │   │   │   └── UseContext(context=sampling, key=shapes)\n",
      "    │   │   ├── Interpolate()\n",
      "    │   │   └── Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
      "    │   ├── (CAT) #3\n",
      "    │   │   ├── Identity()\n",
      "    │   │   └── UseContext(context=unet, key=residuals)\n",
      "    │   └── Conv2d(in_channels=128, out_channels=32, kernel_size=(3, 3), padding=(1, 1)) #3\n",
      "    └── Conv2d(in_channels=32, out_channels=3, kernel_size=(1, 1)) #2\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class DownBlock(fl.Chain):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            fl.Conv2d(32, 64, 3, padding=1),\n",
    "            fl.Lambda(self.append_residual),\n",
    "            fl.Downsample(64, 2),\n",
    "            fl.Conv2d(64, 128, 3, padding=1),\n",
    "            fl.Lambda(self.append_residual),\n",
    "            fl.Downsample(128, 2),\n",
    "            fl.Conv2d(128, 256, 3, padding=1),\n",
    "            fl.Lambda(self.append_residual),\n",
    "        )\n",
    "\n",
    "    def append_residual(self, x: torch.Tensor):\n",
    "        self.use_context(\"unet\")[\"residuals\"].append(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(fl.Residual):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            fl.Conv2d(256, 256, 3, padding=1),\n",
    "        )\n",
    "\n",
    "\n",
    "class UpBlock(fl.Chain):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            fl.Concatenate(\n",
    "                fl.Identity(),\n",
    "                fl.UseContext(context=\"unet\", key=\"residuals\").compose(func=lambda residuals: residuals.pop(-1)),\n",
    "                dim=1,\n",
    "            ),\n",
    "            fl.Conv2d(in_channels=512, out_channels=128, kernel_size=3, padding=1),\n",
    "            fl.Upsample(channels=128),\n",
    "            fl.Concatenate(\n",
    "                fl.Identity(),\n",
    "                fl.UseContext(context=\"unet\", key=\"residuals\").compose(func=lambda residuals: residuals.pop(-1)),\n",
    "                dim=1,\n",
    "            ),\n",
    "            fl.Conv2d(in_channels=256, out_channels=64, kernel_size=3, padding=1),\n",
    "            fl.Upsample(channels=64),\n",
    "            fl.Concatenate(\n",
    "                fl.Identity(),\n",
    "                fl.UseContext(context=\"unet\", key=\"residuals\").compose(func=lambda residuals: residuals.pop(-1)),\n",
    "                dim=1,\n",
    "            ),\n",
    "            fl.Conv2d(in_channels=128, out_channels=32, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "\n",
    "class UNet(fl.Chain):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            fl.Conv2d(in_channels=3, out_channels=32, kernel_size=1),\n",
    "            DownBlock(),\n",
    "            MiddleBlock(),\n",
    "            UpBlock(),\n",
    "            fl.Conv2d(in_channels=32, out_channels=3, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def init_context(self) -> Contexts:\n",
    "        return {\"sampling\": {\"shapes\": []}, \"unet\": {\"residuals\": []}}\n",
    "\n",
    "\n",
    "unet = UNet()\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "print(repr(unet))\n",
    "print(unet(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resblock(fl.Sum):\n",
    "    def __init__(self, in_channels: int=1, out_channels: int=1) -> None:\n",
    "        super().__init__(\n",
    "            fl.Chain(\n",
    "                fl.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                fl.SiLU(),\n",
    "                fl.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            ),\n",
    "            fl.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Dropout, fl.Module):\n",
    "    def __init__(self, probability: float = 0.5, inplace: bool = False) -> None:\n",
    "        super().__init__(p=probability, inplace=inplace)\n",
    "\n",
    "class MaxPool2d(nn.MaxPool2d, fl.Module):\n",
    "    def __init__(self, factor: int = 2) -> None:\n",
    "        super().__init__(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(fl.Chain):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__(\n",
    "            fl.Conv2d(input_channels, 32, 1, padding=0),\n",
    "            Resblock(32, 64),\n",
    "            MaxPool2d(2),\n",
    "            Resblock(64, 128),\n",
    "            MaxPool2d(2),\n",
    "            Resblock(128, 256),\n",
    "            MaxPool2d(2),\n",
    "            fl.Conv2d(256, 4, 1, padding=0),\n",
    "        )\n",
    "\n",
    "class Decoder(fl.Chain):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__(\n",
    "            fl.Conv2d(4, 256, 1, padding=0),\n",
    "            Resblock(256, 128),\n",
    "            fl.Upsample(channels=128,upsample_factor=2),\n",
    "            Resblock(128, 64),\n",
    "            fl.Upsample(channels=64,upsample_factor=2),\n",
    "            Resblock(64, 32),\n",
    "            fl.Upsample(channels = 32,upsample_factor=2),\n",
    "            Resblock(32, output_channels),\n",
    "            fl.Conv2d(output_channels, output_channels, 1, padding=0),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(fl.Chain):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\n",
    "            Encoder(),\n",
    "            Decoder(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutAdapter(Adapter[fl.SiLU], fl.Chain):\n",
    "    def __init__(self, target: fl.SiLU, dropout: float = 0.5):\n",
    "        self.dropout = dropout\n",
    "        with self.setup_adapter(target):\n",
    "            super().__init__(target)\n",
    "\n",
    "    def inject(self, parent: fl.Chain | None = None):\n",
    "        self.append(Dropout(self.dropout))\n",
    "        super().inject(parent)\n",
    "\n",
    "    def eject(self):\n",
    "        dropout = self.ensure_find(Dropout)\n",
    "        #  ensure_find : meme chose que find mais ne peut pas renovyer None\n",
    "        self.remove(dropout)\n",
    "        super().eject()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SUM) Resblock()\n",
      "    ├── (CHAIN)\n",
      "    │   ├── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1)) #1\n",
      "    │   ├── (CHAIN) DropoutAdapter()\n",
      "    │   │   ├── SiLU()\n",
      "    │   │   └── Dropout()\n",
      "    │   └── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1)) #2\n",
      "    └── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1))\n",
      "(CHAIN) DropoutAdapter()\n",
      "    ├── SiLU()\n",
      "    └── Dropout()\n",
      "(SUM) Resblock()\n",
      "    ├── (CHAIN)\n",
      "    │   ├── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1)) #1\n",
      "    │   ├── SiLU()\n",
      "    │   └── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1)) #2\n",
      "    └── Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=(1, 1))\n",
      "(CHAIN) DropoutAdapter()\n",
      "    └── SiLU()\n"
     ]
    }
   ],
   "source": [
    "resnet = Resblock(128, 128)\n",
    "silu = resnet.Chain.SiLU\n",
    "adapter = DropoutAdapter(silu, dropout=0.5)\n",
    "\n",
    "adapter.inject(resnet.Chain)\n",
    "print(repr(resnet))\n",
    "print(repr(adapter))\n",
    "adapter.eject()\n",
    "print(repr(resnet))\n",
    "print(repr(adapter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dropout(chain : fl.Chain, dropout : float = 0.5):\n",
    "    for silu, parent in chain.walk(fl.SiLU):\n",
    "        DropoutAdapter(silu, dropout).inject(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(size, num_images, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(1, num_images + 1):\n",
    "        # Create a new image with a random background color\n",
    "        img = Image.new(\"RGB\", size, color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "        # Get a drawing context\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Choose a random shape (circle, square, or triangle)\n",
    "        # shape = random.choice([\"circle\", \"square\", \"triangle\"])\n",
    "        shape = \"square\"\n",
    "\n",
    "        # Choose a random position\n",
    "        position = (random.randint(20, size[0]-20), random.randint(20, size[1]-20))\n",
    "\n",
    "        # Choose a random color for the shape\n",
    "        shape_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Draw the shape on the image\n",
    "        if shape == \"circle\":\n",
    "            draw.ellipse([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"square\":\n",
    "            draw.rectangle([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"triangle\":\n",
    "            draw.polygon([(position[0], position[1]-20), (position[0]-20, position[1]+20), (position[0]+20, position[1]+20)], fill=shape_color)\n",
    "\n",
    "        # Save the image to the output folder\n",
    "        img.save(os.path.join(output_folder, f\"image_{i}.png\"))\n",
    "\n",
    "# Example usage\n",
    "generate_images((128, 128), 100, \"../data/dataset_train\")\n",
    "generate_images((128, 128), 20, \"../data/dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, path) -> None:\n",
    "        self.data = list(range(100))\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Dataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "    \n",
    "    def __getitem__(self, key : str|int) -> int:\n",
    "        match key:\n",
    "            case key if isinstance(key, str):\n",
    "                raise ValueError('Dataset does not take string as index.')\n",
    "            case _:\n",
    "                return self.data[key]\n",
    "            \n",
    "class ImageDataset:\n",
    "    def __init__(self, path) -> None:\n",
    "        self.path = path\n",
    "        self.image_files = [f for f in os.listdir(path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.data = [self.load_image(file) for file in self.image_files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'ImageDataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image.Image:\n",
    "        return self.data[key]\n",
    "\n",
    "    def load_image(self, file: str) -> Image.Image:\n",
    "        image_path = os.path.join(self.path, file)\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image '{file}': {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/laure/vton/laure_f/refiner_autoencoder.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/refiner_autoencoder.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m1e-4\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/refiner_autoencoder.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/refiner_autoencoder.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m a\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/refiner_autoencoder.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(autoencoder\u001b[39m.\u001b[39mparameters() , lr\u001b[39m=\u001b[39mlr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/refiner_autoencoder.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "path_dataset_train = \"../data/dataset_train/\"\n",
    "path_dataset_test = \"../data/dataset_test/\"\n",
    "dataset_train = ImageDataset(path_dataset_train).data\n",
    "dataset_test = ImageDataset(path_dataset_test).data\n",
    "autoencoder = AutoEncoder()\n",
    "load_dropout(autoencoder, dropout=0.5)\n",
    "autoencoder.to(device)\n",
    "autoencoder.train()\n",
    "lr = 1e-4\n",
    "num_epochs = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for epoch in range(num_epochs):\n",
    "    loss_iter = 0\n",
    "    for image in dataset_train:\n",
    "        image = utils.image_to_tensor(image).to(device)\n",
    "        y = autoencoder(image)\n",
    "        loss = (y-image).norm()\n",
    "        loss_iter += loss\n",
    "    loss = loss_iter/len(dataset_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"epoch {epoch} : loss {loss.item()}\")\n",
    "\n",
    "autoencoder.eval()\n",
    "for image in dataset_test:\n",
    "    image = utils.image_to_tensor(image).to(device)\n",
    "    result = autoencoder(image)\n",
    "    utils.tensor_to_image(image.data).show()\n",
    "    utils.tensor_to_image(result.data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:efqvuydb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>27.00144</td></tr><tr><td>step</td><td>188</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1000epoch, refiners, squares</strong> at: <a href='https://wandb.ai/laureemrt/autoencoder/runs/efqvuydb' target=\"_blank\">https://wandb.ai/laureemrt/autoencoder/runs/efqvuydb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231116_171255-efqvuydb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:efqvuydb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/laure/vton/laure_f/wandb/run-20231116_171500-buje57z0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/finegrain-cs/autoencoder/runs/buje57z0' target=\"_blank\">1000epoch, refiners, squares</a></strong> to <a href='https://wandb.ai/finegrain-cs/autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/finegrain-cs/autoencoder' target=\"_blank\">https://wandb.ai/finegrain-cs/autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/finegrain-cs/autoencoder/runs/buje57z0' target=\"_blank\">https://wandb.ai/finegrain-cs/autoencoder/runs/buje57z0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 145.10202026367188\n",
      "step 1 : loss 143.31204223632812\n",
      "step 2 : loss 141.11109924316406\n",
      "step 3 : loss 137.99388122558594\n",
      "step 4 : loss 134.60145568847656\n",
      "step 5 : loss 133.7023162841797\n",
      "step 6 : loss 130.5761260986328\n",
      "step 7 : loss 126.23130798339844\n",
      "step 8 : loss 122.20919799804688\n",
      "step 9 : loss 116.1917724609375\n",
      "step 10 : loss 105.69256591796875\n",
      "step 11 : loss 91.06919860839844\n",
      "step 12 : loss 81.76445770263672\n",
      "step 13 : loss 86.25621795654297\n",
      "step 14 : loss 76.3280258178711\n",
      "step 15 : loss 72.77108764648438\n",
      "step 16 : loss 71.07861328125\n",
      "step 17 : loss 73.41088104248047\n",
      "step 18 : loss 73.78038787841797\n",
      "step 19 : loss 70.8030776977539\n",
      "step 20 : loss 68.08450317382812\n",
      "step 21 : loss 66.84661102294922\n",
      "step 22 : loss 66.78334045410156\n",
      "step 23 : loss 68.0683364868164\n",
      "step 24 : loss 65.31645965576172\n",
      "step 25 : loss 63.663856506347656\n",
      "step 26 : loss 63.80863952636719\n",
      "step 27 : loss 63.99565887451172\n",
      "step 28 : loss 63.79499816894531\n",
      "step 29 : loss 62.354896545410156\n",
      "step 30 : loss 60.943817138671875\n",
      "step 31 : loss 60.99642562866211\n",
      "step 32 : loss 61.043270111083984\n",
      "step 33 : loss 60.586727142333984\n",
      "step 34 : loss 59.224578857421875\n",
      "step 35 : loss 58.405941009521484\n",
      "step 36 : loss 58.20148849487305\n",
      "step 37 : loss 57.181331634521484\n",
      "step 38 : loss 55.76832962036133\n",
      "step 39 : loss 54.13754653930664\n",
      "step 40 : loss 52.84649658203125\n",
      "step 41 : loss 51.961971282958984\n",
      "step 42 : loss 50.95427703857422\n",
      "step 43 : loss 50.62637710571289\n",
      "step 44 : loss 50.28693771362305\n",
      "step 45 : loss 49.86457443237305\n",
      "step 46 : loss 49.70966720581055\n",
      "step 47 : loss 49.3838996887207\n",
      "step 48 : loss 48.654666900634766\n",
      "step 49 : loss 48.38095474243164\n",
      "step 50 : loss 48.45933532714844\n",
      "step 51 : loss 48.51357650756836\n",
      "step 52 : loss 48.41413879394531\n",
      "step 53 : loss 47.95286560058594\n",
      "step 54 : loss 47.323524475097656\n",
      "step 55 : loss 46.884002685546875\n",
      "step 56 : loss 46.596370697021484\n",
      "step 57 : loss 46.32963180541992\n",
      "step 58 : loss 46.15067672729492\n",
      "step 59 : loss 45.95718002319336\n",
      "step 60 : loss 45.94125747680664\n",
      "step 61 : loss 45.94440460205078\n",
      "step 62 : loss 45.763397216796875\n",
      "step 63 : loss 45.700286865234375\n",
      "step 64 : loss 46.01798629760742\n",
      "step 65 : loss 46.20848846435547\n",
      "step 66 : loss 45.12454605102539\n",
      "step 67 : loss 44.55892562866211\n",
      "step 68 : loss 45.059349060058594\n",
      "step 69 : loss 44.9837646484375\n",
      "step 70 : loss 44.2747688293457\n",
      "step 71 : loss 43.86122131347656\n",
      "step 72 : loss 44.02800750732422\n",
      "step 73 : loss 43.93265914916992\n",
      "step 74 : loss 43.42898178100586\n",
      "step 75 : loss 43.23720169067383\n",
      "step 76 : loss 43.33354568481445\n",
      "step 77 : loss 43.08179473876953\n",
      "step 78 : loss 42.71293258666992\n",
      "step 79 : loss 42.63752746582031\n",
      "step 80 : loss 42.61653137207031\n",
      "step 81 : loss 42.364891052246094\n",
      "step 82 : loss 42.047035217285156\n",
      "step 83 : loss 41.928466796875\n",
      "step 84 : loss 41.892906188964844\n",
      "step 85 : loss 41.65016555786133\n",
      "step 86 : loss 41.38857650756836\n",
      "step 87 : loss 41.229827880859375\n",
      "step 88 : loss 41.11900329589844\n",
      "step 89 : loss 40.971580505371094\n",
      "step 90 : loss 40.77692413330078\n",
      "step 91 : loss 40.56049346923828\n",
      "step 92 : loss 40.4073486328125\n",
      "step 93 : loss 40.224693298339844\n",
      "step 94 : loss 40.062339782714844\n",
      "step 95 : loss 39.903564453125\n",
      "step 96 : loss 39.74951934814453\n",
      "step 97 : loss 39.66864776611328\n",
      "step 98 : loss 39.81256103515625\n",
      "step 99 : loss 40.928443908691406\n",
      "step 100 : loss 42.16999435424805\n",
      "step 101 : loss 41.91937255859375\n",
      "step 102 : loss 38.95450210571289\n",
      "step 103 : loss 41.75029754638672\n",
      "step 104 : loss 41.18024444580078\n",
      "step 105 : loss 39.47886276245117\n",
      "step 106 : loss 41.894691467285156\n",
      "step 107 : loss 38.63949203491211\n",
      "step 108 : loss 40.662681579589844\n",
      "step 109 : loss 38.50981140136719\n",
      "step 110 : loss 40.23104476928711\n",
      "step 111 : loss 38.61254119873047\n",
      "step 112 : loss 39.43855285644531\n",
      "step 113 : loss 38.36388397216797\n",
      "step 114 : loss 38.912200927734375\n",
      "step 115 : loss 38.471771240234375\n",
      "step 116 : loss 38.36586380004883\n",
      "step 117 : loss 38.348628997802734\n",
      "step 118 : loss 37.742305755615234\n",
      "step 119 : loss 38.20349884033203\n",
      "step 120 : loss 37.43111801147461\n",
      "step 121 : loss 37.934104919433594\n",
      "step 122 : loss 37.44595718383789\n",
      "step 123 : loss 37.32918930053711\n",
      "step 124 : loss 37.6207389831543\n",
      "step 125 : loss 36.99925231933594\n",
      "step 126 : loss 37.11031723022461\n",
      "step 127 : loss 37.180782318115234\n",
      "step 128 : loss 36.720298767089844\n",
      "step 129 : loss 36.74274826049805\n",
      "step 130 : loss 36.89958953857422\n",
      "step 131 : loss 36.576168060302734\n",
      "step 132 : loss 36.34732437133789\n",
      "step 133 : loss 36.47629165649414\n",
      "step 134 : loss 36.494873046875\n",
      "step 135 : loss 36.15702819824219\n",
      "step 136 : loss 36.056236267089844\n",
      "step 137 : loss 36.15949249267578\n",
      "step 138 : loss 36.08095932006836\n",
      "step 139 : loss 35.82089614868164\n",
      "step 140 : loss 35.630859375\n",
      "step 141 : loss 35.65541458129883\n",
      "step 142 : loss 35.81100845336914\n",
      "step 143 : loss 35.84747314453125\n",
      "step 144 : loss 35.78584671020508\n",
      "step 145 : loss 35.44245910644531\n",
      "step 146 : loss 35.117347717285156\n",
      "step 147 : loss 34.859615325927734\n",
      "step 148 : loss 34.75033187866211\n",
      "step 149 : loss 34.75375747680664\n",
      "step 150 : loss 34.917030334472656\n",
      "step 151 : loss 35.79273986816406\n",
      "step 152 : loss 36.47705078125\n",
      "step 153 : loss 36.74374008178711\n",
      "step 154 : loss 33.94550323486328\n",
      "step 155 : loss 33.23093032836914\n",
      "step 156 : loss 34.163414001464844\n",
      "step 157 : loss 33.647281646728516\n",
      "step 158 : loss 32.0488166809082\n",
      "step 159 : loss 30.518625259399414\n",
      "step 160 : loss 29.9046688079834\n",
      "step 161 : loss 30.19808578491211\n",
      "step 162 : loss 29.990501403808594\n",
      "step 163 : loss 29.639127731323242\n",
      "step 164 : loss 26.471872329711914\n",
      "step 165 : loss 25.64653205871582\n",
      "step 166 : loss 25.030057907104492\n",
      "step 167 : loss 26.21363067626953\n",
      "step 168 : loss 27.083717346191406\n",
      "step 169 : loss 24.198049545288086\n",
      "step 170 : loss 24.285228729248047\n",
      "step 171 : loss 25.719383239746094\n",
      "step 172 : loss 23.92727279663086\n",
      "step 173 : loss 23.1285343170166\n",
      "step 174 : loss 23.89081573486328\n",
      "step 175 : loss 22.80078125\n",
      "step 176 : loss 22.324533462524414\n",
      "step 177 : loss 22.586978912353516\n",
      "step 178 : loss 22.322460174560547\n",
      "step 179 : loss 22.02120590209961\n",
      "step 180 : loss 21.479990005493164\n",
      "step 181 : loss 21.47810935974121\n",
      "step 182 : loss 21.572511672973633\n",
      "step 183 : loss 21.694015502929688\n",
      "step 184 : loss 21.965787887573242\n",
      "step 185 : loss 21.68510627746582\n",
      "step 186 : loss 21.829853057861328\n",
      "step 187 : loss 21.38489532470703\n",
      "step 188 : loss 21.26563835144043\n",
      "step 189 : loss 21.09311294555664\n",
      "step 190 : loss 20.78396987915039\n",
      "step 191 : loss 20.738666534423828\n",
      "step 192 : loss 20.800579071044922\n",
      "step 193 : loss 20.37358856201172\n",
      "step 194 : loss 20.366758346557617\n",
      "step 195 : loss 20.18702507019043\n",
      "step 196 : loss 20.195556640625\n",
      "step 197 : loss 20.30845069885254\n",
      "step 198 : loss 20.433801651000977\n",
      "step 199 : loss 20.595947265625\n",
      "step 200 : loss 21.544960021972656\n",
      "step 201 : loss 20.915746688842773\n",
      "step 202 : loss 20.72002410888672\n",
      "step 203 : loss 19.891841888427734\n",
      "step 204 : loss 19.566198348999023\n",
      "step 205 : loss 19.32898712158203\n",
      "step 206 : loss 19.296857833862305\n",
      "step 207 : loss 19.31718635559082\n",
      "step 208 : loss 19.70417594909668\n",
      "step 209 : loss 19.941274642944336\n",
      "step 210 : loss 20.379920959472656\n",
      "step 211 : loss 19.584665298461914\n",
      "step 212 : loss 19.10517120361328\n",
      "step 213 : loss 18.49188232421875\n",
      "step 214 : loss 18.242876052856445\n",
      "step 215 : loss 18.17605972290039\n",
      "step 216 : loss 18.163150787353516\n",
      "step 217 : loss 18.237220764160156\n",
      "step 218 : loss 18.587665557861328\n",
      "step 219 : loss 19.33892822265625\n",
      "step 220 : loss 19.527618408203125\n",
      "step 221 : loss 20.131010055541992\n",
      "step 222 : loss 19.3875732421875\n",
      "step 223 : loss 19.660430908203125\n",
      "step 224 : loss 19.43899154663086\n",
      "step 225 : loss 18.13521385192871\n",
      "step 226 : loss 17.449460983276367\n",
      "step 227 : loss 17.672964096069336\n",
      "step 228 : loss 18.202016830444336\n",
      "step 229 : loss 18.40296745300293\n",
      "step 230 : loss 18.049776077270508\n",
      "step 231 : loss 18.259611129760742\n",
      "step 232 : loss 19.612199783325195\n",
      "step 233 : loss 19.164838790893555\n",
      "step 234 : loss 18.598079681396484\n",
      "step 235 : loss 17.71863555908203\n",
      "step 236 : loss 17.737382888793945\n",
      "step 237 : loss 17.589637756347656\n",
      "step 238 : loss 16.967248916625977\n",
      "step 239 : loss 16.84178352355957\n",
      "step 240 : loss 17.205080032348633\n",
      "step 241 : loss 17.133045196533203\n",
      "step 242 : loss 16.84270668029785\n",
      "step 243 : loss 17.12655258178711\n",
      "step 244 : loss 18.145116806030273\n",
      "step 245 : loss 18.923694610595703\n",
      "step 246 : loss 19.754953384399414\n",
      "step 247 : loss 17.44856834411621\n",
      "step 248 : loss 16.608333587646484\n",
      "step 249 : loss 16.538543701171875\n",
      "step 250 : loss 17.078737258911133\n",
      "step 251 : loss 18.342859268188477\n",
      "step 252 : loss 17.924787521362305\n",
      "step 253 : loss 17.413537979125977\n",
      "step 254 : loss 16.346057891845703\n",
      "step 255 : loss 16.065916061401367\n",
      "step 256 : loss 16.29437255859375\n",
      "step 257 : loss 16.561893463134766\n",
      "step 258 : loss 17.138906478881836\n",
      "step 259 : loss 17.38401985168457\n",
      "step 260 : loss 17.553266525268555\n",
      "step 261 : loss 16.679706573486328\n",
      "step 262 : loss 16.23798942565918\n",
      "step 263 : loss 15.890642166137695\n",
      "step 264 : loss 15.66486930847168\n",
      "step 265 : loss 15.634231567382812\n",
      "step 266 : loss 15.695473670959473\n",
      "step 267 : loss 15.619308471679688\n",
      "step 268 : loss 15.537410736083984\n",
      "step 269 : loss 15.523162841796875\n",
      "step 270 : loss 15.584540367126465\n",
      "step 271 : loss 15.63979721069336\n",
      "step 272 : loss 16.00535774230957\n",
      "step 273 : loss 17.216516494750977\n",
      "step 274 : loss 19.76700782775879\n",
      "step 275 : loss 16.651283264160156\n",
      "step 276 : loss 15.782896041870117\n",
      "step 277 : loss 15.876415252685547\n",
      "step 278 : loss 15.83631706237793\n",
      "step 279 : loss 15.754828453063965\n",
      "step 280 : loss 15.94836711883545\n",
      "step 281 : loss 16.993425369262695\n",
      "step 282 : loss 16.820615768432617\n",
      "step 283 : loss 16.480934143066406\n",
      "step 284 : loss 15.48575496673584\n",
      "step 285 : loss 15.090764045715332\n",
      "step 286 : loss 15.206304550170898\n",
      "step 287 : loss 15.57729434967041\n",
      "step 288 : loss 15.41649055480957\n",
      "step 289 : loss 15.510833740234375\n",
      "step 290 : loss 15.997441291809082\n",
      "step 291 : loss 16.79648208618164\n",
      "step 292 : loss 15.64448356628418\n",
      "step 293 : loss 15.137529373168945\n",
      "step 294 : loss 14.974143981933594\n",
      "step 295 : loss 14.854947090148926\n",
      "step 296 : loss 14.65846061706543\n",
      "step 297 : loss 14.756803512573242\n",
      "step 298 : loss 15.039978981018066\n",
      "step 299 : loss 15.355804443359375\n",
      "step 300 : loss 15.5686674118042\n",
      "step 301 : loss 16.313661575317383\n",
      "step 302 : loss 15.388450622558594\n",
      "step 303 : loss 14.704734802246094\n",
      "step 304 : loss 14.468219757080078\n",
      "step 305 : loss 14.679194450378418\n",
      "step 306 : loss 14.783397674560547\n",
      "step 307 : loss 15.036439895629883\n",
      "step 308 : loss 15.321929931640625\n",
      "step 309 : loss 15.934487342834473\n",
      "step 310 : loss 14.897844314575195\n",
      "step 311 : loss 14.552460670471191\n",
      "step 312 : loss 14.525957107543945\n",
      "step 313 : loss 14.29552173614502\n",
      "step 314 : loss 14.066481590270996\n",
      "step 315 : loss 14.280749320983887\n",
      "step 316 : loss 14.752145767211914\n",
      "step 317 : loss 15.298076629638672\n",
      "step 318 : loss 14.868318557739258\n",
      "step 319 : loss 15.023163795471191\n",
      "step 320 : loss 14.510539054870605\n",
      "step 321 : loss 14.036078453063965\n",
      "step 322 : loss 13.932140350341797\n",
      "step 323 : loss 14.164996147155762\n",
      "step 324 : loss 14.196941375732422\n",
      "step 325 : loss 14.329401969909668\n",
      "step 326 : loss 14.495112419128418\n",
      "step 327 : loss 14.945558547973633\n",
      "step 328 : loss 14.343594551086426\n",
      "step 329 : loss 14.090128898620605\n",
      "step 330 : loss 14.027437210083008\n",
      "step 331 : loss 14.181233406066895\n",
      "step 332 : loss 13.883963584899902\n",
      "step 333 : loss 13.917831420898438\n",
      "step 334 : loss 14.175715446472168\n",
      "step 335 : loss 14.427000045776367\n",
      "step 336 : loss 14.012969970703125\n",
      "step 337 : loss 14.05518627166748\n",
      "step 338 : loss 14.037589073181152\n",
      "step 339 : loss 13.948545455932617\n",
      "step 340 : loss 13.64371109008789\n",
      "step 341 : loss 13.811859130859375\n",
      "step 342 : loss 13.910348892211914\n",
      "step 343 : loss 13.84517765045166\n",
      "step 344 : loss 13.709904670715332\n",
      "step 345 : loss 14.093892097473145\n",
      "step 346 : loss 13.898393630981445\n",
      "step 347 : loss 13.73373794555664\n",
      "step 348 : loss 13.633733749389648\n",
      "step 349 : loss 13.927546501159668\n",
      "step 350 : loss 13.655162811279297\n",
      "step 351 : loss 13.480751991271973\n",
      "step 352 : loss 13.5386962890625\n",
      "step 353 : loss 13.797337532043457\n",
      "step 354 : loss 13.468093872070312\n",
      "step 355 : loss 13.457319259643555\n",
      "step 356 : loss 13.624016761779785\n",
      "step 357 : loss 13.596769332885742\n",
      "step 358 : loss 13.260579109191895\n",
      "step 359 : loss 13.291657447814941\n",
      "step 360 : loss 13.436038970947266\n",
      "step 361 : loss 13.633488655090332\n",
      "step 362 : loss 13.498880386352539\n",
      "step 363 : loss 13.787864685058594\n",
      "step 364 : loss 13.49400520324707\n",
      "step 365 : loss 13.101341247558594\n",
      "step 366 : loss 12.873981475830078\n",
      "step 367 : loss 13.080109596252441\n",
      "step 368 : loss 13.097647666931152\n",
      "step 369 : loss 13.0554780960083\n",
      "step 370 : loss 13.134352684020996\n",
      "step 371 : loss 13.466496467590332\n",
      "step 372 : loss 13.309005737304688\n",
      "step 373 : loss 13.19672679901123\n",
      "step 374 : loss 13.183012008666992\n",
      "step 375 : loss 13.54149341583252\n",
      "step 376 : loss 13.246088981628418\n",
      "step 377 : loss 13.18101978302002\n",
      "step 378 : loss 12.985468864440918\n",
      "step 379 : loss 12.89880084991455\n",
      "step 380 : loss 12.730180740356445\n",
      "step 381 : loss 12.797561645507812\n",
      "step 382 : loss 12.94697093963623\n",
      "step 383 : loss 13.263925552368164\n",
      "step 384 : loss 13.03448486328125\n",
      "step 385 : loss 12.950915336608887\n",
      "step 386 : loss 12.79728889465332\n",
      "step 387 : loss 12.860175132751465\n",
      "step 388 : loss 12.841506958007812\n",
      "step 389 : loss 12.970816612243652\n",
      "step 390 : loss 12.874464988708496\n",
      "step 391 : loss 12.871102333068848\n",
      "step 392 : loss 12.611038208007812\n",
      "step 393 : loss 12.542449951171875\n",
      "step 394 : loss 12.500337600708008\n",
      "step 395 : loss 12.684903144836426\n",
      "step 396 : loss 12.738836288452148\n",
      "step 397 : loss 12.8304443359375\n",
      "step 398 : loss 12.628392219543457\n",
      "step 399 : loss 12.601045608520508\n",
      "step 400 : loss 12.533204078674316\n",
      "step 401 : loss 12.627192497253418\n",
      "step 402 : loss 12.538830757141113\n",
      "step 403 : loss 12.672795295715332\n",
      "step 404 : loss 12.63681697845459\n",
      "step 405 : loss 12.677774429321289\n",
      "step 406 : loss 12.460494995117188\n",
      "step 407 : loss 12.386627197265625\n",
      "step 408 : loss 12.30882740020752\n",
      "step 409 : loss 12.470404624938965\n",
      "step 410 : loss 12.495502471923828\n",
      "step 411 : loss 12.627927780151367\n",
      "step 412 : loss 12.47291374206543\n",
      "step 413 : loss 12.52355670928955\n",
      "step 414 : loss 12.367401123046875\n",
      "step 415 : loss 12.298141479492188\n",
      "step 416 : loss 12.196076393127441\n",
      "step 417 : loss 12.273987770080566\n",
      "step 418 : loss 12.305818557739258\n",
      "step 419 : loss 12.49599838256836\n",
      "step 420 : loss 12.376209259033203\n",
      "step 421 : loss 12.401315689086914\n",
      "step 422 : loss 12.166304588317871\n",
      "step 423 : loss 12.119049072265625\n",
      "step 424 : loss 12.0248384475708\n",
      "step 425 : loss 12.100102424621582\n",
      "step 426 : loss 12.098660469055176\n",
      "step 427 : loss 12.244488716125488\n",
      "step 428 : loss 12.173576354980469\n",
      "step 429 : loss 12.227644920349121\n",
      "step 430 : loss 12.089479446411133\n",
      "step 431 : loss 12.109018325805664\n",
      "step 432 : loss 12.03868293762207\n",
      "step 433 : loss 12.127619743347168\n",
      "step 434 : loss 12.084282875061035\n",
      "step 435 : loss 12.165911674499512\n",
      "step 436 : loss 12.043135643005371\n",
      "step 437 : loss 12.026935577392578\n",
      "step 438 : loss 11.98261833190918\n",
      "step 439 : loss 11.977261543273926\n",
      "step 440 : loss 11.945316314697266\n",
      "step 441 : loss 11.909646034240723\n",
      "step 442 : loss 11.821669578552246\n",
      "step 443 : loss 11.752453804016113\n",
      "step 444 : loss 11.706768989562988\n",
      "step 445 : loss 11.844754219055176\n",
      "step 446 : loss 11.992045402526855\n",
      "step 447 : loss 12.270109176635742\n",
      "step 448 : loss 12.14488697052002\n",
      "step 449 : loss 12.296576499938965\n",
      "step 450 : loss 12.080113410949707\n",
      "step 451 : loss 11.881705284118652\n",
      "step 452 : loss 11.558211326599121\n",
      "step 453 : loss 11.45521354675293\n",
      "step 454 : loss 11.463193893432617\n",
      "step 455 : loss 11.70260238647461\n",
      "step 456 : loss 11.916048049926758\n",
      "step 457 : loss 12.158339500427246\n",
      "step 458 : loss 11.884665489196777\n",
      "step 459 : loss 11.797896385192871\n",
      "step 460 : loss 11.72311019897461\n",
      "step 461 : loss 11.797839164733887\n",
      "step 462 : loss 11.793574333190918\n",
      "step 463 : loss 11.933829307556152\n",
      "step 464 : loss 11.890664100646973\n",
      "step 465 : loss 11.84549617767334\n",
      "step 466 : loss 11.572396278381348\n",
      "step 467 : loss 11.508946418762207\n",
      "step 468 : loss 11.469391822814941\n",
      "step 469 : loss 11.662267684936523\n",
      "step 470 : loss 11.768369674682617\n",
      "step 471 : loss 11.949136734008789\n",
      "step 472 : loss 11.634586334228516\n",
      "step 473 : loss 11.498312950134277\n",
      "step 474 : loss 11.386463165283203\n",
      "step 475 : loss 11.467694282531738\n",
      "step 476 : loss 11.605674743652344\n",
      "step 477 : loss 11.825379371643066\n",
      "step 478 : loss 11.697202682495117\n",
      "step 479 : loss 11.540410995483398\n",
      "step 480 : loss 11.351165771484375\n",
      "step 481 : loss 11.296942710876465\n",
      "step 482 : loss 11.370513916015625\n",
      "step 483 : loss 11.519058227539062\n",
      "step 484 : loss 11.557181358337402\n",
      "step 485 : loss 11.555525779724121\n",
      "step 486 : loss 11.341811180114746\n",
      "step 487 : loss 11.313794136047363\n",
      "step 488 : loss 11.412168502807617\n",
      "step 489 : loss 11.82646656036377\n",
      "step 490 : loss 11.814327239990234\n",
      "step 491 : loss 11.755550384521484\n",
      "step 492 : loss 11.289520263671875\n",
      "step 493 : loss 11.078436851501465\n",
      "step 494 : loss 10.969843864440918\n",
      "step 495 : loss 10.961431503295898\n",
      "step 496 : loss 11.021413803100586\n",
      "step 497 : loss 11.25322151184082\n",
      "step 498 : loss 11.415990829467773\n",
      "step 499 : loss 11.691773414611816\n",
      "step 500 : loss 11.456382751464844\n",
      "step 501 : loss 11.39008903503418\n",
      "step 502 : loss 11.26904010772705\n",
      "step 503 : loss 11.322885513305664\n",
      "step 504 : loss 11.337346076965332\n",
      "step 505 : loss 11.4228515625\n",
      "step 506 : loss 11.294291496276855\n",
      "step 507 : loss 11.274153709411621\n",
      "step 508 : loss 11.173998832702637\n",
      "step 509 : loss 11.171238899230957\n",
      "step 510 : loss 11.092947959899902\n",
      "step 511 : loss 11.10728645324707\n",
      "step 512 : loss 11.070595741271973\n",
      "step 513 : loss 11.228620529174805\n",
      "step 514 : loss 11.323884010314941\n",
      "step 515 : loss 11.488941192626953\n",
      "step 516 : loss 11.252676963806152\n",
      "step 517 : loss 11.107614517211914\n",
      "step 518 : loss 10.921246528625488\n",
      "step 519 : loss 10.931991577148438\n",
      "step 520 : loss 10.99386978149414\n",
      "step 521 : loss 11.189231872558594\n",
      "step 522 : loss 11.155539512634277\n",
      "step 523 : loss 11.228167533874512\n",
      "step 524 : loss 11.099662780761719\n",
      "step 525 : loss 11.081217765808105\n",
      "step 526 : loss 10.990548133850098\n",
      "step 527 : loss 10.96832275390625\n",
      "step 528 : loss 10.933074951171875\n",
      "step 529 : loss 10.922395706176758\n",
      "step 530 : loss 10.950352668762207\n",
      "step 531 : loss 11.066110610961914\n",
      "step 532 : loss 11.008051872253418\n",
      "step 533 : loss 11.02513313293457\n",
      "step 534 : loss 10.94974422454834\n",
      "step 535 : loss 11.1080322265625\n",
      "step 536 : loss 11.0663423538208\n",
      "step 537 : loss 11.168354988098145\n",
      "step 538 : loss 10.944632530212402\n",
      "step 539 : loss 10.869921684265137\n",
      "step 540 : loss 10.788780212402344\n",
      "step 541 : loss 10.825247764587402\n",
      "step 542 : loss 10.810328483581543\n",
      "step 543 : loss 10.854772567749023\n",
      "step 544 : loss 10.823060035705566\n",
      "step 545 : loss 10.905762672424316\n",
      "step 546 : loss 10.904461860656738\n",
      "step 547 : loss 11.029306411743164\n",
      "step 548 : loss 10.984528541564941\n",
      "step 549 : loss 11.02085018157959\n",
      "step 550 : loss 10.886034965515137\n",
      "step 551 : loss 10.784904479980469\n",
      "step 552 : loss 10.711722373962402\n",
      "step 553 : loss 10.686518669128418\n",
      "step 554 : loss 10.667704582214355\n",
      "step 555 : loss 10.661934852600098\n",
      "step 556 : loss 10.657400131225586\n",
      "step 557 : loss 10.714132308959961\n",
      "step 558 : loss 10.693760871887207\n",
      "step 559 : loss 10.792609214782715\n",
      "step 560 : loss 10.821170806884766\n",
      "step 561 : loss 11.009713172912598\n",
      "step 562 : loss 10.964369773864746\n",
      "step 563 : loss 10.99514389038086\n",
      "step 564 : loss 10.71876049041748\n",
      "step 565 : loss 10.588235855102539\n",
      "step 566 : loss 10.484624862670898\n",
      "step 567 : loss 10.477880477905273\n",
      "step 568 : loss 10.520889282226562\n",
      "step 569 : loss 10.708731651306152\n",
      "step 570 : loss 10.802067756652832\n",
      "step 571 : loss 10.963134765625\n",
      "step 572 : loss 10.782645225524902\n",
      "step 573 : loss 10.732548713684082\n",
      "step 574 : loss 10.552774429321289\n",
      "step 575 : loss 10.519046783447266\n",
      "step 576 : loss 10.514667510986328\n",
      "step 577 : loss 10.631126403808594\n",
      "step 578 : loss 10.63843822479248\n",
      "step 579 : loss 10.783881187438965\n",
      "step 580 : loss 10.64266300201416\n",
      "step 581 : loss 10.625669479370117\n",
      "step 582 : loss 10.515894889831543\n",
      "step 583 : loss 10.488046646118164\n",
      "step 584 : loss 10.428725242614746\n",
      "step 585 : loss 10.45481014251709\n",
      "step 586 : loss 10.434513092041016\n",
      "step 587 : loss 10.506978988647461\n",
      "step 588 : loss 10.486515045166016\n",
      "step 589 : loss 10.539861679077148\n",
      "step 590 : loss 10.537917137145996\n",
      "step 591 : loss 10.679466247558594\n",
      "step 592 : loss 10.619352340698242\n",
      "step 593 : loss 10.695124626159668\n",
      "step 594 : loss 10.597234725952148\n",
      "step 595 : loss 10.640739440917969\n",
      "step 596 : loss 10.616155624389648\n",
      "step 597 : loss 10.602094650268555\n",
      "step 598 : loss 10.565885543823242\n",
      "step 599 : loss 10.421231269836426\n",
      "step 600 : loss 10.307493209838867\n",
      "step 601 : loss 10.182914733886719\n",
      "step 602 : loss 10.134243965148926\n",
      "step 603 : loss 10.109121322631836\n",
      "step 604 : loss 10.158743858337402\n",
      "step 605 : loss 10.213040351867676\n",
      "step 606 : loss 10.306994438171387\n",
      "step 607 : loss 10.398247718811035\n",
      "step 608 : loss 10.67947769165039\n",
      "step 609 : loss 10.803133010864258\n",
      "step 610 : loss 10.911527633666992\n",
      "step 611 : loss 10.433826446533203\n",
      "step 612 : loss 10.1557035446167\n",
      "step 613 : loss 10.021697044372559\n",
      "step 614 : loss 10.109405517578125\n",
      "step 615 : loss 10.288078308105469\n",
      "step 616 : loss 10.604619979858398\n",
      "step 617 : loss 10.715325355529785\n",
      "step 618 : loss 10.822383880615234\n",
      "step 619 : loss 10.372357368469238\n",
      "step 620 : loss 10.19515609741211\n",
      "step 621 : loss 10.1113920211792\n",
      "step 622 : loss 10.206731796264648\n",
      "step 623 : loss 10.333974838256836\n",
      "step 624 : loss 10.545699119567871\n",
      "step 625 : loss 10.510296821594238\n",
      "step 626 : loss 10.445523262023926\n",
      "step 627 : loss 10.239092826843262\n",
      "step 628 : loss 10.18563461303711\n",
      "step 629 : loss 10.154885292053223\n",
      "step 630 : loss 10.288819313049316\n",
      "step 631 : loss 10.385648727416992\n",
      "step 632 : loss 10.576136589050293\n",
      "step 633 : loss 10.421754837036133\n",
      "step 634 : loss 10.371061325073242\n",
      "step 635 : loss 10.237847328186035\n",
      "step 636 : loss 10.183205604553223\n",
      "step 637 : loss 10.1510591506958\n",
      "step 638 : loss 10.146954536437988\n",
      "step 639 : loss 10.172805786132812\n",
      "step 640 : loss 10.159549713134766\n",
      "step 641 : loss 10.132521629333496\n",
      "step 642 : loss 10.120686531066895\n",
      "step 643 : loss 10.107015609741211\n",
      "step 644 : loss 10.098119735717773\n",
      "step 645 : loss 10.062100410461426\n",
      "step 646 : loss 10.117044448852539\n",
      "step 647 : loss 10.208112716674805\n",
      "step 648 : loss 10.48119831085205\n",
      "step 649 : loss 10.529699325561523\n",
      "step 650 : loss 10.610970497131348\n",
      "step 651 : loss 10.271167755126953\n",
      "step 652 : loss 10.098127365112305\n",
      "step 653 : loss 9.916112899780273\n",
      "step 654 : loss 9.866223335266113\n",
      "step 655 : loss 9.857280731201172\n",
      "step 656 : loss 9.969619750976562\n",
      "step 657 : loss 10.158405303955078\n",
      "step 658 : loss 10.537641525268555\n",
      "step 659 : loss 10.467761993408203\n",
      "step 660 : loss 10.363216400146484\n",
      "step 661 : loss 10.005041122436523\n",
      "step 662 : loss 9.865509986877441\n",
      "step 663 : loss 9.808626174926758\n",
      "step 664 : loss 9.895438194274902\n",
      "step 665 : loss 10.037090301513672\n",
      "step 666 : loss 10.347390174865723\n",
      "step 667 : loss 10.340873718261719\n",
      "step 668 : loss 10.337053298950195\n",
      "step 669 : loss 10.041303634643555\n",
      "step 670 : loss 9.877473831176758\n",
      "step 671 : loss 9.81547737121582\n",
      "step 672 : loss 9.862812042236328\n",
      "step 673 : loss 9.951404571533203\n",
      "step 674 : loss 10.184225082397461\n",
      "step 675 : loss 10.234858512878418\n",
      "step 676 : loss 10.303742408752441\n",
      "step 677 : loss 10.032301902770996\n",
      "step 678 : loss 9.885759353637695\n",
      "step 679 : loss 9.807453155517578\n",
      "step 680 : loss 9.82026481628418\n",
      "step 681 : loss 9.861045837402344\n",
      "step 682 : loss 10.037449836730957\n",
      "step 683 : loss 10.090803146362305\n",
      "step 684 : loss 10.228869438171387\n",
      "step 685 : loss 10.049453735351562\n",
      "step 686 : loss 9.952946662902832\n",
      "step 687 : loss 9.82721996307373\n",
      "step 688 : loss 9.871533393859863\n",
      "step 689 : loss 9.932924270629883\n",
      "step 690 : loss 10.145119667053223\n",
      "step 691 : loss 10.17044734954834\n",
      "step 692 : loss 10.202529907226562\n",
      "step 693 : loss 10.010315895080566\n",
      "step 694 : loss 9.904541969299316\n",
      "step 695 : loss 9.812318801879883\n",
      "step 696 : loss 9.82277774810791\n",
      "step 697 : loss 9.85263442993164\n",
      "step 698 : loss 9.94666862487793\n",
      "step 699 : loss 9.984630584716797\n",
      "step 700 : loss 9.928984642028809\n",
      "step 701 : loss 9.85888385772705\n",
      "step 702 : loss 9.784846305847168\n",
      "step 703 : loss 9.714567184448242\n",
      "step 704 : loss 9.706217765808105\n",
      "step 705 : loss 9.736417770385742\n",
      "step 706 : loss 9.925527572631836\n",
      "step 707 : loss 10.112245559692383\n",
      "step 708 : loss 10.418787956237793\n",
      "step 709 : loss 10.20646858215332\n",
      "step 710 : loss 10.119867324829102\n",
      "step 711 : loss 9.86310863494873\n",
      "step 712 : loss 9.79214859008789\n",
      "step 713 : loss 9.691106796264648\n",
      "step 714 : loss 9.657415390014648\n",
      "step 715 : loss 9.64239501953125\n",
      "step 716 : loss 9.71104907989502\n",
      "step 717 : loss 9.8139066696167\n",
      "step 718 : loss 10.050283432006836\n",
      "step 719 : loss 10.091733932495117\n",
      "step 720 : loss 10.16115665435791\n",
      "step 721 : loss 9.9447021484375\n",
      "step 722 : loss 9.873120307922363\n",
      "step 723 : loss 9.781861305236816\n",
      "step 724 : loss 9.815731048583984\n",
      "step 725 : loss 9.810004234313965\n",
      "step 726 : loss 9.850750923156738\n",
      "step 727 : loss 9.798089027404785\n",
      "step 728 : loss 9.870416641235352\n",
      "step 729 : loss 9.837215423583984\n",
      "step 730 : loss 9.94493579864502\n",
      "step 731 : loss 9.913457870483398\n",
      "step 732 : loss 9.966958045959473\n",
      "step 733 : loss 9.84477710723877\n",
      "step 734 : loss 9.828932762145996\n",
      "step 735 : loss 9.759414672851562\n",
      "step 736 : loss 9.790966987609863\n",
      "step 737 : loss 9.749197006225586\n",
      "step 738 : loss 9.792572975158691\n",
      "step 739 : loss 9.760137557983398\n",
      "step 740 : loss 9.813824653625488\n",
      "step 741 : loss 9.794055938720703\n",
      "step 742 : loss 9.862560272216797\n",
      "step 743 : loss 9.77436637878418\n",
      "step 744 : loss 9.788497924804688\n",
      "step 745 : loss 9.677976608276367\n",
      "step 746 : loss 9.680669784545898\n",
      "step 747 : loss 9.642989158630371\n",
      "step 748 : loss 9.696954727172852\n",
      "step 749 : loss 9.730010986328125\n",
      "step 750 : loss 9.872885704040527\n",
      "step 751 : loss 9.797270774841309\n",
      "step 752 : loss 9.792562484741211\n",
      "step 753 : loss 9.66257381439209\n",
      "step 754 : loss 9.645401954650879\n",
      "step 755 : loss 9.608968734741211\n",
      "step 756 : loss 9.655937194824219\n",
      "step 757 : loss 9.67767333984375\n",
      "step 758 : loss 9.760336875915527\n",
      "step 759 : loss 9.739319801330566\n",
      "step 760 : loss 9.764354705810547\n",
      "step 761 : loss 9.725418090820312\n",
      "step 762 : loss 9.77440071105957\n",
      "step 763 : loss 9.769388198852539\n",
      "step 764 : loss 9.760587692260742\n",
      "step 765 : loss 9.633612632751465\n",
      "step 766 : loss 9.546201705932617\n",
      "step 767 : loss 9.510574340820312\n",
      "step 768 : loss 9.585494995117188\n",
      "step 769 : loss 9.669783592224121\n",
      "step 770 : loss 9.763381958007812\n",
      "step 771 : loss 9.651153564453125\n",
      "step 772 : loss 9.58230972290039\n",
      "step 773 : loss 9.56244945526123\n",
      "step 774 : loss 9.705178260803223\n",
      "step 775 : loss 9.72829532623291\n",
      "step 776 : loss 9.753332138061523\n",
      "step 777 : loss 9.659000396728516\n",
      "step 778 : loss 9.659942626953125\n",
      "step 779 : loss 9.602261543273926\n",
      "step 780 : loss 9.571737289428711\n",
      "step 781 : loss 9.533774375915527\n",
      "step 782 : loss 9.645737648010254\n",
      "step 783 : loss 9.741965293884277\n",
      "step 784 : loss 9.877004623413086\n",
      "step 785 : loss 9.70555305480957\n",
      "step 786 : loss 9.639816284179688\n",
      "step 787 : loss 9.514005661010742\n",
      "step 788 : loss 9.478397369384766\n",
      "step 789 : loss 9.46593952178955\n",
      "step 790 : loss 9.577362060546875\n",
      "step 791 : loss 9.654855728149414\n",
      "step 792 : loss 9.814851760864258\n",
      "step 793 : loss 9.685577392578125\n",
      "step 794 : loss 9.609207153320312\n",
      "step 795 : loss 9.47716236114502\n",
      "step 796 : loss 9.447036743164062\n",
      "step 797 : loss 9.417352676391602\n",
      "step 798 : loss 9.474689483642578\n",
      "step 799 : loss 9.554464340209961\n",
      "step 800 : loss 9.739542961120605\n",
      "step 801 : loss 9.860880851745605\n",
      "step 802 : loss 9.827181816101074\n",
      "step 803 : loss 9.798039436340332\n",
      "step 804 : loss 9.682575225830078\n",
      "step 805 : loss 9.755563735961914\n",
      "step 806 : loss 9.684618949890137\n",
      "step 807 : loss 9.682621002197266\n",
      "step 808 : loss 9.485528945922852\n",
      "step 809 : loss 9.39639949798584\n",
      "step 810 : loss 9.351055145263672\n",
      "step 811 : loss 9.418152809143066\n",
      "step 812 : loss 9.497547149658203\n",
      "step 813 : loss 9.671722412109375\n",
      "step 814 : loss 9.686884880065918\n",
      "step 815 : loss 9.711427688598633\n",
      "step 816 : loss 9.572094917297363\n",
      "step 817 : loss 9.501846313476562\n",
      "step 818 : loss 9.448217391967773\n",
      "step 819 : loss 9.479328155517578\n",
      "step 820 : loss 9.49159049987793\n",
      "step 821 : loss 9.569263458251953\n",
      "step 822 : loss 9.55343246459961\n",
      "step 823 : loss 9.551298141479492\n",
      "step 824 : loss 9.447586059570312\n",
      "step 825 : loss 9.43920612335205\n",
      "step 826 : loss 9.451467514038086\n",
      "step 827 : loss 9.558972358703613\n",
      "step 828 : loss 9.552953720092773\n",
      "step 829 : loss 9.532987594604492\n",
      "step 830 : loss 9.391813278198242\n",
      "step 831 : loss 9.309774398803711\n",
      "step 832 : loss 9.310530662536621\n",
      "step 833 : loss 9.432032585144043\n",
      "step 834 : loss 9.547459602355957\n",
      "step 835 : loss 9.709691047668457\n",
      "step 836 : loss 9.608590126037598\n",
      "step 837 : loss 9.557173728942871\n",
      "step 838 : loss 9.393522262573242\n",
      "step 839 : loss 9.371135711669922\n",
      "step 840 : loss 9.332157135009766\n",
      "step 841 : loss 9.413016319274902\n",
      "step 842 : loss 9.435550689697266\n",
      "step 843 : loss 9.54586124420166\n",
      "step 844 : loss 9.510091781616211\n",
      "step 845 : loss 9.533220291137695\n",
      "step 846 : loss 9.424206733703613\n",
      "step 847 : loss 9.3923978805542\n",
      "step 848 : loss 9.316863059997559\n",
      "step 849 : loss 9.356881141662598\n",
      "step 850 : loss 9.378419876098633\n",
      "step 851 : loss 9.492189407348633\n",
      "step 852 : loss 9.47802448272705\n",
      "step 853 : loss 9.507301330566406\n",
      "step 854 : loss 9.413780212402344\n",
      "step 855 : loss 9.391379356384277\n",
      "step 856 : loss 9.360956192016602\n",
      "step 857 : loss 9.410299301147461\n",
      "step 858 : loss 9.404643058776855\n",
      "step 859 : loss 9.446910858154297\n",
      "step 860 : loss 9.414395332336426\n",
      "step 861 : loss 9.42807388305664\n",
      "step 862 : loss 9.364039421081543\n",
      "step 863 : loss 9.35057258605957\n",
      "step 864 : loss 9.321878433227539\n",
      "step 865 : loss 9.331438064575195\n",
      "step 866 : loss 9.338799476623535\n",
      "step 867 : loss 9.387147903442383\n",
      "step 868 : loss 9.388994216918945\n",
      "step 869 : loss 9.353719711303711\n",
      "step 870 : loss 9.301929473876953\n",
      "step 871 : loss 9.29275894165039\n",
      "step 872 : loss 9.25670051574707\n",
      "step 873 : loss 9.297550201416016\n",
      "step 874 : loss 9.29690933227539\n",
      "step 875 : loss 9.348695755004883\n",
      "step 876 : loss 9.387689590454102\n",
      "step 877 : loss 9.432259559631348\n",
      "step 878 : loss 9.381576538085938\n",
      "step 879 : loss 9.406001091003418\n",
      "step 880 : loss 9.415311813354492\n",
      "step 881 : loss 9.607794761657715\n",
      "step 882 : loss 9.627055168151855\n",
      "step 883 : loss 9.61139965057373\n",
      "step 884 : loss 9.342086791992188\n",
      "step 885 : loss 9.27474308013916\n",
      "step 886 : loss 9.285317420959473\n",
      "step 887 : loss 9.338949203491211\n",
      "step 888 : loss 9.310505867004395\n",
      "step 889 : loss 9.33261489868164\n",
      "step 890 : loss 9.323559761047363\n",
      "step 891 : loss 9.424735069274902\n",
      "step 892 : loss 9.44426155090332\n",
      "step 893 : loss 9.516646385192871\n",
      "step 894 : loss 9.378071784973145\n",
      "step 895 : loss 9.333809852600098\n",
      "step 896 : loss 9.26647663116455\n",
      "step 897 : loss 9.301815032958984\n",
      "step 898 : loss 9.274635314941406\n",
      "step 899 : loss 9.275589942932129\n",
      "step 900 : loss 9.203277587890625\n",
      "step 901 : loss 9.210570335388184\n",
      "step 902 : loss 9.207796096801758\n",
      "step 903 : loss 9.246158599853516\n",
      "step 904 : loss 9.239765167236328\n",
      "step 905 : loss 9.314431190490723\n",
      "step 906 : loss 9.344644546508789\n",
      "step 907 : loss 9.436530113220215\n",
      "step 908 : loss 9.362306594848633\n",
      "step 909 : loss 9.37237548828125\n",
      "step 910 : loss 9.288260459899902\n",
      "step 911 : loss 9.31191349029541\n",
      "step 912 : loss 9.238211631774902\n",
      "step 913 : loss 9.275328636169434\n",
      "step 914 : loss 9.256023406982422\n",
      "step 915 : loss 9.292535781860352\n",
      "step 916 : loss 9.281038284301758\n",
      "step 917 : loss 9.276349067687988\n",
      "step 918 : loss 9.237072944641113\n",
      "step 919 : loss 9.241522789001465\n",
      "step 920 : loss 9.208223342895508\n",
      "step 921 : loss 9.238337516784668\n",
      "step 922 : loss 9.21227741241455\n",
      "step 923 : loss 9.262900352478027\n",
      "step 924 : loss 9.248138427734375\n",
      "step 925 : loss 9.301709175109863\n",
      "step 926 : loss 9.312551498413086\n",
      "step 927 : loss 9.388897895812988\n",
      "step 928 : loss 9.290854454040527\n",
      "step 929 : loss 9.266194343566895\n",
      "step 930 : loss 9.173465728759766\n",
      "step 931 : loss 9.174615859985352\n",
      "step 932 : loss 9.172714233398438\n",
      "step 933 : loss 9.270029067993164\n",
      "step 934 : loss 9.31207275390625\n",
      "step 935 : loss 9.365875244140625\n",
      "step 936 : loss 9.233695983886719\n",
      "step 937 : loss 9.142630577087402\n",
      "step 938 : loss 9.0451021194458\n",
      "step 939 : loss 9.002202033996582\n",
      "step 940 : loss 8.98617172241211\n",
      "step 941 : loss 9.024873733520508\n",
      "step 942 : loss 9.041938781738281\n",
      "step 943 : loss 9.144160270690918\n",
      "step 944 : loss 9.187837600708008\n",
      "step 945 : loss 9.265019416809082\n",
      "step 946 : loss 9.228412628173828\n",
      "step 947 : loss 9.269281387329102\n",
      "step 948 : loss 9.259115219116211\n",
      "step 949 : loss 9.338031768798828\n",
      "step 950 : loss 9.34163761138916\n",
      "step 951 : loss 9.3933687210083\n",
      "step 952 : loss 9.341172218322754\n",
      "step 953 : loss 9.261051177978516\n",
      "step 954 : loss 9.19654369354248\n",
      "step 955 : loss 9.15227222442627\n",
      "step 956 : loss 9.168355941772461\n",
      "step 957 : loss 9.205500602722168\n",
      "step 958 : loss 9.266898155212402\n",
      "step 959 : loss 9.285901069641113\n",
      "step 960 : loss 9.229909896850586\n",
      "step 961 : loss 9.131876945495605\n",
      "step 962 : loss 9.057268142700195\n",
      "step 963 : loss 9.126880645751953\n",
      "step 964 : loss 9.241628646850586\n",
      "step 965 : loss 9.440024375915527\n",
      "step 966 : loss 9.303235054016113\n",
      "step 967 : loss 9.249515533447266\n",
      "step 968 : loss 9.164931297302246\n",
      "step 969 : loss 9.210349082946777\n",
      "step 970 : loss 9.201282501220703\n",
      "step 971 : loss 9.21914005279541\n",
      "step 972 : loss 9.166435241699219\n",
      "step 973 : loss 9.167339324951172\n",
      "step 974 : loss 9.09130859375\n",
      "step 975 : loss 9.088995933532715\n",
      "step 976 : loss 9.092971801757812\n",
      "step 977 : loss 9.1881103515625\n",
      "step 978 : loss 9.253057479858398\n",
      "step 979 : loss 9.387882232666016\n",
      "step 980 : loss 9.273443222045898\n",
      "step 981 : loss 9.212491035461426\n",
      "step 982 : loss 9.113865852355957\n",
      "step 983 : loss 9.106786727905273\n",
      "step 984 : loss 9.041242599487305\n",
      "step 985 : loss 9.045780181884766\n",
      "step 986 : loss 9.043696403503418\n",
      "step 987 : loss 9.141984939575195\n",
      "step 988 : loss 9.17662239074707\n",
      "step 989 : loss 9.268144607543945\n",
      "step 990 : loss 9.21033763885498\n",
      "step 991 : loss 9.202066421508789\n",
      "step 992 : loss 9.083440780639648\n",
      "step 993 : loss 9.032201766967773\n",
      "step 994 : loss 8.983977317810059\n",
      "step 995 : loss 9.026823997497559\n",
      "step 996 : loss 9.074557304382324\n",
      "step 997 : loss 9.169734954833984\n",
      "step 998 : loss 9.157391548156738\n",
      "step 999 : loss 9.181184768676758\n",
      "Test Loss at the end of training: 10.291955947875977\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "num_epochs = 1000\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='autoencoder', entity = \"finegrain-cs\", name = '1000epoch, refiners, squares', config={\n",
    "    'lr': lr,\n",
    "    'num_epochs': num_epochs\n",
    "})\n",
    "\n",
    "path_dataset_train = \"../data/dataset_train/\"\n",
    "path_dataset_test = \"../data/dataset_test/\"\n",
    "dataset_train = ImageDataset(path_dataset_train).data\n",
    "dataset_test = ImageDataset(path_dataset_test).data\n",
    "autoencoder = AutoEncoder()\n",
    "load_dropout(autoencoder, dropout=0.5)\n",
    "autoencoder.to(device)\n",
    "autoencoder.train()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "\n",
    "# Convert your datasets to DataLoader for easy batch processing\n",
    "# train_dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "\n",
    "# Modify your training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss_iter = 0\n",
    "    for images in dataset_train:\n",
    "        images = utils.image_to_tensor(images).to(device)\n",
    "        y = autoencoder(images)\n",
    "        loss = (y - images).norm()\n",
    "        loss_iter += loss\n",
    "\n",
    "    loss = loss_iter / len(dataset_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Log the loss to wandb\n",
    "    wandb.log({'step': epoch, 'loss': loss.item()})\n",
    "\n",
    "    print(f\"step {epoch} : loss {loss.item()}\")\n",
    "\n",
    "# Testing at the end of training\n",
    "loss_iter_test = 0\n",
    "reconstructed_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_test in dataset_test:\n",
    "        images_test = utils.image_to_tensor(images_test).to(device)\n",
    "        y_test = autoencoder(images_test)\n",
    "        loss_test = (y_test - images_test).norm()\n",
    "        loss_iter_test += loss_test\n",
    "\n",
    "        # # Append the reconstructed images for visualization\n",
    "        concat = Image.new('RGB', (256, 128))\n",
    "        concat.paste(utils.tensor_to_image(images_test.data), (0, 0))\n",
    "        concat.paste(utils.tensor_to_image(y_test.data), (128, 0))\n",
    "        \n",
    "        reconstructed_images.append(concat)\n",
    "\n",
    "images = [PIL.Image.fromarray(np.array(image)) for image in reconstructed_images]\n",
    "\n",
    "wandb.log({\"reconstructed_images\": [wandb.Image(image) for image in images]})\n",
    "\n",
    "loss_test = loss_iter_test / len(dataset_test)\n",
    "\n",
    "# Log the test loss to wandb\n",
    "wandb.log({'test_loss': loss_test.item()})\n",
    "\n",
    "print(f\"Test Loss at the end of training: {loss_test.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
