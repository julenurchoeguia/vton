{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image: Image) -> torch.Tensor:\n",
    "    img_array = np.array(image)\n",
    "    img_array = img_array/255\n",
    "    img_array = img_array.transpose(2, 0, 1).astype(np.float32)\n",
    "    img_tensor = torch.from_numpy(img_array)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a Tensor to a PIL Image.\n",
    "\n",
    "    The tensor must have shape `[1, channels, height, width]` where the number of\n",
    "    channels is either 1 (grayscale) or 3 (RGB) or 4 (RGBA).\n",
    "\n",
    "    Expected values are in the range `[0, 1]` and are clamped to this range.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 4 and tensor.shape[0] == 1, f\"Unsupported tensor shape: {tensor.shape}\"\n",
    "    num_channels = tensor.shape[1]\n",
    "    tensor = tensor.clamp(0, 1).squeeze(0)\n",
    "\n",
    "    match num_channels:\n",
    "        case 1:\n",
    "            tensor = tensor.squeeze(0)\n",
    "        case 3 | 4:\n",
    "            tensor = tensor.permute(1, 2, 0)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported number of channels: {num_channels}\")\n",
    "\n",
    "    return Image.fromarray((tensor.cpu().numpy() * 255).astype(\"uint8\"))  # type: ignore[reportUnknownType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = tuple[int, int, int]\n",
    "def create_empty_image(resolution: int, color: color=(0,0,0)) -> Image:\n",
    "    return Image.new('RGB', (resolution, resolution), color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = image_to_tensor(create_empty_image(128, color=(255, 0, 0)))\n",
    "target_tensor = image_to_tensor(create_empty_image(128, color=(0, 255, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/julen/pytorch_test/autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     init_tensor \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr\u001b[39m*\u001b[39;49minit_tensor\u001b[39m.\u001b[39;49mgrad\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     init_tensor\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mzero_()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3, 3, 3, padding=1)\n",
    "lr = 0.01\n",
    "min_steps = 100\n",
    "\n",
    "for step in range(min_steps):\n",
    "    y = conv(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        init_tensor -= lr*init_tensor.grad\n",
    "        init_tensor.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int=1, out_features: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "linear = Linear(in_features=3, out_features=3)\n",
    "x = torch.randn(1, 3)\n",
    "target_tensor = torch.randn(1, 3)\n",
    "y = linear(x)\n",
    "loss = ((y - target_tensor)**2).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resblock(nn.Module):\n",
    "    def __init__(self, in_channels: int=1, out_channels: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(x)\n",
    "        y = self.conv2(x)\n",
    "        return y+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/julen/pytorch_test/autoencoder.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(min_steps):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     y \u001b[39m=\u001b[39m block(init_tensor)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m (y \u001b[39m-\u001b[39;49m target_tensor)\u001b[39m.\u001b[39mnorm()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "block = resblock(3, 3)\n",
    "\n",
    "lr = 1e-5\n",
    "min_steps = 10000\n",
    "for step in range(min_steps):\n",
    "    y = block(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step: {step}, loss: {loss.item()}')\n",
    "        for param in block.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr*param.grad\n",
    "\n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv1_1 = nn.Conv2d(256, 4, 1, padding=0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv1_1(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, output_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv1_1 = nn.Conv2d(4, 256, 1, padding=0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1_1(x)\n",
    "        y = self.conv1(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        return y\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 128.6528778076172\n",
      "step 1 : loss 128.21038818359375\n",
      "step 2 : loss 127.76188659667969\n",
      "step 3 : loss 127.30679321289062\n",
      "step 4 : loss 126.84478759765625\n",
      "step 5 : loss 126.37632751464844\n",
      "step 6 : loss 125.90122985839844\n",
      "step 7 : loss 125.42005157470703\n",
      "step 8 : loss 124.93059539794922\n",
      "step 9 : loss 124.43502807617188\n",
      "step 10 : loss 123.93255615234375\n",
      "step 11 : loss 123.42205810546875\n",
      "step 12 : loss 122.9047622680664\n",
      "step 13 : loss 122.37893676757812\n",
      "step 14 : loss 121.83979034423828\n",
      "step 15 : loss 121.29861450195312\n",
      "step 16 : loss 120.74919891357422\n",
      "step 17 : loss 120.19261932373047\n",
      "step 18 : loss 119.62739562988281\n",
      "step 19 : loss 119.05462646484375\n",
      "step 20 : loss 118.47195434570312\n",
      "step 21 : loss 117.88108825683594\n",
      "step 22 : loss 117.28244018554688\n",
      "step 23 : loss 116.67326354980469\n",
      "step 24 : loss 116.05571746826172\n",
      "step 25 : loss 115.4289321899414\n",
      "step 26 : loss 114.79264831542969\n",
      "step 27 : loss 114.14610290527344\n",
      "step 28 : loss 113.49009704589844\n",
      "step 29 : loss 112.82311248779297\n",
      "step 30 : loss 112.14706420898438\n",
      "step 31 : loss 111.45945739746094\n",
      "step 32 : loss 110.76154327392578\n",
      "step 33 : loss 110.05264282226562\n",
      "step 34 : loss 109.33202362060547\n",
      "step 35 : loss 108.59356689453125\n",
      "step 36 : loss 107.84986114501953\n",
      "step 37 : loss 107.09502410888672\n",
      "step 38 : loss 106.32749938964844\n",
      "step 39 : loss 105.54853820800781\n",
      "step 40 : loss 104.7568130493164\n",
      "step 41 : loss 103.95157623291016\n",
      "step 42 : loss 103.13362121582031\n",
      "step 43 : loss 102.30205535888672\n",
      "step 44 : loss 101.45672607421875\n",
      "step 45 : loss 100.59754943847656\n",
      "step 46 : loss 99.72352600097656\n",
      "step 47 : loss 98.83531951904297\n",
      "step 48 : loss 97.93225860595703\n",
      "step 49 : loss 97.0132064819336\n",
      "step 50 : loss 96.07886505126953\n",
      "step 51 : loss 95.1281509399414\n",
      "step 52 : loss 94.16124725341797\n",
      "step 53 : loss 93.17729949951172\n",
      "step 54 : loss 92.1759033203125\n",
      "step 55 : loss 91.15656280517578\n",
      "step 56 : loss 90.10955047607422\n",
      "step 57 : loss 89.05341339111328\n",
      "step 58 : loss 87.97832489013672\n",
      "step 59 : loss 86.8835678100586\n",
      "step 60 : loss 85.76830291748047\n",
      "step 61 : loss 84.63184356689453\n",
      "step 62 : loss 83.47002410888672\n",
      "step 63 : loss 82.29059600830078\n",
      "step 64 : loss 81.08902740478516\n",
      "step 65 : loss 79.86420440673828\n",
      "step 66 : loss 78.6158447265625\n",
      "step 67 : loss 77.34282684326172\n",
      "step 68 : loss 76.04470825195312\n",
      "step 69 : loss 74.72045135498047\n",
      "step 70 : loss 73.3695068359375\n",
      "step 71 : loss 71.99124908447266\n",
      "step 72 : loss 70.58399963378906\n",
      "step 73 : loss 69.14176940917969\n",
      "step 74 : loss 67.6749038696289\n",
      "step 75 : loss 66.1768569946289\n",
      "step 76 : loss 64.64688110351562\n",
      "step 77 : loss 63.08351516723633\n",
      "step 78 : loss 61.485435485839844\n",
      "step 79 : loss 59.85185623168945\n",
      "step 80 : loss 58.18140411376953\n",
      "step 81 : loss 56.46913146972656\n",
      "step 82 : loss 54.720970153808594\n",
      "step 83 : loss 52.93168640136719\n",
      "step 84 : loss 51.10032272338867\n",
      "step 85 : loss 49.22503662109375\n",
      "step 86 : loss 47.299835205078125\n",
      "step 87 : loss 45.331058502197266\n",
      "step 88 : loss 43.31260681152344\n",
      "step 89 : loss 41.24523162841797\n",
      "step 90 : loss 39.1226921081543\n",
      "step 91 : loss 36.94887161254883\n",
      "step 92 : loss 34.71923828125\n",
      "step 93 : loss 32.42914581298828\n",
      "step 94 : loss 30.083284378051758\n",
      "step 95 : loss 27.675813674926758\n",
      "step 96 : loss 25.20761489868164\n",
      "step 97 : loss 22.678876876831055\n",
      "step 98 : loss 20.093463897705078\n",
      "step 99 : loss 17.454614639282227\n",
      "step 100 : loss 14.777742385864258\n",
      "step 101 : loss 12.08700942993164\n",
      "step 102 : loss 9.439600944519043\n",
      "step 103 : loss 6.97160530090332\n",
      "step 104 : loss 5.0047101974487305\n",
      "step 105 : loss 4.032219409942627\n",
      "step 106 : loss 3.858414888381958\n",
      "step 107 : loss 3.82747220993042\n",
      "step 108 : loss 3.8061022758483887\n",
      "step 109 : loss 3.7883870601654053\n",
      "step 110 : loss 3.773381233215332\n",
      "step 111 : loss 3.7602016925811768\n",
      "step 112 : loss 3.748246908187866\n",
      "step 113 : loss 3.7370989322662354\n",
      "step 114 : loss 3.7266900539398193\n",
      "step 115 : loss 3.716681718826294\n",
      "step 116 : loss 3.706825017929077\n",
      "step 117 : loss 3.6973509788513184\n",
      "step 118 : loss 3.688006639480591\n",
      "step 119 : loss 3.6786162853240967\n",
      "step 120 : loss 3.669431447982788\n",
      "step 121 : loss 3.6603195667266846\n",
      "step 122 : loss 3.651224136352539\n",
      "step 123 : loss 3.642159938812256\n",
      "step 124 : loss 3.6331238746643066\n",
      "step 125 : loss 3.624105930328369\n",
      "step 126 : loss 3.61513614654541\n",
      "step 127 : loss 3.6061713695526123\n",
      "step 128 : loss 3.5972304344177246\n",
      "step 129 : loss 3.5883281230926514\n",
      "step 130 : loss 3.57942533493042\n",
      "step 131 : loss 3.5705339908599854\n",
      "step 132 : loss 3.5616543292999268\n",
      "step 133 : loss 3.552795171737671\n",
      "step 134 : loss 3.5439581871032715\n",
      "step 135 : loss 3.5351343154907227\n",
      "step 136 : loss 3.526327610015869\n",
      "step 137 : loss 3.517537832260132\n",
      "step 138 : loss 3.508782148361206\n",
      "step 139 : loss 3.5000081062316895\n",
      "step 140 : loss 3.4912869930267334\n",
      "step 141 : loss 3.4825589656829834\n",
      "step 142 : loss 3.473851203918457\n",
      "step 143 : loss 3.465160608291626\n",
      "step 144 : loss 3.456482172012329\n",
      "step 145 : loss 3.4478249549865723\n",
      "step 146 : loss 3.439180612564087\n",
      "step 147 : loss 3.4305388927459717\n",
      "step 148 : loss 3.4219510555267334\n",
      "step 149 : loss 3.4133453369140625\n",
      "step 150 : loss 3.4047744274139404\n",
      "step 151 : loss 3.396210193634033\n",
      "step 152 : loss 3.3876631259918213\n",
      "step 153 : loss 3.3791329860687256\n",
      "step 154 : loss 3.370628833770752\n",
      "step 155 : loss 3.3621182441711426\n",
      "step 156 : loss 3.3536407947540283\n",
      "step 157 : loss 3.3451762199401855\n",
      "step 158 : loss 3.3367347717285156\n",
      "step 159 : loss 3.3282833099365234\n",
      "step 160 : loss 3.319864511489868\n",
      "step 161 : loss 3.3114686012268066\n",
      "step 162 : loss 3.30307674407959\n",
      "step 163 : loss 3.2947046756744385\n",
      "step 164 : loss 3.286341428756714\n",
      "step 165 : loss 3.2780094146728516\n",
      "step 166 : loss 3.2696805000305176\n",
      "step 167 : loss 3.2613718509674072\n",
      "step 168 : loss 3.2530758380889893\n",
      "step 169 : loss 3.244798183441162\n",
      "step 170 : loss 3.236531972885132\n",
      "step 171 : loss 3.228276014328003\n",
      "step 172 : loss 3.2200491428375244\n",
      "step 173 : loss 3.2118301391601562\n",
      "step 174 : loss 3.2036266326904297\n",
      "step 175 : loss 3.1954538822174072\n",
      "step 176 : loss 3.187269687652588\n",
      "step 177 : loss 3.179110527038574\n",
      "step 178 : loss 3.1709694862365723\n",
      "step 179 : loss 3.162851572036743\n",
      "step 180 : loss 3.1547372341156006\n",
      "step 181 : loss 3.146638870239258\n",
      "step 182 : loss 3.1385600566864014\n",
      "step 183 : loss 3.1304819583892822\n",
      "step 184 : loss 3.1224400997161865\n",
      "step 185 : loss 3.1144039630889893\n",
      "step 186 : loss 3.106386423110962\n",
      "step 187 : loss 3.0983738899230957\n",
      "step 188 : loss 3.0903775691986084\n",
      "step 189 : loss 3.082409620285034\n",
      "step 190 : loss 3.0744409561157227\n",
      "step 191 : loss 3.066494941711426\n",
      "step 192 : loss 3.0585553646087646\n",
      "step 193 : loss 3.050640821456909\n",
      "step 194 : loss 3.0427401065826416\n",
      "step 195 : loss 3.03485107421875\n",
      "step 196 : loss 3.026977300643921\n",
      "step 197 : loss 3.019116163253784\n",
      "step 198 : loss 3.01127290725708\n",
      "step 199 : loss 3.003441333770752\n",
      "step 200 : loss 2.9956233501434326\n",
      "step 201 : loss 2.98783278465271\n",
      "step 202 : loss 2.9800491333007812\n",
      "step 203 : loss 2.9722795486450195\n",
      "step 204 : loss 2.964510917663574\n",
      "step 205 : loss 2.956761360168457\n",
      "step 206 : loss 2.9490392208099365\n",
      "step 207 : loss 2.9413230419158936\n",
      "step 208 : loss 2.933628559112549\n",
      "step 209 : loss 2.925940752029419\n",
      "step 210 : loss 2.918274164199829\n",
      "step 211 : loss 2.91062068939209\n",
      "step 212 : loss 2.9029717445373535\n",
      "step 213 : loss 2.8953473567962646\n",
      "step 214 : loss 2.8877289295196533\n",
      "step 215 : loss 2.880129337310791\n",
      "step 216 : loss 2.872540235519409\n",
      "step 217 : loss 2.8649790287017822\n",
      "step 218 : loss 2.857415199279785\n",
      "step 219 : loss 2.849865674972534\n",
      "step 220 : loss 2.8423428535461426\n",
      "step 221 : loss 2.834826946258545\n",
      "step 222 : loss 2.8273305892944336\n",
      "step 223 : loss 2.8198330402374268\n",
      "step 224 : loss 2.8123626708984375\n",
      "step 225 : loss 2.8049087524414062\n",
      "step 226 : loss 2.7974531650543213\n",
      "step 227 : loss 2.790022611618042\n",
      "step 228 : loss 2.782609224319458\n",
      "step 229 : loss 2.775193452835083\n",
      "step 230 : loss 2.7678120136260986\n",
      "step 231 : loss 2.7604379653930664\n",
      "step 232 : loss 2.7530758380889893\n",
      "step 233 : loss 2.745718479156494\n",
      "step 234 : loss 2.7383861541748047\n",
      "step 235 : loss 2.7310566902160645\n",
      "step 236 : loss 2.7237491607666016\n",
      "step 237 : loss 2.716458320617676\n",
      "step 238 : loss 2.709174871444702\n",
      "step 239 : loss 2.7019097805023193\n",
      "step 240 : loss 2.69463849067688\n",
      "step 241 : loss 2.687411308288574\n",
      "step 242 : loss 2.680173873901367\n",
      "step 243 : loss 2.672964572906494\n",
      "step 244 : loss 2.66576886177063\n",
      "step 245 : loss 2.6585679054260254\n",
      "step 246 : loss 2.6514062881469727\n",
      "step 247 : loss 2.644249200820923\n",
      "step 248 : loss 2.6370935440063477\n",
      "step 249 : loss 2.6299591064453125\n",
      "step 250 : loss 2.6228368282318115\n",
      "step 251 : loss 2.615732192993164\n",
      "step 252 : loss 2.6086366176605225\n",
      "step 253 : loss 2.6015608310699463\n",
      "step 254 : loss 2.5944840908050537\n",
      "step 255 : loss 2.587437391281128\n",
      "step 256 : loss 2.580399751663208\n",
      "step 257 : loss 2.5733697414398193\n",
      "step 258 : loss 2.566361665725708\n",
      "step 259 : loss 2.559344530105591\n",
      "step 260 : loss 2.5523600578308105\n",
      "step 261 : loss 2.5453941822052\n",
      "step 262 : loss 2.5384342670440674\n",
      "step 263 : loss 2.5314691066741943\n",
      "step 264 : loss 2.524538516998291\n",
      "step 265 : loss 2.517617702484131\n",
      "step 266 : loss 2.510713577270508\n",
      "step 267 : loss 2.503814220428467\n",
      "step 268 : loss 2.4969234466552734\n",
      "step 269 : loss 2.4900619983673096\n",
      "step 270 : loss 2.4832119941711426\n",
      "step 271 : loss 2.4763643741607666\n",
      "step 272 : loss 2.469529390335083\n",
      "step 273 : loss 2.4627091884613037\n",
      "step 274 : loss 2.455900192260742\n",
      "step 275 : loss 2.449096918106079\n",
      "step 276 : loss 2.442319869995117\n",
      "step 277 : loss 2.435547113418579\n",
      "step 278 : loss 2.4287896156311035\n",
      "step 279 : loss 2.4220457077026367\n",
      "step 280 : loss 2.4153144359588623\n",
      "step 281 : loss 2.4085912704467773\n",
      "step 282 : loss 2.4018824100494385\n",
      "step 283 : loss 2.395184278488159\n",
      "step 284 : loss 2.3885021209716797\n",
      "step 285 : loss 2.381838083267212\n",
      "step 286 : loss 2.3751778602600098\n",
      "step 287 : loss 2.36852765083313\n",
      "step 288 : loss 2.361893892288208\n",
      "step 289 : loss 2.355274200439453\n",
      "step 290 : loss 2.348674774169922\n",
      "step 291 : loss 2.3420770168304443\n",
      "step 292 : loss 2.3354930877685547\n",
      "step 293 : loss 2.3289239406585693\n",
      "step 294 : loss 2.3223586082458496\n",
      "step 295 : loss 2.315814971923828\n",
      "step 296 : loss 2.3092873096466064\n",
      "step 297 : loss 2.3027563095092773\n",
      "step 298 : loss 2.2962517738342285\n",
      "step 299 : loss 2.2897584438323975\n",
      "step 300 : loss 2.283266067504883\n",
      "step 301 : loss 2.2767930030822754\n",
      "step 302 : loss 2.2703306674957275\n",
      "step 303 : loss 2.2638871669769287\n",
      "step 304 : loss 2.2574539184570312\n",
      "step 305 : loss 2.251032829284668\n",
      "step 306 : loss 2.2446107864379883\n",
      "step 307 : loss 2.238210678100586\n",
      "step 308 : loss 2.231826066970825\n",
      "step 309 : loss 2.2254550457000732\n",
      "step 310 : loss 2.219085454940796\n",
      "step 311 : loss 2.212733745574951\n",
      "step 312 : loss 2.206387758255005\n",
      "step 313 : loss 2.2000553607940674\n",
      "step 314 : loss 2.1937365531921387\n",
      "step 315 : loss 2.1874332427978516\n",
      "step 316 : loss 2.1811399459838867\n",
      "step 317 : loss 2.17486572265625\n",
      "step 318 : loss 2.1685917377471924\n",
      "step 319 : loss 2.162327289581299\n",
      "step 320 : loss 2.1560912132263184\n",
      "step 321 : loss 2.149860382080078\n",
      "step 322 : loss 2.143632411956787\n",
      "step 323 : loss 2.137423038482666\n",
      "step 324 : loss 2.1312146186828613\n",
      "step 325 : loss 2.125032901763916\n",
      "step 326 : loss 2.118859052658081\n",
      "step 327 : loss 2.112684726715088\n",
      "step 328 : loss 2.1065335273742676\n",
      "step 329 : loss 2.100391149520874\n",
      "step 330 : loss 2.094264268875122\n",
      "step 331 : loss 2.088144063949585\n",
      "step 332 : loss 2.0820350646972656\n",
      "step 333 : loss 2.0759329795837402\n",
      "step 334 : loss 2.06984806060791\n",
      "step 335 : loss 2.0637762546539307\n",
      "step 336 : loss 2.057717800140381\n",
      "step 337 : loss 2.051666259765625\n",
      "step 338 : loss 2.045624017715454\n",
      "step 339 : loss 2.039606809616089\n",
      "step 340 : loss 2.0335845947265625\n",
      "step 341 : loss 2.0275754928588867\n",
      "step 342 : loss 2.021580934524536\n",
      "step 343 : loss 2.0155911445617676\n",
      "step 344 : loss 2.009629249572754\n",
      "step 345 : loss 2.003664970397949\n",
      "step 346 : loss 1.9977139234542847\n",
      "step 347 : loss 1.991773009300232\n",
      "step 348 : loss 1.985849380493164\n",
      "step 349 : loss 1.9799320697784424\n",
      "step 350 : loss 1.9740298986434937\n",
      "step 351 : loss 1.968127727508545\n",
      "step 352 : loss 1.9622408151626587\n",
      "step 353 : loss 1.956370234489441\n",
      "step 354 : loss 1.9505060911178589\n",
      "step 355 : loss 1.9446613788604736\n",
      "step 356 : loss 1.9388256072998047\n",
      "step 357 : loss 1.9329979419708252\n",
      "step 358 : loss 1.9271844625473022\n",
      "step 359 : loss 1.921378254890442\n",
      "step 360 : loss 1.9155826568603516\n",
      "step 361 : loss 1.9097933769226074\n",
      "step 362 : loss 1.9040213823318481\n",
      "step 363 : loss 1.8982553482055664\n",
      "step 364 : loss 1.8924959897994995\n",
      "step 365 : loss 1.8867526054382324\n",
      "step 366 : loss 1.8810323476791382\n",
      "step 367 : loss 1.8753025531768799\n",
      "step 368 : loss 1.869598150253296\n",
      "step 369 : loss 1.8638995885849\n",
      "step 370 : loss 1.8582077026367188\n",
      "step 371 : loss 1.8525367975234985\n",
      "step 372 : loss 1.846865177154541\n",
      "step 373 : loss 1.841207504272461\n",
      "step 374 : loss 1.8355729579925537\n",
      "step 375 : loss 1.8299381732940674\n",
      "step 376 : loss 1.824310541152954\n",
      "step 377 : loss 1.8187021017074585\n",
      "step 378 : loss 1.813098669052124\n",
      "step 379 : loss 1.8075048923492432\n",
      "step 380 : loss 1.8019253015518188\n",
      "step 381 : loss 1.7963539361953735\n",
      "step 382 : loss 1.7907873392105103\n",
      "step 383 : loss 1.7852295637130737\n",
      "step 384 : loss 1.7797091007232666\n",
      "step 385 : loss 1.7741732597351074\n",
      "step 386 : loss 1.7686623334884644\n",
      "step 387 : loss 1.7631597518920898\n",
      "step 388 : loss 1.7576631307601929\n",
      "step 389 : loss 1.7521696090698242\n",
      "step 390 : loss 1.7466908693313599\n",
      "step 391 : loss 1.741222620010376\n",
      "step 392 : loss 1.7357654571533203\n",
      "step 393 : loss 1.730320692062378\n",
      "step 394 : loss 1.7248775959014893\n",
      "step 395 : loss 1.7194525003433228\n",
      "step 396 : loss 1.7140402793884277\n",
      "step 397 : loss 1.708633303642273\n",
      "step 398 : loss 1.703235387802124\n",
      "step 399 : loss 1.6978462934494019\n",
      "step 400 : loss 1.6924749612808228\n",
      "step 401 : loss 1.687114953994751\n",
      "step 402 : loss 1.6817514896392822\n",
      "step 403 : loss 1.67640221118927\n",
      "step 404 : loss 1.6710703372955322\n",
      "step 405 : loss 1.6657452583312988\n",
      "step 406 : loss 1.6604251861572266\n",
      "step 407 : loss 1.6551282405853271\n",
      "step 408 : loss 1.6498275995254517\n",
      "step 409 : loss 1.6445471048355103\n",
      "step 410 : loss 1.6392614841461182\n",
      "step 411 : loss 1.634004831314087\n",
      "step 412 : loss 1.6287472248077393\n",
      "step 413 : loss 1.623497486114502\n",
      "step 414 : loss 1.618260145187378\n",
      "step 415 : loss 1.6130329370498657\n",
      "step 416 : loss 1.6078189611434937\n",
      "step 417 : loss 1.6026158332824707\n",
      "step 418 : loss 1.597425103187561\n",
      "step 419 : loss 1.5922363996505737\n",
      "step 420 : loss 1.5870518684387207\n",
      "step 421 : loss 1.5818860530853271\n",
      "step 422 : loss 1.5767297744750977\n",
      "step 423 : loss 1.5715910196304321\n",
      "step 424 : loss 1.566440224647522\n",
      "step 425 : loss 1.5613139867782593\n",
      "step 426 : loss 1.556190848350525\n",
      "step 427 : loss 1.5510811805725098\n",
      "step 428 : loss 1.545984148979187\n",
      "step 429 : loss 1.5408855676651\n",
      "step 430 : loss 1.5358128547668457\n",
      "step 431 : loss 1.5307378768920898\n",
      "step 432 : loss 1.5256750583648682\n",
      "step 433 : loss 1.5206283330917358\n",
      "step 434 : loss 1.515580177307129\n",
      "step 435 : loss 1.5105491876602173\n",
      "step 436 : loss 1.5055279731750488\n",
      "step 437 : loss 1.500503659248352\n",
      "step 438 : loss 1.4955016374588013\n",
      "step 439 : loss 1.4905129671096802\n",
      "step 440 : loss 1.4855152368545532\n",
      "step 441 : loss 1.4805474281311035\n",
      "step 442 : loss 1.4755736589431763\n",
      "step 443 : loss 1.4706156253814697\n",
      "step 444 : loss 1.4656686782836914\n",
      "step 445 : loss 1.4607313871383667\n",
      "step 446 : loss 1.4557982683181763\n",
      "step 447 : loss 1.4508756399154663\n",
      "step 448 : loss 1.4459691047668457\n",
      "step 449 : loss 1.441070318222046\n",
      "step 450 : loss 1.4361768960952759\n",
      "step 451 : loss 1.4312876462936401\n",
      "step 452 : loss 1.4264130592346191\n",
      "step 453 : loss 1.42154061794281\n",
      "step 454 : loss 1.4166860580444336\n",
      "step 455 : loss 1.4118369817733765\n",
      "step 456 : loss 1.4069980382919312\n",
      "step 457 : loss 1.4021717309951782\n",
      "step 458 : loss 1.3973548412322998\n",
      "step 459 : loss 1.3925353288650513\n",
      "step 460 : loss 1.3877356052398682\n",
      "step 461 : loss 1.3829394578933716\n",
      "step 462 : loss 1.3781598806381226\n",
      "step 463 : loss 1.3733789920806885\n",
      "step 464 : loss 1.3686165809631348\n",
      "step 465 : loss 1.363856554031372\n",
      "step 466 : loss 1.3590987920761108\n",
      "step 467 : loss 1.3543709516525269\n",
      "step 468 : loss 1.3496379852294922\n",
      "step 469 : loss 1.344919204711914\n",
      "step 470 : loss 1.3402036428451538\n",
      "step 471 : loss 1.3354982137680054\n",
      "step 472 : loss 1.330801248550415\n",
      "step 473 : loss 1.326109766960144\n",
      "step 474 : loss 1.3214441537857056\n",
      "step 475 : loss 1.316766381263733\n",
      "step 476 : loss 1.3121076822280884\n",
      "step 477 : loss 1.3074605464935303\n",
      "step 478 : loss 1.3028132915496826\n",
      "step 479 : loss 1.298173189163208\n",
      "step 480 : loss 1.2935504913330078\n",
      "step 481 : loss 1.2889357805252075\n",
      "step 482 : loss 1.284327507019043\n",
      "step 483 : loss 1.2797303199768066\n",
      "step 484 : loss 1.2751469612121582\n",
      "step 485 : loss 1.2705625295639038\n",
      "step 486 : loss 1.2659838199615479\n",
      "step 487 : loss 1.2614190578460693\n",
      "step 488 : loss 1.2568631172180176\n",
      "step 489 : loss 1.2523161172866821\n",
      "step 490 : loss 1.2477796077728271\n",
      "step 491 : loss 1.24324369430542\n",
      "step 492 : loss 1.2387226819992065\n",
      "step 493 : loss 1.2342067956924438\n",
      "step 494 : loss 1.229703664779663\n",
      "step 495 : loss 1.225202202796936\n",
      "step 496 : loss 1.2207181453704834\n",
      "step 497 : loss 1.2162350416183472\n",
      "step 498 : loss 1.2117637395858765\n",
      "step 499 : loss 1.2073043584823608\n",
      "step 500 : loss 1.2028452157974243\n",
      "step 501 : loss 1.1983975172042847\n",
      "step 502 : loss 1.1939553022384644\n",
      "step 503 : loss 1.189527153968811\n",
      "step 504 : loss 1.185104489326477\n",
      "step 505 : loss 1.1806923151016235\n",
      "step 506 : loss 1.1762856245040894\n",
      "step 507 : loss 1.171891212463379\n",
      "step 508 : loss 1.1674989461898804\n",
      "step 509 : loss 1.1631237268447876\n",
      "step 510 : loss 1.1587510108947754\n",
      "step 511 : loss 1.154384970664978\n",
      "step 512 : loss 1.1500245332717896\n",
      "step 513 : loss 1.1456806659698486\n",
      "step 514 : loss 1.1413389444351196\n",
      "step 515 : loss 1.1370071172714233\n",
      "step 516 : loss 1.1326864957809448\n",
      "step 517 : loss 1.1283735036849976\n",
      "step 518 : loss 1.124067783355713\n",
      "step 519 : loss 1.119769811630249\n",
      "step 520 : loss 1.1154754161834717\n",
      "step 521 : loss 1.111189603805542\n",
      "step 522 : loss 1.1069202423095703\n",
      "step 523 : loss 1.1026452779769897\n",
      "step 524 : loss 1.0983836650848389\n",
      "step 525 : loss 1.0941356420516968\n",
      "step 526 : loss 1.0898948907852173\n",
      "step 527 : loss 1.0856587886810303\n",
      "step 528 : loss 1.081432819366455\n",
      "step 529 : loss 1.077211856842041\n",
      "step 530 : loss 1.0730000734329224\n",
      "step 531 : loss 1.068793535232544\n",
      "step 532 : loss 1.0646001100540161\n",
      "step 533 : loss 1.0604108572006226\n",
      "step 534 : loss 1.0562353134155273\n",
      "step 535 : loss 1.0520583391189575\n",
      "step 536 : loss 1.0478910207748413\n",
      "step 537 : loss 1.0437390804290771\n",
      "step 538 : loss 1.0395878553390503\n",
      "step 539 : loss 1.0354481935501099\n",
      "step 540 : loss 1.0313217639923096\n",
      "step 541 : loss 1.0271928310394287\n",
      "step 542 : loss 1.0230839252471924\n",
      "step 543 : loss 1.0189845561981201\n",
      "step 544 : loss 1.0148988962173462\n",
      "step 545 : loss 1.010832667350769\n",
      "step 546 : loss 1.0067957639694214\n",
      "step 547 : loss 1.0027992725372314\n",
      "step 548 : loss 0.9988873600959778\n",
      "step 549 : loss 0.9951019883155823\n",
      "step 550 : loss 0.9915550351142883\n",
      "step 551 : loss 0.9884337782859802\n",
      "step 552 : loss 0.9861090183258057\n",
      "step 553 : loss 0.9852467775344849\n",
      "step 554 : loss 0.9870471358299255\n",
      "step 555 : loss 0.9935027956962585\n",
      "step 556 : loss 1.0072177648544312\n",
      "step 557 : loss 1.0309584140777588\n",
      "step 558 : loss 1.0628812313079834\n",
      "step 559 : loss 1.0978665351867676\n",
      "step 560 : loss 1.1230539083480835\n",
      "step 561 : loss 1.1404229402542114\n",
      "step 562 : loss 1.144850730895996\n",
      "step 563 : loss 1.1501154899597168\n",
      "step 564 : loss 1.1478679180145264\n",
      "step 565 : loss 1.1508115530014038\n",
      "step 566 : loss 1.1473617553710938\n",
      "step 567 : loss 1.1501185894012451\n",
      "step 568 : loss 1.1464024782180786\n",
      "step 569 : loss 1.1492700576782227\n",
      "step 570 : loss 1.1454020738601685\n",
      "step 571 : loss 1.1484099626541138\n",
      "step 572 : loss 1.1443674564361572\n",
      "step 573 : loss 1.1475296020507812\n",
      "step 574 : loss 1.143354892730713\n",
      "step 575 : loss 1.1466811895370483\n",
      "step 576 : loss 1.1423566341400146\n",
      "step 577 : loss 1.1458386182785034\n",
      "step 578 : loss 1.1413192749023438\n",
      "step 579 : loss 1.1449824571609497\n",
      "step 580 : loss 1.1403635740280151\n",
      "step 581 : loss 1.1441409587860107\n",
      "step 582 : loss 1.1393245458602905\n",
      "step 583 : loss 1.1432745456695557\n",
      "step 584 : loss 1.1383100748062134\n",
      "step 585 : loss 1.1424509286880493\n",
      "step 586 : loss 1.1373519897460938\n",
      "step 587 : loss 1.1416348218917847\n",
      "step 588 : loss 1.1363554000854492\n",
      "step 589 : loss 1.1408101320266724\n",
      "step 590 : loss 1.13535475730896\n",
      "step 591 : loss 1.1399939060211182\n",
      "step 592 : loss 1.1343598365783691\n",
      "step 593 : loss 1.1391918659210205\n",
      "step 594 : loss 1.1334187984466553\n",
      "step 595 : loss 1.1383863687515259\n",
      "step 596 : loss 1.1324243545532227\n",
      "step 597 : loss 1.1375852823257446\n",
      "step 598 : loss 1.1314488649368286\n",
      "step 599 : loss 1.136805534362793\n",
      "step 600 : loss 1.130489468574524\n",
      "step 601 : loss 1.1360186338424683\n",
      "step 602 : loss 1.1295396089553833\n",
      "step 603 : loss 1.1352514028549194\n",
      "step 604 : loss 1.1285679340362549\n",
      "step 605 : loss 1.134462594985962\n",
      "step 606 : loss 1.127587914466858\n",
      "step 607 : loss 1.1337010860443115\n",
      "step 608 : loss 1.1266571283340454\n",
      "step 609 : loss 1.1329573392868042\n",
      "step 610 : loss 1.1257506608963013\n",
      "step 611 : loss 1.1322144269943237\n",
      "step 612 : loss 1.1247981786727905\n",
      "step 613 : loss 1.1314724683761597\n",
      "step 614 : loss 1.12386953830719\n",
      "step 615 : loss 1.1307144165039062\n",
      "step 616 : loss 1.1229078769683838\n",
      "step 617 : loss 1.1299954652786255\n",
      "step 618 : loss 1.122007966041565\n",
      "step 619 : loss 1.12930166721344\n",
      "step 620 : loss 1.1211186647415161\n",
      "step 621 : loss 1.1285806894302368\n",
      "step 622 : loss 1.1201897859573364\n",
      "step 623 : loss 1.1278866529464722\n",
      "step 624 : loss 1.1192814111709595\n",
      "step 625 : loss 1.1271721124649048\n",
      "step 626 : loss 1.118394374847412\n",
      "step 627 : loss 1.1264711618423462\n",
      "step 628 : loss 1.1174638271331787\n",
      "step 629 : loss 1.1258031129837036\n",
      "step 630 : loss 1.116589903831482\n",
      "step 631 : loss 1.125112533569336\n",
      "step 632 : loss 1.1157124042510986\n",
      "step 633 : loss 1.1244629621505737\n",
      "step 634 : loss 1.1148078441619873\n",
      "step 635 : loss 1.1237727403640747\n",
      "step 636 : loss 1.1139516830444336\n",
      "step 637 : loss 1.1231372356414795\n",
      "step 638 : loss 1.1130894422531128\n",
      "step 639 : loss 1.1224735975265503\n",
      "step 640 : loss 1.1122044324874878\n",
      "step 641 : loss 1.121838927268982\n",
      "step 642 : loss 1.111352801322937\n",
      "step 643 : loss 1.1211949586868286\n",
      "step 644 : loss 1.1104705333709717\n",
      "step 645 : loss 1.120546817779541\n",
      "step 646 : loss 1.109611988067627\n",
      "step 647 : loss 1.1199440956115723\n",
      "step 648 : loss 1.1087599992752075\n",
      "step 649 : loss 1.1193287372589111\n",
      "step 650 : loss 1.1079221963882446\n",
      "step 651 : loss 1.1186953783035278\n",
      "step 652 : loss 1.1070505380630493\n",
      "step 653 : loss 1.1181014776229858\n",
      "step 654 : loss 1.1062211990356445\n",
      "step 655 : loss 1.1175041198730469\n",
      "step 656 : loss 1.1054044961929321\n",
      "step 657 : loss 1.1169325113296509\n",
      "step 658 : loss 1.1045912504196167\n",
      "step 659 : loss 1.1163413524627686\n",
      "step 660 : loss 1.103755235671997\n",
      "step 661 : loss 1.1157475709915161\n",
      "step 662 : loss 1.1029282808303833\n",
      "step 663 : loss 1.1151847839355469\n",
      "step 664 : loss 1.1021008491516113\n",
      "step 665 : loss 1.1146118640899658\n",
      "step 666 : loss 1.1012808084487915\n",
      "step 667 : loss 1.1140271425247192\n",
      "step 668 : loss 1.1004722118377686\n",
      "step 669 : loss 1.113494873046875\n",
      "step 670 : loss 1.0996538400650024\n",
      "step 671 : loss 1.11294686794281\n",
      "step 672 : loss 1.0988603830337524\n",
      "step 673 : loss 1.1124223470687866\n",
      "step 674 : loss 1.0980873107910156\n",
      "step 675 : loss 1.1118674278259277\n",
      "step 676 : loss 1.0972644090652466\n",
      "step 677 : loss 1.111363172531128\n",
      "step 678 : loss 1.096454381942749\n",
      "step 679 : loss 1.110818862915039\n",
      "step 680 : loss 1.0957099199295044\n",
      "step 681 : loss 1.1103168725967407\n",
      "step 682 : loss 1.0949151515960693\n",
      "step 683 : loss 1.1097986698150635\n",
      "step 684 : loss 1.0941364765167236\n",
      "step 685 : loss 1.1093127727508545\n",
      "step 686 : loss 1.0933481454849243\n",
      "step 687 : loss 1.1087900400161743\n",
      "step 688 : loss 1.0925649404525757\n",
      "step 689 : loss 1.10829758644104\n",
      "step 690 : loss 1.0918262004852295\n",
      "step 691 : loss 1.107844591140747\n",
      "step 692 : loss 1.0910438299179077\n",
      "step 693 : loss 1.1073546409606934\n",
      "step 694 : loss 1.0902818441390991\n",
      "step 695 : loss 1.1068572998046875\n",
      "step 696 : loss 1.0895329713821411\n",
      "step 697 : loss 1.1064115762710571\n",
      "step 698 : loss 1.0887634754180908\n",
      "step 699 : loss 1.10593581199646\n",
      "step 700 : loss 1.0880063772201538\n",
      "step 701 : loss 1.105480670928955\n",
      "step 702 : loss 1.0872594118118286\n",
      "step 703 : loss 1.1050132513046265\n",
      "step 704 : loss 1.086505651473999\n",
      "step 705 : loss 1.1045938730239868\n",
      "step 706 : loss 1.0857807397842407\n",
      "step 707 : loss 1.1041626930236816\n",
      "step 708 : loss 1.08499014377594\n",
      "step 709 : loss 1.1036961078643799\n",
      "step 710 : loss 1.084308385848999\n",
      "step 711 : loss 1.1032772064208984\n",
      "step 712 : loss 1.0835459232330322\n",
      "step 713 : loss 1.1028438806533813\n",
      "step 714 : loss 1.0828043222427368\n",
      "step 715 : loss 1.1024028062820435\n",
      "step 716 : loss 1.0820757150650024\n",
      "step 717 : loss 1.101994276046753\n",
      "step 718 : loss 1.0813313722610474\n",
      "step 719 : loss 1.101600170135498\n",
      "step 720 : loss 1.0806443691253662\n",
      "step 721 : loss 1.101213812828064\n",
      "step 722 : loss 1.0799022912979126\n",
      "step 723 : loss 1.100817322731018\n",
      "step 724 : loss 1.0791795253753662\n",
      "step 725 : loss 1.1004046201705933\n",
      "step 726 : loss 1.0784770250320435\n",
      "step 727 : loss 1.1000494956970215\n",
      "step 728 : loss 1.077744960784912\n",
      "step 729 : loss 1.099661946296692\n",
      "step 730 : loss 1.0770317316055298\n",
      "step 731 : loss 1.0992728471755981\n",
      "step 732 : loss 1.076322317123413\n",
      "step 733 : loss 1.0989060401916504\n",
      "step 734 : loss 1.0756123065948486\n",
      "step 735 : loss 1.0985466241836548\n",
      "step 736 : loss 1.0749136209487915\n",
      "step 737 : loss 1.0981839895248413\n",
      "step 738 : loss 1.0742061138153076\n",
      "step 739 : loss 1.0978082418441772\n",
      "step 740 : loss 1.0734976530075073\n",
      "step 741 : loss 1.0974745750427246\n",
      "step 742 : loss 1.0727989673614502\n",
      "step 743 : loss 1.0971399545669556\n",
      "step 744 : loss 1.0720889568328857\n",
      "step 745 : loss 1.0967954397201538\n",
      "step 746 : loss 1.071426272392273\n",
      "step 747 : loss 1.096450686454773\n",
      "step 748 : loss 1.0707030296325684\n",
      "step 749 : loss 1.0961272716522217\n",
      "step 750 : loss 1.0700076818466187\n",
      "step 751 : loss 1.0958020687103271\n",
      "step 752 : loss 1.069326639175415\n",
      "step 753 : loss 1.0954904556274414\n",
      "step 754 : loss 1.0686156749725342\n",
      "step 755 : loss 1.0951757431030273\n",
      "step 756 : loss 1.0679198503494263\n",
      "step 757 : loss 1.0948615074157715\n",
      "step 758 : loss 1.067259430885315\n",
      "step 759 : loss 1.094563603401184\n",
      "step 760 : loss 1.0665819644927979\n",
      "step 761 : loss 1.0942386388778687\n",
      "step 762 : loss 1.0658799409866333\n",
      "step 763 : loss 1.0939637422561646\n",
      "step 764 : loss 1.0652308464050293\n",
      "step 765 : loss 1.0936665534973145\n",
      "step 766 : loss 1.0645267963409424\n",
      "step 767 : loss 1.0933618545532227\n",
      "step 768 : loss 1.0638604164123535\n",
      "step 769 : loss 1.093105673789978\n",
      "step 770 : loss 1.0631636381149292\n",
      "step 771 : loss 1.092817783355713\n",
      "step 772 : loss 1.0625121593475342\n",
      "step 773 : loss 1.0925554037094116\n",
      "step 774 : loss 1.0618231296539307\n",
      "step 775 : loss 1.0922845602035522\n",
      "step 776 : loss 1.0611774921417236\n",
      "step 777 : loss 1.0920157432556152\n",
      "step 778 : loss 1.0604654550552368\n",
      "step 779 : loss 1.0917741060256958\n",
      "step 780 : loss 1.0598053932189941\n",
      "step 781 : loss 1.091528058052063\n",
      "step 782 : loss 1.0591543912887573\n",
      "step 783 : loss 1.0912625789642334\n",
      "step 784 : loss 1.0584841966629028\n",
      "step 785 : loss 1.0909831523895264\n",
      "step 786 : loss 1.057845950126648\n",
      "step 787 : loss 1.0907611846923828\n",
      "step 788 : loss 1.057144284248352\n",
      "step 789 : loss 1.0905516147613525\n",
      "step 790 : loss 1.0565013885498047\n",
      "step 791 : loss 1.0902864933013916\n",
      "step 792 : loss 1.0558242797851562\n",
      "step 793 : loss 1.0900942087173462\n",
      "step 794 : loss 1.0551652908325195\n",
      "step 795 : loss 1.0898494720458984\n",
      "step 796 : loss 1.0545234680175781\n",
      "step 797 : loss 1.0896236896514893\n",
      "step 798 : loss 1.0538628101348877\n",
      "step 799 : loss 1.0894148349761963\n",
      "step 800 : loss 1.0531924962997437\n",
      "step 801 : loss 1.0892257690429688\n",
      "step 802 : loss 1.052546739578247\n",
      "step 803 : loss 1.0890107154846191\n",
      "step 804 : loss 1.0519124269485474\n",
      "step 805 : loss 1.0887868404388428\n",
      "step 806 : loss 1.0512659549713135\n",
      "step 807 : loss 1.088592290878296\n",
      "step 808 : loss 1.050634741783142\n",
      "step 809 : loss 1.0883945226669312\n",
      "step 810 : loss 1.0499451160430908\n",
      "step 811 : loss 1.088205337524414\n",
      "step 812 : loss 1.0492796897888184\n",
      "step 813 : loss 1.0880330801010132\n",
      "step 814 : loss 1.048652172088623\n",
      "step 815 : loss 1.0878490209579468\n",
      "step 816 : loss 1.0480073690414429\n",
      "step 817 : loss 1.087642788887024\n",
      "step 818 : loss 1.047362208366394\n",
      "step 819 : loss 1.0874890089035034\n",
      "step 820 : loss 1.0467041730880737\n",
      "step 821 : loss 1.0872920751571655\n",
      "step 822 : loss 1.0460542440414429\n",
      "step 823 : loss 1.087131142616272\n",
      "step 824 : loss 1.045406460762024\n",
      "step 825 : loss 1.0869884490966797\n",
      "step 826 : loss 1.0447508096694946\n",
      "step 827 : loss 1.0868364572525024\n",
      "step 828 : loss 1.0441343784332275\n",
      "step 829 : loss 1.0866926908493042\n",
      "step 830 : loss 1.0434727668762207\n",
      "step 831 : loss 1.086545705795288\n",
      "step 832 : loss 1.042830467224121\n",
      "step 833 : loss 1.086413025856018\n",
      "step 834 : loss 1.0421720743179321\n",
      "step 835 : loss 1.0862786769866943\n",
      "step 836 : loss 1.0415440797805786\n",
      "step 837 : loss 1.086129903793335\n",
      "step 838 : loss 1.0409096479415894\n",
      "step 839 : loss 1.0859853029251099\n",
      "step 840 : loss 1.0402647256851196\n",
      "step 841 : loss 1.0858443975448608\n",
      "step 842 : loss 1.039637565612793\n",
      "step 843 : loss 1.0857338905334473\n",
      "step 844 : loss 1.0389831066131592\n",
      "step 845 : loss 1.0855979919433594\n",
      "step 846 : loss 1.038339376449585\n",
      "step 847 : loss 1.085503101348877\n",
      "step 848 : loss 1.0376988649368286\n",
      "step 849 : loss 1.0853803157806396\n",
      "step 850 : loss 1.0370588302612305\n",
      "step 851 : loss 1.0852633714675903\n",
      "step 852 : loss 1.0364363193511963\n",
      "step 853 : loss 1.0851597785949707\n",
      "step 854 : loss 1.035762906074524\n",
      "step 855 : loss 1.0850579738616943\n",
      "step 856 : loss 1.0351297855377197\n",
      "step 857 : loss 1.0849695205688477\n",
      "step 858 : loss 1.0344947576522827\n",
      "step 859 : loss 1.084869384765625\n",
      "step 860 : loss 1.0338540077209473\n",
      "step 861 : loss 1.0847922563552856\n",
      "step 862 : loss 1.0332263708114624\n",
      "step 863 : loss 1.0846978425979614\n",
      "step 864 : loss 1.0325958728790283\n",
      "step 865 : loss 1.084628701210022\n",
      "step 866 : loss 1.0319480895996094\n",
      "step 867 : loss 1.0845651626586914\n",
      "step 868 : loss 1.0312838554382324\n",
      "step 869 : loss 1.0844976902008057\n",
      "step 870 : loss 1.0306307077407837\n",
      "step 871 : loss 1.0844333171844482\n",
      "step 872 : loss 1.0299817323684692\n",
      "step 873 : loss 1.0843671560287476\n",
      "step 874 : loss 1.0293524265289307\n",
      "step 875 : loss 1.0843040943145752\n",
      "step 876 : loss 1.0287092924118042\n",
      "step 877 : loss 1.0842281579971313\n",
      "step 878 : loss 1.0280840396881104\n",
      "step 879 : loss 1.084182620048523\n",
      "step 880 : loss 1.0274633169174194\n",
      "step 881 : loss 1.084120512008667\n",
      "step 882 : loss 1.0268280506134033\n",
      "step 883 : loss 1.084075689315796\n",
      "step 884 : loss 1.0261638164520264\n",
      "step 885 : loss 1.0840250253677368\n",
      "step 886 : loss 1.0255398750305176\n",
      "step 887 : loss 1.0840002298355103\n",
      "step 888 : loss 1.0249053239822388\n",
      "step 889 : loss 1.0839482545852661\n",
      "step 890 : loss 1.0242674350738525\n",
      "step 891 : loss 1.0839052200317383\n",
      "step 892 : loss 1.0236073732376099\n",
      "step 893 : loss 1.0838968753814697\n",
      "step 894 : loss 1.0229651927947998\n",
      "step 895 : loss 1.0838662385940552\n",
      "step 896 : loss 1.022347331047058\n",
      "step 897 : loss 1.0838360786437988\n",
      "step 898 : loss 1.0216953754425049\n",
      "step 899 : loss 1.0838165283203125\n",
      "step 900 : loss 1.021068811416626\n",
      "step 901 : loss 1.0838028192520142\n",
      "step 902 : loss 1.0204240083694458\n",
      "step 903 : loss 1.0837756395339966\n",
      "step 904 : loss 1.0198131799697876\n",
      "step 905 : loss 1.0837469100952148\n",
      "step 906 : loss 1.0191792249679565\n",
      "step 907 : loss 1.0837247371673584\n",
      "step 908 : loss 1.0185389518737793\n",
      "step 909 : loss 1.083733320236206\n",
      "step 910 : loss 1.0178807973861694\n",
      "step 911 : loss 1.0837178230285645\n",
      "step 912 : loss 1.0172507762908936\n",
      "step 913 : loss 1.0837434530258179\n",
      "step 914 : loss 1.0165894031524658\n",
      "step 915 : loss 1.0837403535842896\n",
      "step 916 : loss 1.0159521102905273\n",
      "step 917 : loss 1.0837666988372803\n",
      "step 918 : loss 1.0153391361236572\n",
      "step 919 : loss 1.0837640762329102\n",
      "step 920 : loss 1.0146831274032593\n",
      "step 921 : loss 1.0837490558624268\n",
      "step 922 : loss 1.0140734910964966\n",
      "step 923 : loss 1.0837781429290771\n",
      "step 924 : loss 1.013388991355896\n",
      "step 925 : loss 1.0838009119033813\n",
      "step 926 : loss 1.0127414464950562\n",
      "step 927 : loss 1.083822250366211\n",
      "step 928 : loss 1.012137770652771\n",
      "step 929 : loss 1.0838476419448853\n",
      "step 930 : loss 1.0114879608154297\n",
      "step 931 : loss 1.0838643312454224\n",
      "step 932 : loss 1.0108592510223389\n",
      "step 933 : loss 1.083890676498413\n",
      "step 934 : loss 1.0102126598358154\n",
      "step 935 : loss 1.0839409828186035\n",
      "step 936 : loss 1.0095624923706055\n",
      "step 937 : loss 1.0839887857437134\n",
      "step 938 : loss 1.0089472532272339\n",
      "step 939 : loss 1.0840264558792114\n",
      "step 940 : loss 1.0083023309707642\n",
      "step 941 : loss 1.0840781927108765\n",
      "step 942 : loss 1.0076464414596558\n",
      "step 943 : loss 1.0840972661972046\n",
      "step 944 : loss 1.0069828033447266\n",
      "step 945 : loss 1.0841649770736694\n",
      "step 946 : loss 1.006369709968567\n",
      "step 947 : loss 1.0842344760894775\n",
      "step 948 : loss 1.0056976079940796\n",
      "step 949 : loss 1.0842825174331665\n",
      "step 950 : loss 1.005092978477478\n",
      "step 951 : loss 1.0843422412872314\n",
      "step 952 : loss 1.0044472217559814\n",
      "step 953 : loss 1.0843924283981323\n",
      "step 954 : loss 1.0037685632705688\n",
      "step 955 : loss 1.0844637155532837\n",
      "step 956 : loss 1.0031400918960571\n",
      "step 957 : loss 1.0845555067062378\n",
      "step 958 : loss 1.002476453781128\n",
      "step 959 : loss 1.0846104621887207\n",
      "step 960 : loss 1.001833438873291\n",
      "step 961 : loss 1.0846943855285645\n",
      "step 962 : loss 1.0011870861053467\n",
      "step 963 : loss 1.0847946405410767\n",
      "step 964 : loss 1.0005398988723755\n",
      "step 965 : loss 1.084869146347046\n",
      "step 966 : loss 0.999880313873291\n",
      "step 967 : loss 1.08495032787323\n",
      "step 968 : loss 0.9992081522941589\n",
      "step 969 : loss 1.0850563049316406\n",
      "step 970 : loss 0.9985664486885071\n",
      "step 971 : loss 1.0851337909698486\n",
      "step 972 : loss 0.9979118704795837\n",
      "step 973 : loss 1.0852346420288086\n",
      "step 974 : loss 0.9972727298736572\n",
      "step 975 : loss 1.0853348970413208\n",
      "step 976 : loss 0.9966303706169128\n",
      "step 977 : loss 1.0854424238204956\n",
      "step 978 : loss 0.9959686398506165\n",
      "step 979 : loss 1.085508108139038\n",
      "step 980 : loss 0.995306134223938\n",
      "step 981 : loss 1.0856508016586304\n",
      "step 982 : loss 0.9946460127830505\n",
      "step 983 : loss 1.0857415199279785\n",
      "step 984 : loss 0.9939812421798706\n",
      "step 985 : loss 1.0858465433120728\n",
      "step 986 : loss 0.9933448433876038\n",
      "step 987 : loss 1.085957646369934\n",
      "step 988 : loss 0.9926947951316833\n",
      "step 989 : loss 1.0860559940338135\n",
      "step 990 : loss 0.9920527935028076\n",
      "step 991 : loss 1.0861752033233643\n",
      "step 992 : loss 0.9914097785949707\n",
      "step 993 : loss 1.086299180984497\n",
      "step 994 : loss 0.9907552003860474\n",
      "step 995 : loss 1.0863964557647705\n",
      "step 996 : loss 0.9901168942451477\n",
      "step 997 : loss 1.086514949798584\n",
      "step 998 : loss 0.9894612431526184\n",
      "step 999 : loss 1.0866281986236572\n"
     ]
    }
   ],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.SGD(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(init_tensor)\n",
    "    loss = (y-init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADC0lEQVR4Ae2dgW7iMBAFScX/f3EpNUREriy0MfJmMB2ku+bi1i+e8SMgpN5yOS2nU/lzLX/7yCZwY/14XO4HX49/+pUhsKw7v/ZgF8aqqHd9PfPP/VmnJl+PenwQgfNBOf8y5tner2HYgJoGcHxeLdWu6uP1ita7QnseuN6Pi7QBsNIOAaUH2wuk+hheweTxHQImX+mbXn73q6CtBGVB9XG9vhnvFu1a1lXU5zPWZQPqnQMcdzegvcaMfdGmZJ95topn50ddjw0YRfLFeboFlB2RvSleXMqcP9YtYM5lvu9VD7gH1K8T1oXOWJF2Fa20jHXZgJbzoWe6G7Bnp7QreO2n2nk+74wNgJ12N2DP9X7qft+zrt77hA3Ys6MSv0cBg+GWluwpypaqgA0FcxDcA7pkMiuYPNUGwAIVoACYABwfNKC8qu19YQsvaLb4QMBsy5nvehUAO1OAAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNgAWEPz2dPjqJozv/UXDNgCWrIDBAvwfNAYDzZ7OBmQTDuZXQAAoe1gB2YSD+RUQAMoeVkA24WB+BQSAsocVkE04mF8BAaDsYQVkEw7mV0AAKHtYAdmEg/kVEADKHlZANuFgfgUEgLKH/URsMGE/ERsMNHs6n4KyCQfzKyAAlD2sgMGE/Ux4MNDs6WxANuFgfgUEgLKHFZBNOJhfAQGg7GHfCQ8m7DvhwUCzp/MpKJtwML8CAkDZw94DBhMu74S3x577gQ3YcDEHCmC4b6kK2FAwB7vuAXuey5jLnz/VBsAOi4Cyv93imAYbgKFfg5fv+1c9HO/hcr29Z5D88eT/JP4CXKkjDX9mR4EAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(autoencoder(init_tensor).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(init_tensor.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_images(size, num_images, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(1, num_images + 1):\n",
    "        # Create a new image with a random background color\n",
    "        img = Image.new(\"RGB\", size, color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "        # Get a drawing context\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Choose a random shape (circle, square, or triangle)\n",
    "        # shape = random.choice([\"circle\", \"square\", \"triangle\"])\n",
    "        shape = \"circle\"\n",
    "\n",
    "        # Choose a random position\n",
    "        position = (random.randint(20, size[0]-20), random.randint(20, size[1]-20))\n",
    "\n",
    "        # Choose a random color for the shape\n",
    "        shape_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Draw the shape on the image\n",
    "        if shape == \"circle\":\n",
    "            draw.ellipse([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"square\":\n",
    "            draw.rectangle([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"triangle\":\n",
    "            draw.polygon([(position[0], position[1]-20), (position[0]-20, position[1]+20), (position[0]+20, position[1]+20)], fill=shape_color)\n",
    "\n",
    "        # Save the image to the output folder\n",
    "        img.save(os.path.join(output_folder, f\"image_{i}.png\"))\n",
    "\n",
    "# Example usage\n",
    "generate_images((128, 128), 20, \"../data/dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, path) -> None:\n",
    "        self.data = list(range(100))\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Dataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "    \n",
    "    def __getitem__(self, key : str|int) -> int:\n",
    "        match key:\n",
    "            case key if isinstance(key, str):\n",
    "                raise ValueError('Dataset does not take string as index.')\n",
    "            case _:\n",
    "                return self.data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageDataset:\n",
    "    def __init__(self, path) -> None:\n",
    "        self.path = path\n",
    "        self.image_files = [f for f in os.listdir(path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.data = [self.load_image(file) for file in self.image_files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'ImageDataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image.Image:\n",
    "        return self.data[key]\n",
    "\n",
    "    def load_image(self, file: str) -> Image.Image:\n",
    "        image_path = os.path.join(self.path, file)\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image '{file}': {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 123.21394348144531\n",
      "step 1 : loss 122.26897430419922\n",
      "step 2 : loss 121.24037170410156\n",
      "step 3 : loss 120.0500259399414\n",
      "step 4 : loss 118.62422180175781\n",
      "step 5 : loss 116.88932800292969\n",
      "step 6 : loss 114.76747131347656\n",
      "step 7 : loss 112.17282104492188\n",
      "step 8 : loss 109.01488494873047\n",
      "step 9 : loss 105.21585083007812\n",
      "step 10 : loss 100.74581909179688\n",
      "step 11 : loss 95.69642639160156\n",
      "step 12 : loss 90.43476867675781\n",
      "step 13 : loss 85.88189697265625\n",
      "step 14 : loss 83.70207977294922\n",
      "step 15 : loss 84.5237808227539\n",
      "step 16 : loss 85.05467987060547\n",
      "step 17 : loss 83.18534088134766\n",
      "step 18 : loss 79.83197021484375\n",
      "step 19 : loss 76.52706909179688\n",
      "step 20 : loss 74.20878601074219\n",
      "step 21 : loss 73.02535247802734\n",
      "step 22 : loss 72.64973449707031\n",
      "step 23 : loss 72.67387390136719\n",
      "step 24 : loss 72.81929016113281\n",
      "step 25 : loss 72.96639251708984\n",
      "step 26 : loss 73.09656524658203\n",
      "step 27 : loss 73.20834350585938\n",
      "step 28 : loss 73.25165557861328\n",
      "step 29 : loss 73.13587951660156\n",
      "step 30 : loss 72.80183410644531\n",
      "step 31 : loss 72.26734161376953\n",
      "step 32 : loss 71.61111450195312\n",
      "step 33 : loss 70.92749786376953\n",
      "step 34 : loss 70.28730773925781\n",
      "step 35 : loss 69.72246551513672\n",
      "step 36 : loss 69.2327651977539\n",
      "step 37 : loss 68.80256652832031\n",
      "step 38 : loss 68.41523742675781\n",
      "step 39 : loss 68.05984497070312\n",
      "step 40 : loss 67.7297592163086\n",
      "step 41 : loss 67.41618347167969\n",
      "step 42 : loss 67.1007080078125\n",
      "step 43 : loss 66.75272369384766\n",
      "step 44 : loss 66.33563995361328\n",
      "step 45 : loss 65.82290649414062\n",
      "step 46 : loss 65.21349334716797\n",
      "step 47 : loss 64.53103637695312\n",
      "step 48 : loss 63.80400085449219\n",
      "step 49 : loss 63.05034255981445\n",
      "step 50 : loss 62.29389953613281\n",
      "step 51 : loss 61.60282897949219\n",
      "step 52 : loss 61.044071197509766\n",
      "step 53 : loss 60.541473388671875\n",
      "step 54 : loss 60.1571159362793\n",
      "step 55 : loss 59.87202072143555\n",
      "step 56 : loss 59.59153747558594\n",
      "step 57 : loss 59.400691986083984\n",
      "step 58 : loss 59.228118896484375\n",
      "step 59 : loss 59.118316650390625\n",
      "step 60 : loss 58.992095947265625\n",
      "step 61 : loss 58.68537521362305\n",
      "step 62 : loss 58.29969787597656\n",
      "step 63 : loss 57.85506820678711\n",
      "step 64 : loss 57.4363899230957\n",
      "step 65 : loss 57.07951736450195\n",
      "step 66 : loss 56.687015533447266\n",
      "step 67 : loss 56.285926818847656\n",
      "step 68 : loss 55.936370849609375\n",
      "step 69 : loss 55.6427116394043\n",
      "step 70 : loss 55.39507293701172\n",
      "step 71 : loss 55.15644454956055\n",
      "step 72 : loss 54.887062072753906\n",
      "step 73 : loss 54.6145133972168\n",
      "step 74 : loss 54.3780632019043\n",
      "step 75 : loss 54.17607498168945\n",
      "step 76 : loss 53.995479583740234\n",
      "step 77 : loss 53.86692428588867\n",
      "step 78 : loss 53.877986907958984\n",
      "step 79 : loss 53.93842697143555\n",
      "step 80 : loss 53.863834381103516\n",
      "step 81 : loss 53.40354537963867\n",
      "step 82 : loss 53.459869384765625\n",
      "step 83 : loss 53.47740173339844\n",
      "step 84 : loss 53.01274490356445\n",
      "step 85 : loss 53.14640808105469\n",
      "step 86 : loss 52.921363830566406\n",
      "step 87 : loss 52.67974090576172\n",
      "step 88 : loss 52.76478958129883\n",
      "step 89 : loss 52.42550277709961\n",
      "step 90 : loss 52.514564514160156\n",
      "step 91 : loss 52.285587310791016\n",
      "step 92 : loss 52.322078704833984\n",
      "step 93 : loss 52.19065856933594\n",
      "step 94 : loss 52.15459442138672\n",
      "step 95 : loss 52.0859375\n",
      "step 96 : loss 51.997215270996094\n",
      "step 97 : loss 51.96367263793945\n",
      "step 98 : loss 51.8436279296875\n",
      "step 99 : loss 51.829341888427734\n"
     ]
    }
   ],
   "source": [
    "path_dataset_train = \"../data/dataset_train/\"\n",
    "path_dataset_test = \"../data/dataset_test/\"\n",
    "dataset_train = ImageDataset(path_dataset_train).data\n",
    "dataset_test = ImageDataset(path_dataset_test).data\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.train()\n",
    "lr = 1e-4\n",
    "num_steps = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    loss_iter = 0\n",
    "    for image in dataset_train:\n",
    "        image = image_to_tensor(image)\n",
    "        y = autoencoder(image)\n",
    "        loss = (y-image).norm()\n",
    "        loss_iter += loss\n",
    "    loss = loss_iter/len(dataset_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:88mswdn4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr><tr><td>step</td><td></td></tr><tr><td>test_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>111.20601</td></tr><tr><td>step</td><td>9</td></tr><tr><td>test_loss</td><td>113.12148</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-smoke-20</strong> at: <a href='https://wandb.ai/finegrain-cs/finegrain-cs/runs/88mswdn4' target=\"_blank\">https://wandb.ai/finegrain-cs/finegrain-cs/runs/88mswdn4</a><br/>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231116_103514-88mswdn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:88mswdn4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/julen/vton/julen/wandb/run-20231116_103757-n87e3w04</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/finegrain-cs/finegrain-cs/runs/n87e3w04' target=\"_blank\">1000_step_100_img</a></strong> to <a href='https://wandb.ai/finegrain-cs/finegrain-cs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/finegrain-cs/finegrain-cs' target=\"_blank\">https://wandb.ai/finegrain-cs/finegrain-cs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/finegrain-cs/finegrain-cs/runs/n87e3w04' target=\"_blank\">https://wandb.ai/finegrain-cs/finegrain-cs/runs/n87e3w04</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 127.8471908569336\n",
      "step 1 : loss 126.98172760009766\n",
      "step 2 : loss 126.02904510498047\n",
      "step 3 : loss 124.91508483886719\n",
      "step 4 : loss 123.57235717773438\n",
      "step 5 : loss 121.93327331542969\n",
      "step 6 : loss 119.92459106445312\n",
      "step 7 : loss 117.46563720703125\n",
      "step 8 : loss 114.46715545654297\n",
      "step 9 : loss 110.8344955444336\n",
      "step 10 : loss 106.47940063476562\n",
      "step 11 : loss 101.34246063232422\n",
      "step 12 : loss 95.4439468383789\n",
      "step 13 : loss 88.98930358886719\n",
      "step 14 : loss 82.57461547851562\n",
      "step 15 : loss 77.53211212158203\n",
      "step 16 : loss 75.98564147949219\n",
      "step 17 : loss 78.50843048095703\n",
      "step 18 : loss 80.99042510986328\n",
      "step 19 : loss 80.5259780883789\n",
      "step 20 : loss 78.02440643310547\n",
      "step 21 : loss 75.19908905029297\n",
      "step 22 : loss 73.1849594116211\n",
      "step 23 : loss 72.30620574951172\n",
      "step 24 : loss 72.31853485107422\n",
      "step 25 : loss 72.77925109863281\n",
      "step 26 : loss 73.30518341064453\n",
      "step 27 : loss 73.65884399414062\n",
      "step 28 : loss 73.7333755493164\n",
      "step 29 : loss 73.5112075805664\n",
      "step 30 : loss 73.02991485595703\n",
      "step 31 : loss 72.36077117919922\n",
      "step 32 : loss 71.59542083740234\n",
      "step 33 : loss 70.83423614501953\n",
      "step 34 : loss 70.17149353027344\n",
      "step 35 : loss 69.67530059814453\n",
      "step 36 : loss 69.36608123779297\n",
      "step 37 : loss 69.20472717285156\n",
      "step 38 : loss 69.10310363769531\n",
      "step 39 : loss 68.96078491210938\n",
      "step 40 : loss 68.71089935302734\n",
      "step 41 : loss 68.3472671508789\n",
      "step 42 : loss 67.91796875\n",
      "step 43 : loss 67.49281311035156\n",
      "step 44 : loss 67.12645721435547\n",
      "step 45 : loss 66.83634185791016\n",
      "step 46 : loss 66.60064697265625\n",
      "step 47 : loss 66.37115478515625\n",
      "step 48 : loss 66.09304809570312\n",
      "step 49 : loss 65.72380828857422\n",
      "step 50 : loss 65.24906158447266\n",
      "step 51 : loss 64.69511413574219\n",
      "step 52 : loss 64.13540649414062\n",
      "step 53 : loss 63.67593765258789\n",
      "step 54 : loss 63.37909698486328\n",
      "step 55 : loss 63.11099624633789\n",
      "step 56 : loss 62.570404052734375\n",
      "step 57 : loss 61.79511260986328\n",
      "step 58 : loss 61.191062927246094\n",
      "step 59 : loss 60.79970169067383\n",
      "step 60 : loss 60.30564880371094\n",
      "step 61 : loss 59.76759338378906\n",
      "step 62 : loss 59.7066535949707\n",
      "step 63 : loss 59.496368408203125\n",
      "step 64 : loss 59.197940826416016\n",
      "step 65 : loss 59.241458892822266\n",
      "step 66 : loss 58.92363357543945\n",
      "step 67 : loss 58.83224105834961\n",
      "step 68 : loss 58.673030853271484\n",
      "step 69 : loss 58.37001419067383\n",
      "step 70 : loss 58.22699737548828\n",
      "step 71 : loss 57.797119140625\n",
      "step 72 : loss 57.564510345458984\n",
      "step 73 : loss 57.21815872192383\n",
      "step 74 : loss 56.906646728515625\n",
      "step 75 : loss 56.6943244934082\n",
      "step 76 : loss 56.375850677490234\n",
      "step 77 : loss 56.19093704223633\n",
      "step 78 : loss 55.940372467041016\n",
      "step 79 : loss 55.701663970947266\n",
      "step 80 : loss 55.52777862548828\n",
      "step 81 : loss 55.28028869628906\n",
      "step 82 : loss 55.10079574584961\n",
      "step 83 : loss 54.86083984375\n",
      "step 84 : loss 54.62815856933594\n",
      "step 85 : loss 54.406497955322266\n",
      "step 86 : loss 54.15436553955078\n",
      "step 87 : loss 53.95169448852539\n",
      "step 88 : loss 53.712894439697266\n",
      "step 89 : loss 53.527061462402344\n",
      "step 90 : loss 53.304725646972656\n",
      "step 91 : loss 53.116371154785156\n",
      "step 92 : loss 52.93544006347656\n",
      "step 93 : loss 52.735939025878906\n",
      "step 94 : loss 52.57854461669922\n",
      "step 95 : loss 52.402584075927734\n",
      "step 96 : loss 52.22291564941406\n",
      "step 97 : loss 52.087615966796875\n",
      "step 98 : loss 51.97739791870117\n",
      "step 99 : loss 51.858741760253906\n",
      "step 100 : loss 51.737709045410156\n",
      "step 101 : loss 51.61626434326172\n",
      "step 102 : loss 51.49945831298828\n",
      "step 103 : loss 51.38380432128906\n",
      "step 104 : loss 51.27494812011719\n",
      "step 105 : loss 51.176544189453125\n",
      "step 106 : loss 51.09624099731445\n",
      "step 107 : loss 51.067108154296875\n",
      "step 108 : loss 51.10493469238281\n",
      "step 109 : loss 51.142494201660156\n",
      "step 110 : loss 50.851016998291016\n",
      "step 111 : loss 50.698787689208984\n",
      "step 112 : loss 50.77827453613281\n",
      "step 113 : loss 50.637184143066406\n",
      "step 114 : loss 50.48774337768555\n",
      "step 115 : loss 50.519954681396484\n",
      "step 116 : loss 50.41850280761719\n",
      "step 117 : loss 50.31563949584961\n",
      "step 118 : loss 50.322227478027344\n",
      "step 119 : loss 50.227882385253906\n",
      "step 120 : loss 50.14906311035156\n",
      "step 121 : loss 50.133182525634766\n",
      "step 122 : loss 50.04047393798828\n",
      "step 123 : loss 49.96491622924805\n",
      "step 124 : loss 49.931884765625\n",
      "step 125 : loss 49.84480285644531\n",
      "step 126 : loss 49.757171630859375\n",
      "step 127 : loss 49.704559326171875\n",
      "step 128 : loss 49.624900817871094\n",
      "step 129 : loss 49.519065856933594\n",
      "step 130 : loss 49.427154541015625\n",
      "step 131 : loss 49.33928298950195\n",
      "step 132 : loss 49.224063873291016\n",
      "step 133 : loss 49.07966613769531\n",
      "step 134 : loss 48.928409576416016\n",
      "step 135 : loss 48.767127990722656\n",
      "step 136 : loss 48.57322692871094\n",
      "step 137 : loss 48.33466339111328\n",
      "step 138 : loss 48.035919189453125\n",
      "step 139 : loss 47.675865173339844\n",
      "step 140 : loss 47.2330436706543\n",
      "step 141 : loss 46.686363220214844\n",
      "step 142 : loss 45.99995422363281\n",
      "step 143 : loss 45.14152526855469\n",
      "step 144 : loss 44.08705520629883\n",
      "step 145 : loss 42.860294342041016\n",
      "step 146 : loss 42.2911262512207\n",
      "step 147 : loss 47.88886642456055\n",
      "step 148 : loss 41.6656494140625\n",
      "step 149 : loss 41.719703674316406\n",
      "step 150 : loss 44.11797332763672\n",
      "step 151 : loss 41.07791519165039\n",
      "step 152 : loss 44.19233322143555\n",
      "step 153 : loss 40.820980072021484\n",
      "step 154 : loss 41.6489372253418\n",
      "step 155 : loss 41.001808166503906\n",
      "step 156 : loss 40.62112045288086\n",
      "step 157 : loss 40.444313049316406\n",
      "step 158 : loss 40.67683410644531\n",
      "step 159 : loss 39.400936126708984\n",
      "step 160 : loss 40.719207763671875\n",
      "step 161 : loss 39.1662712097168\n",
      "step 162 : loss 39.79465866088867\n",
      "step 163 : loss 39.79374313354492\n",
      "step 164 : loss 38.69313049316406\n",
      "step 165 : loss 39.60517883300781\n",
      "step 166 : loss 38.723411560058594\n",
      "step 167 : loss 38.81859588623047\n",
      "step 168 : loss 38.87272262573242\n",
      "step 169 : loss 38.42658615112305\n",
      "step 170 : loss 38.462589263916016\n",
      "step 171 : loss 38.41572189331055\n",
      "step 172 : loss 38.105316162109375\n",
      "step 173 : loss 38.201568603515625\n",
      "step 174 : loss 38.05647277832031\n",
      "step 175 : loss 37.83721923828125\n",
      "step 176 : loss 38.010433197021484\n",
      "step 177 : loss 37.635196685791016\n",
      "step 178 : loss 37.79743576049805\n",
      "step 179 : loss 37.590030670166016\n",
      "step 180 : loss 37.500850677490234\n",
      "step 181 : loss 37.53679656982422\n",
      "step 182 : loss 37.28254699707031\n",
      "step 183 : loss 37.396018981933594\n",
      "step 184 : loss 37.175682067871094\n",
      "step 185 : loss 37.19025802612305\n",
      "step 186 : loss 37.0979118347168\n",
      "step 187 : loss 36.99807357788086\n",
      "step 188 : loss 36.98563766479492\n",
      "step 189 : loss 36.84968566894531\n",
      "step 190 : loss 36.836570739746094\n",
      "step 191 : loss 36.73664474487305\n",
      "step 192 : loss 36.6830940246582\n",
      "step 193 : loss 36.633506774902344\n",
      "step 194 : loss 36.54545593261719\n",
      "step 195 : loss 36.51849365234375\n",
      "step 196 : loss 36.42601013183594\n",
      "step 197 : loss 36.39088439941406\n",
      "step 198 : loss 36.31837463378906\n",
      "step 199 : loss 36.26258087158203\n",
      "step 200 : loss 36.21409225463867\n",
      "step 201 : loss 36.143028259277344\n",
      "step 202 : loss 36.10981750488281\n",
      "step 203 : loss 36.033050537109375\n",
      "step 204 : loss 36.004764556884766\n",
      "step 205 : loss 35.93109130859375\n",
      "step 206 : loss 35.899253845214844\n",
      "step 207 : loss 35.834312438964844\n",
      "step 208 : loss 35.79674530029297\n",
      "step 209 : loss 35.739810943603516\n",
      "step 210 : loss 35.697486877441406\n",
      "step 211 : loss 35.647220611572266\n",
      "step 212 : loss 35.60211181640625\n",
      "step 213 : loss 35.555912017822266\n",
      "step 214 : loss 35.5098762512207\n",
      "step 215 : loss 35.465816497802734\n",
      "step 216 : loss 35.419857025146484\n",
      "step 217 : loss 35.37702178955078\n",
      "step 218 : loss 35.33170700073242\n",
      "step 219 : loss 35.28968811035156\n",
      "step 220 : loss 35.245277404785156\n",
      "step 221 : loss 35.20400619506836\n",
      "step 222 : loss 35.16006088256836\n",
      "step 223 : loss 35.119232177734375\n",
      "step 224 : loss 35.075565338134766\n",
      "step 225 : loss 35.03535461425781\n",
      "step 226 : loss 34.991943359375\n",
      "step 227 : loss 34.952274322509766\n",
      "step 228 : loss 34.90924835205078\n",
      "step 229 : loss 34.869956970214844\n",
      "step 230 : loss 34.827415466308594\n",
      "step 231 : loss 34.78815460205078\n",
      "step 232 : loss 34.74633026123047\n",
      "step 233 : loss 34.70698165893555\n",
      "step 234 : loss 34.66594314575195\n",
      "step 235 : loss 34.626468658447266\n",
      "step 236 : loss 34.586158752441406\n",
      "step 237 : loss 34.546730041503906\n",
      "step 238 : loss 34.50686264038086\n",
      "step 239 : loss 34.4677619934082\n",
      "step 240 : loss 34.42821502685547\n",
      "step 241 : loss 34.38963317871094\n",
      "step 242 : loss 34.350345611572266\n",
      "step 243 : loss 34.31220626831055\n",
      "step 244 : loss 34.27335739135742\n",
      "step 245 : loss 34.235477447509766\n",
      "step 246 : loss 34.197242736816406\n",
      "step 247 : loss 34.159515380859375\n",
      "step 248 : loss 34.121944427490234\n",
      "step 249 : loss 34.084495544433594\n",
      "step 250 : loss 34.047420501708984\n",
      "step 251 : loss 34.01042175292969\n",
      "step 252 : loss 33.9737434387207\n",
      "step 253 : loss 33.93722915649414\n",
      "step 254 : loss 33.901004791259766\n",
      "step 255 : loss 33.86488723754883\n",
      "step 256 : loss 33.82916259765625\n",
      "step 257 : loss 33.79344940185547\n",
      "step 258 : loss 33.75813293457031\n",
      "step 259 : loss 33.7228889465332\n",
      "step 260 : loss 33.687896728515625\n",
      "step 261 : loss 33.65312576293945\n",
      "step 262 : loss 33.618465423583984\n",
      "step 263 : loss 33.584049224853516\n",
      "step 264 : loss 33.549739837646484\n",
      "step 265 : loss 33.515628814697266\n",
      "step 266 : loss 33.481590270996094\n",
      "step 267 : loss 33.447696685791016\n",
      "step 268 : loss 33.41383361816406\n",
      "step 269 : loss 33.38005447387695\n",
      "step 270 : loss 33.3463020324707\n",
      "step 271 : loss 33.3124885559082\n",
      "step 272 : loss 33.2786750793457\n",
      "step 273 : loss 33.244747161865234\n",
      "step 274 : loss 33.210693359375\n",
      "step 275 : loss 33.17646026611328\n",
      "step 276 : loss 33.1419792175293\n",
      "step 277 : loss 33.107215881347656\n",
      "step 278 : loss 33.072059631347656\n",
      "step 279 : loss 33.036476135253906\n",
      "step 280 : loss 33.0003662109375\n",
      "step 281 : loss 32.96363830566406\n",
      "step 282 : loss 32.92625427246094\n",
      "step 283 : loss 32.88801574707031\n",
      "step 284 : loss 32.84886169433594\n",
      "step 285 : loss 32.808677673339844\n",
      "step 286 : loss 32.76734924316406\n",
      "step 287 : loss 32.724761962890625\n",
      "step 288 : loss 32.680721282958984\n",
      "step 289 : loss 32.635009765625\n",
      "step 290 : loss 32.58745574951172\n",
      "step 291 : loss 32.53778839111328\n",
      "step 292 : loss 32.485740661621094\n",
      "step 293 : loss 32.4310302734375\n",
      "step 294 : loss 32.37324905395508\n",
      "step 295 : loss 32.31207275390625\n",
      "step 296 : loss 32.2470588684082\n",
      "step 297 : loss 32.17776870727539\n",
      "step 298 : loss 32.10371017456055\n",
      "step 299 : loss 32.02412414550781\n",
      "step 300 : loss 31.938203811645508\n",
      "step 301 : loss 31.845050811767578\n",
      "step 302 : loss 31.74356460571289\n",
      "step 303 : loss 31.63247299194336\n",
      "step 304 : loss 31.510263442993164\n",
      "step 305 : loss 31.375102996826172\n",
      "step 306 : loss 31.224834442138672\n",
      "step 307 : loss 31.056865692138672\n",
      "step 308 : loss 30.868112564086914\n",
      "step 309 : loss 30.654821395874023\n",
      "step 310 : loss 30.412466049194336\n",
      "step 311 : loss 30.1356201171875\n",
      "step 312 : loss 29.817739486694336\n",
      "step 313 : loss 29.451112747192383\n",
      "step 314 : loss 29.02672576904297\n",
      "step 315 : loss 28.5340518951416\n",
      "step 316 : loss 27.961767196655273\n",
      "step 317 : loss 27.298290252685547\n",
      "step 318 : loss 26.535938262939453\n",
      "step 319 : loss 25.682838439941406\n",
      "step 320 : loss 24.938085556030273\n",
      "step 321 : loss 27.05978012084961\n",
      "step 322 : loss 38.708988189697266\n",
      "step 323 : loss 32.220863342285156\n",
      "step 324 : loss 31.584552764892578\n",
      "step 325 : loss 27.760604858398438\n",
      "step 326 : loss 24.829866409301758\n",
      "step 327 : loss 28.06863021850586\n",
      "step 328 : loss 26.264545440673828\n",
      "step 329 : loss 24.687788009643555\n",
      "step 330 : loss 24.518091201782227\n",
      "step 331 : loss 23.9666805267334\n",
      "step 332 : loss 24.69652557373047\n",
      "step 333 : loss 22.063371658325195\n",
      "step 334 : loss 24.37030792236328\n",
      "step 335 : loss 21.675485610961914\n",
      "step 336 : loss 23.201805114746094\n",
      "step 337 : loss 22.057903289794922\n",
      "step 338 : loss 22.04880142211914\n",
      "step 339 : loss 21.978574752807617\n",
      "step 340 : loss 21.42967987060547\n",
      "step 341 : loss 21.859661102294922\n",
      "step 342 : loss 20.945110321044922\n",
      "step 343 : loss 21.562761306762695\n",
      "step 344 : loss 20.833498001098633\n",
      "step 345 : loss 21.024513244628906\n",
      "step 346 : loss 20.905424118041992\n",
      "step 347 : loss 20.522258758544922\n",
      "step 348 : loss 20.788602828979492\n",
      "step 349 : loss 20.200820922851562\n",
      "step 350 : loss 20.62860107421875\n",
      "step 351 : loss 19.984294891357422\n",
      "step 352 : loss 20.42469596862793\n",
      "step 353 : loss 19.87921714782715\n",
      "step 354 : loss 20.171968460083008\n",
      "step 355 : loss 19.84026527404785\n",
      "step 356 : loss 19.940214157104492\n",
      "step 357 : loss 19.770130157470703\n",
      "step 358 : loss 19.745277404785156\n",
      "step 359 : loss 19.701766967773438\n",
      "step 360 : loss 19.56279754638672\n",
      "step 361 : loss 19.601673126220703\n",
      "step 362 : loss 19.430749893188477\n",
      "step 363 : loss 19.48700523376465\n",
      "step 364 : loss 19.30356788635254\n",
      "step 365 : loss 19.375625610351562\n",
      "step 366 : loss 19.1989803314209\n",
      "step 367 : loss 19.25769805908203\n",
      "step 368 : loss 19.10114288330078\n",
      "step 369 : loss 19.146854400634766\n",
      "step 370 : loss 19.012048721313477\n",
      "step 371 : loss 19.03011131286621\n",
      "step 372 : loss 18.926546096801758\n",
      "step 373 : loss 18.922895431518555\n",
      "step 374 : loss 18.835161209106445\n",
      "step 375 : loss 18.822402954101562\n",
      "step 376 : loss 18.73992347717285\n",
      "step 377 : loss 18.72846794128418\n",
      "step 378 : loss 18.646459579467773\n",
      "step 379 : loss 18.632736206054688\n",
      "step 380 : loss 18.55895233154297\n",
      "step 381 : loss 18.538949966430664\n",
      "step 382 : loss 18.471511840820312\n",
      "step 383 : loss 18.447893142700195\n",
      "step 384 : loss 18.385238647460938\n",
      "step 385 : loss 18.358116149902344\n",
      "step 386 : loss 18.300514221191406\n",
      "step 387 : loss 18.26949119567871\n",
      "step 388 : loss 18.214786529541016\n",
      "step 389 : loss 18.183935165405273\n",
      "step 390 : loss 18.130666732788086\n",
      "step 391 : loss 18.096435546875\n",
      "step 392 : loss 18.049901962280273\n",
      "step 393 : loss 18.009737014770508\n",
      "step 394 : loss 17.968582153320312\n",
      "step 395 : loss 17.92604637145996\n",
      "step 396 : loss 17.88759422302246\n",
      "step 397 : loss 17.843320846557617\n",
      "step 398 : loss 17.80718231201172\n",
      "step 399 : loss 17.76297378540039\n",
      "step 400 : loss 17.727123260498047\n",
      "step 401 : loss 17.683988571166992\n",
      "step 402 : loss 17.647472381591797\n",
      "step 403 : loss 17.607757568359375\n",
      "step 404 : loss 17.568580627441406\n",
      "step 405 : loss 17.5317325592041\n",
      "step 406 : loss 17.4923095703125\n",
      "step 407 : loss 17.45659828186035\n",
      "step 408 : loss 17.41765785217285\n",
      "step 409 : loss 17.38190460205078\n",
      "step 410 : loss 17.34465980529785\n",
      "step 411 : loss 17.308250427246094\n",
      "step 412 : loss 17.273170471191406\n",
      "step 413 : loss 17.236190795898438\n",
      "step 414 : loss 17.201658248901367\n",
      "step 415 : loss 17.165966033935547\n",
      "step 416 : loss 17.13113784790039\n",
      "step 417 : loss 17.096927642822266\n",
      "step 418 : loss 17.062040328979492\n",
      "step 419 : loss 17.028621673583984\n",
      "step 420 : loss 16.994422912597656\n",
      "step 421 : loss 16.96078109741211\n",
      "step 422 : loss 16.927810668945312\n",
      "step 423 : loss 16.89430046081543\n",
      "step 424 : loss 16.861675262451172\n",
      "step 425 : loss 16.829069137573242\n",
      "step 426 : loss 16.796607971191406\n",
      "step 427 : loss 16.765039443969727\n",
      "step 428 : loss 16.73370361328125\n",
      "step 429 : loss 16.704185485839844\n",
      "step 430 : loss 16.678808212280273\n",
      "step 431 : loss 16.664592742919922\n",
      "step 432 : loss 16.68120765686035\n",
      "step 433 : loss 16.781017303466797\n",
      "step 434 : loss 16.992677688598633\n",
      "step 435 : loss 17.300552368164062\n",
      "step 436 : loss 17.044099807739258\n",
      "step 437 : loss 16.676658630371094\n",
      "step 438 : loss 16.44118881225586\n",
      "step 439 : loss 16.512052536010742\n",
      "step 440 : loss 16.733192443847656\n",
      "step 441 : loss 16.70146942138672\n",
      "step 442 : loss 16.499353408813477\n",
      "step 443 : loss 16.311290740966797\n",
      "step 444 : loss 16.329696655273438\n",
      "step 445 : loss 16.45758819580078\n",
      "step 446 : loss 16.444900512695312\n",
      "step 447 : loss 16.31424331665039\n",
      "step 448 : loss 16.180156707763672\n",
      "step 449 : loss 16.170652389526367\n",
      "step 450 : loss 16.238317489624023\n",
      "step 451 : loss 16.245765686035156\n",
      "step 452 : loss 16.175630569458008\n",
      "step 453 : loss 16.068361282348633\n",
      "step 454 : loss 16.019733428955078\n",
      "step 455 : loss 16.032787322998047\n",
      "step 456 : loss 16.054737091064453\n",
      "step 457 : loss 16.04741859436035\n",
      "step 458 : loss 15.986865997314453\n",
      "step 459 : loss 15.921921730041504\n",
      "step 460 : loss 15.875452041625977\n",
      "step 461 : loss 15.854981422424316\n",
      "step 462 : loss 15.849278450012207\n",
      "step 463 : loss 15.842082977294922\n",
      "step 464 : loss 15.830755233764648\n",
      "step 465 : loss 15.808484077453613\n",
      "step 466 : loss 15.789649963378906\n",
      "step 467 : loss 15.770956039428711\n",
      "step 468 : loss 15.762184143066406\n",
      "step 469 : loss 15.757347106933594\n",
      "step 470 : loss 15.756991386413574\n",
      "step 471 : loss 15.75521469116211\n",
      "step 472 : loss 15.748592376708984\n",
      "step 473 : loss 15.732699394226074\n",
      "step 474 : loss 15.712152481079102\n",
      "step 475 : loss 15.680815696716309\n",
      "step 476 : loss 15.658977508544922\n",
      "step 477 : loss 15.627979278564453\n",
      "step 478 : loss 15.618305206298828\n",
      "step 479 : loss 15.592171669006348\n",
      "step 480 : loss 15.585031509399414\n",
      "step 481 : loss 15.547370910644531\n",
      "step 482 : loss 15.519135475158691\n",
      "step 483 : loss 15.462667465209961\n",
      "step 484 : loss 15.414896011352539\n",
      "step 485 : loss 15.35937213897705\n",
      "step 486 : loss 15.315157890319824\n",
      "step 487 : loss 15.275177001953125\n",
      "step 488 : loss 15.244571685791016\n",
      "step 489 : loss 15.220612525939941\n",
      "step 490 : loss 15.207619667053223\n",
      "step 491 : loss 15.211106300354004\n",
      "step 492 : loss 15.251482009887695\n",
      "step 493 : loss 15.359572410583496\n",
      "step 494 : loss 15.571294784545898\n",
      "step 495 : loss 15.817744255065918\n",
      "step 496 : loss 15.805480003356934\n",
      "step 497 : loss 15.537239074707031\n",
      "step 498 : loss 15.198381423950195\n",
      "step 499 : loss 15.088823318481445\n",
      "step 500 : loss 15.168710708618164\n",
      "step 501 : loss 15.342582702636719\n",
      "step 502 : loss 15.338345527648926\n",
      "step 503 : loss 15.174152374267578\n",
      "step 504 : loss 14.960712432861328\n",
      "step 505 : loss 14.874670028686523\n",
      "step 506 : loss 14.929848670959473\n",
      "step 507 : loss 15.049888610839844\n",
      "step 508 : loss 15.138998031616211\n",
      "step 509 : loss 15.056985855102539\n",
      "step 510 : loss 14.949666976928711\n",
      "step 511 : loss 14.890064239501953\n",
      "step 512 : loss 14.969286918640137\n",
      "step 513 : loss 15.08897590637207\n",
      "step 514 : loss 15.186667442321777\n",
      "step 515 : loss 15.05450439453125\n",
      "step 516 : loss 14.897470474243164\n",
      "step 517 : loss 14.766121864318848\n",
      "step 518 : loss 14.739696502685547\n",
      "step 519 : loss 14.737740516662598\n",
      "step 520 : loss 14.723729133605957\n",
      "step 521 : loss 14.654755592346191\n",
      "step 522 : loss 14.582380294799805\n",
      "step 523 : loss 14.53539752960205\n",
      "step 524 : loss 14.531658172607422\n",
      "step 525 : loss 14.57386589050293\n",
      "step 526 : loss 14.641789436340332\n",
      "step 527 : loss 14.746294021606445\n",
      "step 528 : loss 14.792204856872559\n",
      "step 529 : loss 14.859606742858887\n",
      "step 530 : loss 14.792171478271484\n",
      "step 531 : loss 14.779080390930176\n",
      "step 532 : loss 14.688345909118652\n",
      "step 533 : loss 14.629289627075195\n",
      "step 534 : loss 14.535440444946289\n",
      "step 535 : loss 14.436232566833496\n",
      "step 536 : loss 14.350833892822266\n",
      "step 537 : loss 14.292281150817871\n",
      "step 538 : loss 14.271934509277344\n",
      "step 539 : loss 14.290129661560059\n",
      "step 540 : loss 14.357810020446777\n",
      "step 541 : loss 14.454773902893066\n",
      "step 542 : loss 14.59317398071289\n",
      "step 543 : loss 14.612283706665039\n",
      "step 544 : loss 14.58313274383545\n",
      "step 545 : loss 14.402134895324707\n",
      "step 546 : loss 14.268580436706543\n",
      "step 547 : loss 14.167353630065918\n",
      "step 548 : loss 14.12537956237793\n",
      "step 549 : loss 14.114570617675781\n",
      "step 550 : loss 14.126813888549805\n",
      "step 551 : loss 14.141840934753418\n",
      "step 552 : loss 14.162693977355957\n",
      "step 553 : loss 14.17166519165039\n",
      "step 554 : loss 14.176814079284668\n",
      "step 555 : loss 14.181537628173828\n",
      "step 556 : loss 14.181987762451172\n",
      "step 557 : loss 14.223213195800781\n",
      "step 558 : loss 14.248390197753906\n",
      "step 559 : loss 14.343631744384766\n",
      "step 560 : loss 14.316407203674316\n",
      "step 561 : loss 14.310016632080078\n",
      "step 562 : loss 14.150376319885254\n",
      "step 563 : loss 14.03075885772705\n",
      "step 564 : loss 13.914384841918945\n",
      "step 565 : loss 13.84918212890625\n",
      "step 566 : loss 13.818262100219727\n",
      "step 567 : loss 13.821988105773926\n",
      "step 568 : loss 13.864677429199219\n",
      "step 569 : loss 13.943931579589844\n",
      "step 570 : loss 14.087435722351074\n",
      "step 571 : loss 14.160425186157227\n",
      "step 572 : loss 14.219796180725098\n",
      "step 573 : loss 14.043858528137207\n",
      "step 574 : loss 13.89317512512207\n",
      "step 575 : loss 13.74443244934082\n",
      "step 576 : loss 13.66303825378418\n",
      "step 577 : loss 13.621959686279297\n",
      "step 578 : loss 13.60992431640625\n",
      "step 579 : loss 13.618719100952148\n",
      "step 580 : loss 13.648536682128906\n",
      "step 581 : loss 13.707530975341797\n",
      "step 582 : loss 13.780518531799316\n",
      "step 583 : loss 13.879602432250977\n",
      "step 584 : loss 13.888101577758789\n",
      "step 585 : loss 13.875044822692871\n",
      "step 586 : loss 13.74695110321045\n",
      "step 587 : loss 13.667092323303223\n",
      "step 588 : loss 13.615017890930176\n",
      "step 589 : loss 13.664806365966797\n",
      "step 590 : loss 13.774993896484375\n",
      "step 591 : loss 13.989465713500977\n",
      "step 592 : loss 13.96849250793457\n",
      "step 593 : loss 13.8714599609375\n",
      "step 594 : loss 13.620399475097656\n",
      "step 595 : loss 13.480664253234863\n",
      "step 596 : loss 13.478411674499512\n",
      "step 597 : loss 13.588150978088379\n",
      "step 598 : loss 13.788631439208984\n",
      "step 599 : loss 13.813699722290039\n",
      "step 600 : loss 13.764805793762207\n",
      "step 601 : loss 13.552223205566406\n",
      "step 602 : loss 13.476543426513672\n",
      "step 603 : loss 13.497203826904297\n",
      "step 604 : loss 13.60824966430664\n",
      "step 605 : loss 13.613176345825195\n",
      "step 606 : loss 13.559544563293457\n",
      "step 607 : loss 13.429779052734375\n",
      "step 608 : loss 13.406347274780273\n",
      "step 609 : loss 13.457691192626953\n",
      "step 610 : loss 13.573577880859375\n",
      "step 611 : loss 13.541242599487305\n",
      "step 612 : loss 13.440267562866211\n",
      "step 613 : loss 13.282208442687988\n",
      "step 614 : loss 13.200379371643066\n",
      "step 615 : loss 13.189383506774902\n",
      "step 616 : loss 13.219267845153809\n",
      "step 617 : loss 13.236303329467773\n",
      "step 618 : loss 13.216789245605469\n",
      "step 619 : loss 13.16494369506836\n",
      "step 620 : loss 13.12927532196045\n",
      "step 621 : loss 13.133216857910156\n",
      "step 622 : loss 13.225341796875\n",
      "step 623 : loss 13.373477935791016\n",
      "step 624 : loss 13.615147590637207\n",
      "step 625 : loss 13.593951225280762\n",
      "step 626 : loss 13.53381633758545\n",
      "step 627 : loss 13.282848358154297\n",
      "step 628 : loss 13.142029762268066\n",
      "step 629 : loss 13.046797752380371\n",
      "step 630 : loss 13.010258674621582\n",
      "step 631 : loss 12.997967720031738\n",
      "step 632 : loss 13.00967788696289\n",
      "step 633 : loss 13.063451766967773\n",
      "step 634 : loss 13.165410995483398\n",
      "step 635 : loss 13.34062385559082\n",
      "step 636 : loss 13.434475898742676\n",
      "step 637 : loss 13.421514511108398\n",
      "step 638 : loss 13.204418182373047\n",
      "step 639 : loss 13.000378608703613\n",
      "step 640 : loss 12.874238014221191\n",
      "step 641 : loss 12.828788757324219\n",
      "step 642 : loss 12.857418060302734\n",
      "step 643 : loss 12.951800346374512\n",
      "step 644 : loss 13.133910179138184\n",
      "step 645 : loss 13.250347137451172\n",
      "step 646 : loss 13.335315704345703\n",
      "step 647 : loss 13.134486198425293\n",
      "step 648 : loss 12.983804702758789\n",
      "step 649 : loss 12.865344047546387\n",
      "step 650 : loss 12.850295066833496\n",
      "step 651 : loss 12.887659072875977\n",
      "step 652 : loss 12.980572700500488\n",
      "step 653 : loss 13.034246444702148\n",
      "step 654 : loss 13.039271354675293\n",
      "step 655 : loss 12.9471435546875\n",
      "step 656 : loss 12.843720436096191\n",
      "step 657 : loss 12.75650405883789\n",
      "step 658 : loss 12.713122367858887\n",
      "step 659 : loss 12.705768585205078\n",
      "step 660 : loss 12.719097137451172\n",
      "step 661 : loss 12.747834205627441\n",
      "step 662 : loss 12.760467529296875\n",
      "step 663 : loss 12.763272285461426\n",
      "step 664 : loss 12.72970199584961\n",
      "step 665 : loss 12.691615104675293\n",
      "step 666 : loss 12.643470764160156\n",
      "step 667 : loss 12.616761207580566\n",
      "step 668 : loss 12.611310958862305\n",
      "step 669 : loss 12.665610313415527\n",
      "step 670 : loss 12.79389762878418\n",
      "step 671 : loss 13.076830863952637\n",
      "step 672 : loss 13.251961708068848\n",
      "step 673 : loss 13.34544563293457\n",
      "step 674 : loss 12.974817276000977\n",
      "step 675 : loss 12.751367568969727\n",
      "step 676 : loss 12.63748550415039\n",
      "step 677 : loss 12.685466766357422\n",
      "step 678 : loss 12.809041976928711\n",
      "step 679 : loss 12.894269943237305\n",
      "step 680 : loss 12.882898330688477\n",
      "step 681 : loss 12.692025184631348\n",
      "step 682 : loss 12.536999702453613\n",
      "step 683 : loss 12.439108848571777\n",
      "step 684 : loss 12.423264503479004\n",
      "step 685 : loss 12.472476959228516\n",
      "step 686 : loss 12.595544815063477\n",
      "step 687 : loss 12.717961311340332\n",
      "step 688 : loss 12.851003646850586\n",
      "step 689 : loss 12.771446228027344\n",
      "step 690 : loss 12.710711479187012\n",
      "step 691 : loss 12.613930702209473\n",
      "step 692 : loss 12.634177207946777\n",
      "step 693 : loss 12.641034126281738\n",
      "step 694 : loss 12.6590576171875\n",
      "step 695 : loss 12.554906845092773\n",
      "step 696 : loss 12.453315734863281\n",
      "step 697 : loss 12.354541778564453\n",
      "step 698 : loss 12.306950569152832\n",
      "step 699 : loss 12.300134658813477\n",
      "step 700 : loss 12.337371826171875\n",
      "step 701 : loss 12.402749061584473\n",
      "step 702 : loss 12.516470909118652\n",
      "step 703 : loss 12.579080581665039\n",
      "step 704 : loss 12.655559539794922\n",
      "step 705 : loss 12.583802223205566\n",
      "step 706 : loss 12.555290222167969\n",
      "step 707 : loss 12.485201835632324\n",
      "step 708 : loss 12.481952667236328\n",
      "step 709 : loss 12.442230224609375\n",
      "step 710 : loss 12.425477027893066\n",
      "step 711 : loss 12.366474151611328\n",
      "step 712 : loss 12.339923858642578\n",
      "step 713 : loss 12.330728530883789\n",
      "step 714 : loss 12.406965255737305\n",
      "step 715 : loss 12.517553329467773\n",
      "step 716 : loss 12.717672348022461\n",
      "step 717 : loss 12.681086540222168\n",
      "step 718 : loss 12.595096588134766\n",
      "step 719 : loss 12.362561225891113\n",
      "step 720 : loss 12.214509010314941\n",
      "step 721 : loss 12.124960899353027\n",
      "step 722 : loss 12.086137771606445\n",
      "step 723 : loss 12.074431419372559\n",
      "step 724 : loss 12.079867362976074\n",
      "step 725 : loss 12.098920822143555\n",
      "step 726 : loss 12.132166862487793\n",
      "step 727 : loss 12.183446884155273\n",
      "step 728 : loss 12.250349044799805\n",
      "step 729 : loss 12.334349632263184\n",
      "step 730 : loss 12.383893013000488\n",
      "step 731 : loss 12.41395378112793\n",
      "step 732 : loss 12.349234580993652\n",
      "step 733 : loss 12.299554824829102\n",
      "step 734 : loss 12.226673126220703\n",
      "step 735 : loss 12.204401016235352\n",
      "step 736 : loss 12.182190895080566\n",
      "step 737 : loss 12.181617736816406\n",
      "step 738 : loss 12.166202545166016\n",
      "step 739 : loss 12.174046516418457\n",
      "step 740 : loss 12.197793960571289\n",
      "step 741 : loss 12.343740463256836\n",
      "step 742 : loss 12.577336311340332\n",
      "step 743 : loss 12.960992813110352\n",
      "step 744 : loss 12.72979736328125\n",
      "step 745 : loss 12.478379249572754\n",
      "step 746 : loss 12.141430854797363\n",
      "step 747 : loss 11.999555587768555\n",
      "step 748 : loss 11.958264350891113\n",
      "step 749 : loss 11.987191200256348\n",
      "step 750 : loss 12.066350936889648\n",
      "step 751 : loss 12.171356201171875\n",
      "step 752 : loss 12.287006378173828\n",
      "step 753 : loss 12.282001495361328\n",
      "step 754 : loss 12.270252227783203\n",
      "step 755 : loss 12.188739776611328\n",
      "step 756 : loss 12.213116645812988\n",
      "step 757 : loss 12.252102851867676\n",
      "step 758 : loss 12.34667682647705\n",
      "step 759 : loss 12.262813568115234\n",
      "step 760 : loss 12.167834281921387\n",
      "step 761 : loss 12.011957168579102\n",
      "step 762 : loss 11.924644470214844\n",
      "step 763 : loss 11.876891136169434\n",
      "step 764 : loss 11.866101264953613\n",
      "step 765 : loss 11.870348930358887\n",
      "step 766 : loss 11.882804870605469\n",
      "step 767 : loss 11.888667106628418\n",
      "step 768 : loss 11.889803886413574\n",
      "step 769 : loss 11.878281593322754\n",
      "step 770 : loss 11.865199089050293\n",
      "step 771 : loss 11.848676681518555\n",
      "step 772 : loss 11.847127914428711\n",
      "step 773 : loss 11.868059158325195\n",
      "step 774 : loss 11.962693214416504\n",
      "step 775 : loss 12.155378341674805\n",
      "step 776 : loss 12.510489463806152\n",
      "step 777 : loss 12.55756950378418\n",
      "step 778 : loss 12.428299903869629\n",
      "step 779 : loss 12.043248176574707\n",
      "step 780 : loss 11.85057258605957\n",
      "step 781 : loss 11.79195785522461\n",
      "step 782 : loss 11.8403959274292\n",
      "step 783 : loss 11.954970359802246\n",
      "step 784 : loss 12.102662086486816\n",
      "step 785 : loss 12.160126686096191\n",
      "step 786 : loss 12.073975563049316\n",
      "step 787 : loss 11.915485382080078\n",
      "step 788 : loss 11.785843849182129\n",
      "step 789 : loss 11.721488952636719\n",
      "step 790 : loss 11.720584869384766\n",
      "step 791 : loss 11.774989128112793\n",
      "step 792 : loss 11.903464317321777\n",
      "step 793 : loss 12.051105499267578\n",
      "step 794 : loss 12.239468574523926\n",
      "step 795 : loss 12.20154094696045\n",
      "step 796 : loss 12.173104286193848\n",
      "step 797 : loss 12.046329498291016\n",
      "step 798 : loss 12.024436950683594\n",
      "step 799 : loss 11.950727462768555\n",
      "step 800 : loss 11.913930892944336\n",
      "step 801 : loss 11.81962776184082\n",
      "step 802 : loss 11.748730659484863\n",
      "step 803 : loss 11.685527801513672\n",
      "step 804 : loss 11.646648406982422\n",
      "step 805 : loss 11.626908302307129\n",
      "step 806 : loss 11.623430252075195\n",
      "step 807 : loss 11.6400785446167\n",
      "step 808 : loss 11.68216323852539\n",
      "step 809 : loss 11.776444435119629\n",
      "step 810 : loss 11.913848876953125\n",
      "step 811 : loss 12.122344017028809\n",
      "step 812 : loss 12.174368858337402\n",
      "step 813 : loss 12.175429344177246\n",
      "step 814 : loss 11.972694396972656\n",
      "step 815 : loss 11.863529205322266\n",
      "step 816 : loss 11.765289306640625\n",
      "step 817 : loss 11.745288848876953\n",
      "step 818 : loss 11.725223541259766\n",
      "step 819 : loss 11.720388412475586\n",
      "step 820 : loss 11.702103614807129\n",
      "step 821 : loss 11.67579460144043\n",
      "step 822 : loss 11.652338981628418\n",
      "step 823 : loss 11.62915325164795\n",
      "step 824 : loss 11.629115104675293\n",
      "step 825 : loss 11.643479347229004\n",
      "step 826 : loss 11.705131530761719\n",
      "step 827 : loss 11.782835960388184\n",
      "step 828 : loss 11.917261123657227\n",
      "step 829 : loss 11.941756248474121\n",
      "step 830 : loss 11.954947471618652\n",
      "step 831 : loss 11.810635566711426\n",
      "step 832 : loss 11.720848083496094\n",
      "step 833 : loss 11.645638465881348\n",
      "step 834 : loss 11.661373138427734\n",
      "step 835 : loss 11.74957275390625\n",
      "step 836 : loss 11.958281517028809\n",
      "step 837 : loss 12.08960247039795\n",
      "step 838 : loss 12.083227157592773\n",
      "step 839 : loss 11.802323341369629\n",
      "step 840 : loss 11.589696884155273\n",
      "step 841 : loss 11.475358963012695\n",
      "step 842 : loss 11.452619552612305\n",
      "step 843 : loss 11.492340087890625\n",
      "step 844 : loss 11.575726509094238\n",
      "step 845 : loss 11.678487777709961\n",
      "step 846 : loss 11.723328590393066\n",
      "step 847 : loss 11.709774017333984\n",
      "step 848 : loss 11.617605209350586\n",
      "step 849 : loss 11.558021545410156\n",
      "step 850 : loss 11.544777870178223\n",
      "step 851 : loss 11.621337890625\n",
      "step 852 : loss 11.746243476867676\n",
      "step 853 : loss 11.915677070617676\n",
      "step 854 : loss 11.867203712463379\n",
      "step 855 : loss 11.779916763305664\n",
      "step 856 : loss 11.615333557128906\n",
      "step 857 : loss 11.540290832519531\n",
      "step 858 : loss 11.515900611877441\n",
      "step 859 : loss 11.550554275512695\n",
      "step 860 : loss 11.589825630187988\n",
      "step 861 : loss 11.641810417175293\n",
      "step 862 : loss 11.633864402770996\n",
      "step 863 : loss 11.61609172821045\n",
      "step 864 : loss 11.557412147521973\n",
      "step 865 : loss 11.530404090881348\n",
      "step 866 : loss 11.517099380493164\n",
      "step 867 : loss 11.557425498962402\n",
      "step 868 : loss 11.611551284790039\n",
      "step 869 : loss 11.703088760375977\n",
      "step 870 : loss 11.708183288574219\n",
      "step 871 : loss 11.697410583496094\n",
      "step 872 : loss 11.58966064453125\n",
      "step 873 : loss 11.512177467346191\n",
      "step 874 : loss 11.441198348999023\n",
      "step 875 : loss 11.409573554992676\n",
      "step 876 : loss 11.39811897277832\n",
      "step 877 : loss 11.416698455810547\n",
      "step 878 : loss 11.454302787780762\n",
      "step 879 : loss 11.531328201293945\n",
      "step 880 : loss 11.602115631103516\n",
      "step 881 : loss 11.68375015258789\n",
      "step 882 : loss 11.655614852905273\n",
      "step 883 : loss 11.605416297912598\n",
      "step 884 : loss 11.49618911743164\n",
      "step 885 : loss 11.422698974609375\n",
      "step 886 : loss 11.367406845092773\n",
      "step 887 : loss 11.345176696777344\n",
      "step 888 : loss 11.342917442321777\n",
      "step 889 : loss 11.367898941040039\n",
      "step 890 : loss 11.413280487060547\n",
      "step 891 : loss 11.4963960647583\n",
      "step 892 : loss 11.562424659729004\n",
      "step 893 : loss 11.632694244384766\n",
      "step 894 : loss 11.58386516571045\n",
      "step 895 : loss 11.528519630432129\n",
      "step 896 : loss 11.43044662475586\n",
      "step 897 : loss 11.374279022216797\n",
      "step 898 : loss 11.339643478393555\n",
      "step 899 : loss 11.347146034240723\n",
      "step 900 : loss 11.387770652770996\n",
      "step 901 : loss 11.498848915100098\n",
      "step 902 : loss 11.61655044555664\n",
      "step 903 : loss 11.781695365905762\n",
      "step 904 : loss 11.69961929321289\n",
      "step 905 : loss 11.606284141540527\n",
      "step 906 : loss 11.430859565734863\n",
      "step 907 : loss 11.334146499633789\n",
      "step 908 : loss 11.280793190002441\n",
      "step 909 : loss 11.273667335510254\n",
      "step 910 : loss 11.311661720275879\n",
      "step 911 : loss 11.405969619750977\n",
      "step 912 : loss 11.578812599182129\n",
      "step 913 : loss 11.7175874710083\n",
      "step 914 : loss 11.756278991699219\n",
      "step 915 : loss 11.562273979187012\n",
      "step 916 : loss 11.37788200378418\n",
      "step 917 : loss 11.250444412231445\n",
      "step 918 : loss 11.197600364685059\n",
      "step 919 : loss 11.193462371826172\n",
      "step 920 : loss 11.22726058959961\n",
      "step 921 : loss 11.293464660644531\n",
      "step 922 : loss 11.379654884338379\n",
      "step 923 : loss 11.450918197631836\n",
      "step 924 : loss 11.463504791259766\n",
      "step 925 : loss 11.406632423400879\n",
      "step 926 : loss 11.334551811218262\n",
      "step 927 : loss 11.277385711669922\n",
      "step 928 : loss 11.281895637512207\n",
      "step 929 : loss 11.344958305358887\n",
      "step 930 : loss 11.4990816116333\n",
      "step 931 : loss 11.628973007202148\n",
      "step 932 : loss 11.715271949768066\n",
      "step 933 : loss 11.560962677001953\n",
      "step 934 : loss 11.419777870178223\n",
      "step 935 : loss 11.296865463256836\n",
      "step 936 : loss 11.25204086303711\n",
      "step 937 : loss 11.239934921264648\n",
      "step 938 : loss 11.262822151184082\n",
      "step 939 : loss 11.285747528076172\n",
      "step 940 : loss 11.323233604431152\n",
      "step 941 : loss 11.332552909851074\n",
      "step 942 : loss 11.347029685974121\n",
      "step 943 : loss 11.328117370605469\n",
      "step 944 : loss 11.322759628295898\n",
      "step 945 : loss 11.305419921875\n",
      "step 946 : loss 11.304115295410156\n",
      "step 947 : loss 11.300304412841797\n",
      "step 948 : loss 11.298354148864746\n",
      "step 949 : loss 11.290558815002441\n",
      "step 950 : loss 11.273155212402344\n",
      "step 951 : loss 11.256002426147461\n",
      "step 952 : loss 11.233697891235352\n",
      "step 953 : loss 11.228933334350586\n",
      "step 954 : loss 11.230892181396484\n",
      "step 955 : loss 11.277360916137695\n",
      "step 956 : loss 11.34424114227295\n",
      "step 957 : loss 11.487370491027832\n",
      "step 958 : loss 11.569231033325195\n",
      "step 959 : loss 11.636028289794922\n",
      "step 960 : loss 11.502223014831543\n",
      "step 961 : loss 11.38072395324707\n",
      "step 962 : loss 11.253440856933594\n",
      "step 963 : loss 11.190177917480469\n",
      "step 964 : loss 11.154437065124512\n",
      "step 965 : loss 11.147726058959961\n",
      "step 966 : loss 11.153027534484863\n",
      "step 967 : loss 11.170928001403809\n",
      "step 968 : loss 11.19491958618164\n",
      "step 969 : loss 11.217650413513184\n",
      "step 970 : loss 11.236547470092773\n",
      "step 971 : loss 11.234831809997559\n",
      "step 972 : loss 11.223555564880371\n",
      "step 973 : loss 11.193697929382324\n",
      "step 974 : loss 11.166521072387695\n",
      "step 975 : loss 11.13838005065918\n",
      "step 976 : loss 11.121464729309082\n",
      "step 977 : loss 11.1106595993042\n",
      "step 978 : loss 11.111522674560547\n",
      "step 979 : loss 11.118746757507324\n",
      "step 980 : loss 11.139086723327637\n",
      "step 981 : loss 11.163247108459473\n",
      "step 982 : loss 11.205192565917969\n",
      "step 983 : loss 11.240344047546387\n",
      "step 984 : loss 11.30157470703125\n",
      "step 985 : loss 11.341486930847168\n",
      "step 986 : loss 11.425226211547852\n",
      "step 987 : loss 11.458227157592773\n",
      "step 988 : loss 11.526213645935059\n",
      "step 989 : loss 11.440421104431152\n",
      "step 990 : loss 11.382887840270996\n",
      "step 991 : loss 11.245613098144531\n",
      "step 992 : loss 11.167862892150879\n",
      "step 993 : loss 11.09980583190918\n",
      "step 994 : loss 11.066956520080566\n",
      "step 995 : loss 11.047952651977539\n",
      "step 996 : loss 11.047162055969238\n",
      "step 997 : loss 11.057944297790527\n",
      "step 998 : loss 11.088854789733887\n",
      "step 999 : loss 11.131056785583496\n",
      "Test Loss at the end of training: 12.800069808959961\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='finegrain-cs', config={\n",
    "    'lr': lr,\n",
    "    'num_steps': num_steps\n",
    "})\n",
    "\n",
    "path_dataset_train = \"../data/dataset_train/\"\n",
    "path_dataset_test = \"../data/dataset_test/\"\n",
    "dataset_train = ImageDataset(path_dataset_train).data\n",
    "dataset_test = ImageDataset(path_dataset_test).data\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.train()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "\n",
    "# Convert your datasets to DataLoader for easy batch processing\n",
    "# train_dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "\n",
    "# Modify your training loop\n",
    "for step in range(num_steps):\n",
    "    loss_iter = 0\n",
    "    for images in dataset_train:\n",
    "        images = image_to_tensor(images)\n",
    "        y = autoencoder(images)\n",
    "        loss = (y - images).norm()\n",
    "        loss_iter += loss\n",
    "\n",
    "    loss = loss_iter / len(dataset_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Log the loss to wandb\n",
    "    wandb.log({'step': step, 'loss': loss.item()})\n",
    "\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "# Testing at the end of training\n",
    "loss_iter_test = 0\n",
    "reconstructed_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_test in dataset_test:\n",
    "        images_test = image_to_tensor(images_test)\n",
    "        y_test = autoencoder(images_test)\n",
    "        loss_test = (y_test - images_test).norm()\n",
    "        loss_iter_test += loss_test\n",
    "\n",
    "        # # Append the reconstructed images for visualization\n",
    "        concat = Image.new('RGB', (256, 128))\n",
    "        concat.paste(tensor_to_image(images_test.data), (0, 0))\n",
    "        concat.paste(tensor_to_image(y_test.data), (128, 0))\n",
    "        \n",
    "        reconstructed_images.append(concat)\n",
    "\n",
    "images = [PIL.Image.fromarray(np.array(image)) for image in reconstructed_images]\n",
    "\n",
    "wandb.log({\"reconstructed_images\": [wandb.Image(image) for image in images]})\n",
    "\n",
    "loss_test = loss_iter_test / len(dataset_test)\n",
    "\n",
    "# Log the test loss to wandb\n",
    "wandb.log({'test_loss': loss_test.item()})\n",
    "\n",
    "print(f\"Test Loss at the end of training: {loss_test.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "for image in dataset_test:\n",
    "    image = image_to_tensor(image)\n",
    "    result = autoencoder(image)\n",
    "    tensor_to_image(image.data).show()\n",
    "    tensor_to_image(result.data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor = image_to_tensor(dataset_test[0])\n",
    "latent = autoencoder.encoder(tensor)\n",
    "latent = (latent - latent.min())/(latent.max() - latent.min())\n",
    "\n",
    "\n",
    "#split latent into 4 channels\n",
    "channels = latent.squeeze(0).split(1)\n",
    "\n",
    "images = [tensor_to_image(channel.unsqueeze(0).detach()) for channel in channels]\n",
    "\n",
    "#write function to make grid using matplotlib in grayscale colorscheme\n",
    "def make_grid_using_matplotlib(images):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    columns = 2\n",
    "    rows = 2\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "\n",
    "        plt.imshow(images[i-1], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "make_grid_using_matplotlib(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
