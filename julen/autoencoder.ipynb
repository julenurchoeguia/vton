{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.new('RGB', (128, 128), color = 'red')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array(img)\n",
    "img_array = img_array/255\n",
    "img_array = img_array.transpose(2, 0, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = torch.from_numpy(img_array)\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image: Image) -> torch.Tensor:\n",
    "    img_array = np.array(image)\n",
    "    img_array = img_array/255\n",
    "    img_array = img_array.transpose(2, 0, 1).astype(np.float32)\n",
    "    img_tensor = torch.from_numpy(img_array)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image:\n",
    "    img_array = tensor.squeeze(0).numpy()\n",
    "    img_array = img_array.transpose(1, 2, 0)\n",
    "    img_array = img_array*255\n",
    "    img_array = img_array.astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = tuple[int, int, int]\n",
    "def create_empty_image(resolution: int, color: color=(0,0,0)) -> Image:\n",
    "    return Image.new('RGB', (resolution, resolution), color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = image_to_tensor(create_empty_image(128, color=(255, 0, 0)))\n",
    "target_tensor = image_to_tensor(create_empty_image(128, color=(0, 255, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/julen/pytorch_test/autoencoder.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     init_tensor \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr\u001b[39m*\u001b[39;49minit_tensor\u001b[39m.\u001b[39;49mgrad\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     init_tensor\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mzero_()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3, 3, 3, padding=1)\n",
    "lr = 0.01\n",
    "min_steps = 100\n",
    "\n",
    "for step in range(min_steps):\n",
    "    y = conv(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        init_tensor -= lr*init_tensor.grad\n",
    "        init_tensor.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int=1, out_features: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "linear = Linear(in_features=3, out_features=3)\n",
    "x = torch.randn(1, 3)\n",
    "target_tensor = torch.randn(1, 3)\n",
    "y = linear(x)\n",
    "loss = ((y - target_tensor)**2).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2304e+00, -5.2909e-01, -6.1829e+00],\n",
       "        [-3.7378e+00,  8.8666e-01,  1.0361e+01],\n",
       "        [-2.3452e-02,  5.5632e-03,  6.5010e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resblock(nn.Module):\n",
    "    def __init__(self, in_channels: int=1, out_channels: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(x)\n",
    "        y = self.conv2(x)\n",
    "        return y+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/julen/pytorch_test/autoencoder.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(min_steps):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     y \u001b[39m=\u001b[39m block(init_tensor)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m (y \u001b[39m-\u001b[39;49m target_tensor)\u001b[39m.\u001b[39mnorm()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/julen/pytorch_test/autoencoder.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "block = resblock(3, 3)\n",
    "\n",
    "lr = 1e-5\n",
    "min_steps = 10000\n",
    "for step in range(min_steps):\n",
    "    y = block(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step: {step}, loss: {loss.item()}')\n",
    "        for param in block.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr*param.grad\n",
    "\n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, output_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        return y\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 128.6528778076172\n",
      "step 1 : loss 128.21038818359375\n",
      "step 2 : loss 127.76188659667969\n",
      "step 3 : loss 127.30679321289062\n",
      "step 4 : loss 126.84478759765625\n",
      "step 5 : loss 126.37632751464844\n",
      "step 6 : loss 125.90122985839844\n",
      "step 7 : loss 125.42005157470703\n",
      "step 8 : loss 124.93059539794922\n",
      "step 9 : loss 124.43502807617188\n",
      "step 10 : loss 123.93255615234375\n",
      "step 11 : loss 123.42205810546875\n",
      "step 12 : loss 122.9047622680664\n",
      "step 13 : loss 122.37893676757812\n",
      "step 14 : loss 121.83979034423828\n",
      "step 15 : loss 121.29861450195312\n",
      "step 16 : loss 120.74919891357422\n",
      "step 17 : loss 120.19261932373047\n",
      "step 18 : loss 119.62739562988281\n",
      "step 19 : loss 119.05462646484375\n",
      "step 20 : loss 118.47195434570312\n",
      "step 21 : loss 117.88108825683594\n",
      "step 22 : loss 117.28244018554688\n",
      "step 23 : loss 116.67326354980469\n",
      "step 24 : loss 116.05571746826172\n",
      "step 25 : loss 115.4289321899414\n",
      "step 26 : loss 114.79264831542969\n",
      "step 27 : loss 114.14610290527344\n",
      "step 28 : loss 113.49009704589844\n",
      "step 29 : loss 112.82311248779297\n",
      "step 30 : loss 112.14706420898438\n",
      "step 31 : loss 111.45945739746094\n",
      "step 32 : loss 110.76154327392578\n",
      "step 33 : loss 110.05264282226562\n",
      "step 34 : loss 109.33202362060547\n",
      "step 35 : loss 108.59356689453125\n",
      "step 36 : loss 107.84986114501953\n",
      "step 37 : loss 107.09502410888672\n",
      "step 38 : loss 106.32749938964844\n",
      "step 39 : loss 105.54853820800781\n",
      "step 40 : loss 104.7568130493164\n",
      "step 41 : loss 103.95157623291016\n",
      "step 42 : loss 103.13362121582031\n",
      "step 43 : loss 102.30205535888672\n",
      "step 44 : loss 101.45672607421875\n",
      "step 45 : loss 100.59754943847656\n",
      "step 46 : loss 99.72352600097656\n",
      "step 47 : loss 98.83531951904297\n",
      "step 48 : loss 97.93225860595703\n",
      "step 49 : loss 97.0132064819336\n",
      "step 50 : loss 96.07886505126953\n",
      "step 51 : loss 95.1281509399414\n",
      "step 52 : loss 94.16124725341797\n",
      "step 53 : loss 93.17729949951172\n",
      "step 54 : loss 92.1759033203125\n",
      "step 55 : loss 91.15656280517578\n",
      "step 56 : loss 90.10955047607422\n",
      "step 57 : loss 89.05341339111328\n",
      "step 58 : loss 87.97832489013672\n",
      "step 59 : loss 86.8835678100586\n",
      "step 60 : loss 85.76830291748047\n",
      "step 61 : loss 84.63184356689453\n",
      "step 62 : loss 83.47002410888672\n",
      "step 63 : loss 82.29059600830078\n",
      "step 64 : loss 81.08902740478516\n",
      "step 65 : loss 79.86420440673828\n",
      "step 66 : loss 78.6158447265625\n",
      "step 67 : loss 77.34282684326172\n",
      "step 68 : loss 76.04470825195312\n",
      "step 69 : loss 74.72045135498047\n",
      "step 70 : loss 73.3695068359375\n",
      "step 71 : loss 71.99124908447266\n",
      "step 72 : loss 70.58399963378906\n",
      "step 73 : loss 69.14176940917969\n",
      "step 74 : loss 67.6749038696289\n",
      "step 75 : loss 66.1768569946289\n",
      "step 76 : loss 64.64688110351562\n",
      "step 77 : loss 63.08351516723633\n",
      "step 78 : loss 61.485435485839844\n",
      "step 79 : loss 59.85185623168945\n",
      "step 80 : loss 58.18140411376953\n",
      "step 81 : loss 56.46913146972656\n",
      "step 82 : loss 54.720970153808594\n",
      "step 83 : loss 52.93168640136719\n",
      "step 84 : loss 51.10032272338867\n",
      "step 85 : loss 49.22503662109375\n",
      "step 86 : loss 47.299835205078125\n",
      "step 87 : loss 45.331058502197266\n",
      "step 88 : loss 43.31260681152344\n",
      "step 89 : loss 41.24523162841797\n",
      "step 90 : loss 39.1226921081543\n",
      "step 91 : loss 36.94887161254883\n",
      "step 92 : loss 34.71923828125\n",
      "step 93 : loss 32.42914581298828\n",
      "step 94 : loss 30.083284378051758\n",
      "step 95 : loss 27.675813674926758\n",
      "step 96 : loss 25.20761489868164\n",
      "step 97 : loss 22.678876876831055\n",
      "step 98 : loss 20.093463897705078\n",
      "step 99 : loss 17.454614639282227\n",
      "step 100 : loss 14.777742385864258\n",
      "step 101 : loss 12.08700942993164\n",
      "step 102 : loss 9.439600944519043\n",
      "step 103 : loss 6.97160530090332\n",
      "step 104 : loss 5.0047101974487305\n",
      "step 105 : loss 4.032219409942627\n",
      "step 106 : loss 3.858414888381958\n",
      "step 107 : loss 3.82747220993042\n",
      "step 108 : loss 3.8061022758483887\n",
      "step 109 : loss 3.7883870601654053\n",
      "step 110 : loss 3.773381233215332\n",
      "step 111 : loss 3.7602016925811768\n",
      "step 112 : loss 3.748246908187866\n",
      "step 113 : loss 3.7370989322662354\n",
      "step 114 : loss 3.7266900539398193\n",
      "step 115 : loss 3.716681718826294\n",
      "step 116 : loss 3.706825017929077\n",
      "step 117 : loss 3.6973509788513184\n",
      "step 118 : loss 3.688006639480591\n",
      "step 119 : loss 3.6786162853240967\n",
      "step 120 : loss 3.669431447982788\n",
      "step 121 : loss 3.6603195667266846\n",
      "step 122 : loss 3.651224136352539\n",
      "step 123 : loss 3.642159938812256\n",
      "step 124 : loss 3.6331238746643066\n",
      "step 125 : loss 3.624105930328369\n",
      "step 126 : loss 3.61513614654541\n",
      "step 127 : loss 3.6061713695526123\n",
      "step 128 : loss 3.5972304344177246\n",
      "step 129 : loss 3.5883281230926514\n",
      "step 130 : loss 3.57942533493042\n",
      "step 131 : loss 3.5705339908599854\n",
      "step 132 : loss 3.5616543292999268\n",
      "step 133 : loss 3.552795171737671\n",
      "step 134 : loss 3.5439581871032715\n",
      "step 135 : loss 3.5351343154907227\n",
      "step 136 : loss 3.526327610015869\n",
      "step 137 : loss 3.517537832260132\n",
      "step 138 : loss 3.508782148361206\n",
      "step 139 : loss 3.5000081062316895\n",
      "step 140 : loss 3.4912869930267334\n",
      "step 141 : loss 3.4825589656829834\n",
      "step 142 : loss 3.473851203918457\n",
      "step 143 : loss 3.465160608291626\n",
      "step 144 : loss 3.456482172012329\n",
      "step 145 : loss 3.4478249549865723\n",
      "step 146 : loss 3.439180612564087\n",
      "step 147 : loss 3.4305388927459717\n",
      "step 148 : loss 3.4219510555267334\n",
      "step 149 : loss 3.4133453369140625\n",
      "step 150 : loss 3.4047744274139404\n",
      "step 151 : loss 3.396210193634033\n",
      "step 152 : loss 3.3876631259918213\n",
      "step 153 : loss 3.3791329860687256\n",
      "step 154 : loss 3.370628833770752\n",
      "step 155 : loss 3.3621182441711426\n",
      "step 156 : loss 3.3536407947540283\n",
      "step 157 : loss 3.3451762199401855\n",
      "step 158 : loss 3.3367347717285156\n",
      "step 159 : loss 3.3282833099365234\n",
      "step 160 : loss 3.319864511489868\n",
      "step 161 : loss 3.3114686012268066\n",
      "step 162 : loss 3.30307674407959\n",
      "step 163 : loss 3.2947046756744385\n",
      "step 164 : loss 3.286341428756714\n",
      "step 165 : loss 3.2780094146728516\n",
      "step 166 : loss 3.2696805000305176\n",
      "step 167 : loss 3.2613718509674072\n",
      "step 168 : loss 3.2530758380889893\n",
      "step 169 : loss 3.244798183441162\n",
      "step 170 : loss 3.236531972885132\n",
      "step 171 : loss 3.228276014328003\n",
      "step 172 : loss 3.2200491428375244\n",
      "step 173 : loss 3.2118301391601562\n",
      "step 174 : loss 3.2036266326904297\n",
      "step 175 : loss 3.1954538822174072\n",
      "step 176 : loss 3.187269687652588\n",
      "step 177 : loss 3.179110527038574\n",
      "step 178 : loss 3.1709694862365723\n",
      "step 179 : loss 3.162851572036743\n",
      "step 180 : loss 3.1547372341156006\n",
      "step 181 : loss 3.146638870239258\n",
      "step 182 : loss 3.1385600566864014\n",
      "step 183 : loss 3.1304819583892822\n",
      "step 184 : loss 3.1224400997161865\n",
      "step 185 : loss 3.1144039630889893\n",
      "step 186 : loss 3.106386423110962\n",
      "step 187 : loss 3.0983738899230957\n",
      "step 188 : loss 3.0903775691986084\n",
      "step 189 : loss 3.082409620285034\n",
      "step 190 : loss 3.0744409561157227\n",
      "step 191 : loss 3.066494941711426\n",
      "step 192 : loss 3.0585553646087646\n",
      "step 193 : loss 3.050640821456909\n",
      "step 194 : loss 3.0427401065826416\n",
      "step 195 : loss 3.03485107421875\n",
      "step 196 : loss 3.026977300643921\n",
      "step 197 : loss 3.019116163253784\n",
      "step 198 : loss 3.01127290725708\n",
      "step 199 : loss 3.003441333770752\n",
      "step 200 : loss 2.9956233501434326\n",
      "step 201 : loss 2.98783278465271\n",
      "step 202 : loss 2.9800491333007812\n",
      "step 203 : loss 2.9722795486450195\n",
      "step 204 : loss 2.964510917663574\n",
      "step 205 : loss 2.956761360168457\n",
      "step 206 : loss 2.9490392208099365\n",
      "step 207 : loss 2.9413230419158936\n",
      "step 208 : loss 2.933628559112549\n",
      "step 209 : loss 2.925940752029419\n",
      "step 210 : loss 2.918274164199829\n",
      "step 211 : loss 2.91062068939209\n",
      "step 212 : loss 2.9029717445373535\n",
      "step 213 : loss 2.8953473567962646\n",
      "step 214 : loss 2.8877289295196533\n",
      "step 215 : loss 2.880129337310791\n",
      "step 216 : loss 2.872540235519409\n",
      "step 217 : loss 2.8649790287017822\n",
      "step 218 : loss 2.857415199279785\n",
      "step 219 : loss 2.849865674972534\n",
      "step 220 : loss 2.8423428535461426\n",
      "step 221 : loss 2.834826946258545\n",
      "step 222 : loss 2.8273305892944336\n",
      "step 223 : loss 2.8198330402374268\n",
      "step 224 : loss 2.8123626708984375\n",
      "step 225 : loss 2.8049087524414062\n",
      "step 226 : loss 2.7974531650543213\n",
      "step 227 : loss 2.790022611618042\n",
      "step 228 : loss 2.782609224319458\n",
      "step 229 : loss 2.775193452835083\n",
      "step 230 : loss 2.7678120136260986\n",
      "step 231 : loss 2.7604379653930664\n",
      "step 232 : loss 2.7530758380889893\n",
      "step 233 : loss 2.745718479156494\n",
      "step 234 : loss 2.7383861541748047\n",
      "step 235 : loss 2.7310566902160645\n",
      "step 236 : loss 2.7237491607666016\n",
      "step 237 : loss 2.716458320617676\n",
      "step 238 : loss 2.709174871444702\n",
      "step 239 : loss 2.7019097805023193\n",
      "step 240 : loss 2.69463849067688\n",
      "step 241 : loss 2.687411308288574\n",
      "step 242 : loss 2.680173873901367\n",
      "step 243 : loss 2.672964572906494\n",
      "step 244 : loss 2.66576886177063\n",
      "step 245 : loss 2.6585679054260254\n",
      "step 246 : loss 2.6514062881469727\n",
      "step 247 : loss 2.644249200820923\n",
      "step 248 : loss 2.6370935440063477\n",
      "step 249 : loss 2.6299591064453125\n",
      "step 250 : loss 2.6228368282318115\n",
      "step 251 : loss 2.615732192993164\n",
      "step 252 : loss 2.6086366176605225\n",
      "step 253 : loss 2.6015608310699463\n",
      "step 254 : loss 2.5944840908050537\n",
      "step 255 : loss 2.587437391281128\n",
      "step 256 : loss 2.580399751663208\n",
      "step 257 : loss 2.5733697414398193\n",
      "step 258 : loss 2.566361665725708\n",
      "step 259 : loss 2.559344530105591\n",
      "step 260 : loss 2.5523600578308105\n",
      "step 261 : loss 2.5453941822052\n",
      "step 262 : loss 2.5384342670440674\n",
      "step 263 : loss 2.5314691066741943\n",
      "step 264 : loss 2.524538516998291\n",
      "step 265 : loss 2.517617702484131\n",
      "step 266 : loss 2.510713577270508\n",
      "step 267 : loss 2.503814220428467\n",
      "step 268 : loss 2.4969234466552734\n",
      "step 269 : loss 2.4900619983673096\n",
      "step 270 : loss 2.4832119941711426\n",
      "step 271 : loss 2.4763643741607666\n",
      "step 272 : loss 2.469529390335083\n",
      "step 273 : loss 2.4627091884613037\n",
      "step 274 : loss 2.455900192260742\n",
      "step 275 : loss 2.449096918106079\n",
      "step 276 : loss 2.442319869995117\n",
      "step 277 : loss 2.435547113418579\n",
      "step 278 : loss 2.4287896156311035\n",
      "step 279 : loss 2.4220457077026367\n",
      "step 280 : loss 2.4153144359588623\n",
      "step 281 : loss 2.4085912704467773\n",
      "step 282 : loss 2.4018824100494385\n",
      "step 283 : loss 2.395184278488159\n",
      "step 284 : loss 2.3885021209716797\n",
      "step 285 : loss 2.381838083267212\n",
      "step 286 : loss 2.3751778602600098\n",
      "step 287 : loss 2.36852765083313\n",
      "step 288 : loss 2.361893892288208\n",
      "step 289 : loss 2.355274200439453\n",
      "step 290 : loss 2.348674774169922\n",
      "step 291 : loss 2.3420770168304443\n",
      "step 292 : loss 2.3354930877685547\n",
      "step 293 : loss 2.3289239406585693\n",
      "step 294 : loss 2.3223586082458496\n",
      "step 295 : loss 2.315814971923828\n",
      "step 296 : loss 2.3092873096466064\n",
      "step 297 : loss 2.3027563095092773\n",
      "step 298 : loss 2.2962517738342285\n",
      "step 299 : loss 2.2897584438323975\n",
      "step 300 : loss 2.283266067504883\n",
      "step 301 : loss 2.2767930030822754\n",
      "step 302 : loss 2.2703306674957275\n",
      "step 303 : loss 2.2638871669769287\n",
      "step 304 : loss 2.2574539184570312\n",
      "step 305 : loss 2.251032829284668\n",
      "step 306 : loss 2.2446107864379883\n",
      "step 307 : loss 2.238210678100586\n",
      "step 308 : loss 2.231826066970825\n",
      "step 309 : loss 2.2254550457000732\n",
      "step 310 : loss 2.219085454940796\n",
      "step 311 : loss 2.212733745574951\n",
      "step 312 : loss 2.206387758255005\n",
      "step 313 : loss 2.2000553607940674\n",
      "step 314 : loss 2.1937365531921387\n",
      "step 315 : loss 2.1874332427978516\n",
      "step 316 : loss 2.1811399459838867\n",
      "step 317 : loss 2.17486572265625\n",
      "step 318 : loss 2.1685917377471924\n",
      "step 319 : loss 2.162327289581299\n",
      "step 320 : loss 2.1560912132263184\n",
      "step 321 : loss 2.149860382080078\n",
      "step 322 : loss 2.143632411956787\n",
      "step 323 : loss 2.137423038482666\n",
      "step 324 : loss 2.1312146186828613\n",
      "step 325 : loss 2.125032901763916\n",
      "step 326 : loss 2.118859052658081\n",
      "step 327 : loss 2.112684726715088\n",
      "step 328 : loss 2.1065335273742676\n",
      "step 329 : loss 2.100391149520874\n",
      "step 330 : loss 2.094264268875122\n",
      "step 331 : loss 2.088144063949585\n",
      "step 332 : loss 2.0820350646972656\n",
      "step 333 : loss 2.0759329795837402\n",
      "step 334 : loss 2.06984806060791\n",
      "step 335 : loss 2.0637762546539307\n",
      "step 336 : loss 2.057717800140381\n",
      "step 337 : loss 2.051666259765625\n",
      "step 338 : loss 2.045624017715454\n",
      "step 339 : loss 2.039606809616089\n",
      "step 340 : loss 2.0335845947265625\n",
      "step 341 : loss 2.0275754928588867\n",
      "step 342 : loss 2.021580934524536\n",
      "step 343 : loss 2.0155911445617676\n",
      "step 344 : loss 2.009629249572754\n",
      "step 345 : loss 2.003664970397949\n",
      "step 346 : loss 1.9977139234542847\n",
      "step 347 : loss 1.991773009300232\n",
      "step 348 : loss 1.985849380493164\n",
      "step 349 : loss 1.9799320697784424\n",
      "step 350 : loss 1.9740298986434937\n",
      "step 351 : loss 1.968127727508545\n",
      "step 352 : loss 1.9622408151626587\n",
      "step 353 : loss 1.956370234489441\n",
      "step 354 : loss 1.9505060911178589\n",
      "step 355 : loss 1.9446613788604736\n",
      "step 356 : loss 1.9388256072998047\n",
      "step 357 : loss 1.9329979419708252\n",
      "step 358 : loss 1.9271844625473022\n",
      "step 359 : loss 1.921378254890442\n",
      "step 360 : loss 1.9155826568603516\n",
      "step 361 : loss 1.9097933769226074\n",
      "step 362 : loss 1.9040213823318481\n",
      "step 363 : loss 1.8982553482055664\n",
      "step 364 : loss 1.8924959897994995\n",
      "step 365 : loss 1.8867526054382324\n",
      "step 366 : loss 1.8810323476791382\n",
      "step 367 : loss 1.8753025531768799\n",
      "step 368 : loss 1.869598150253296\n",
      "step 369 : loss 1.8638995885849\n",
      "step 370 : loss 1.8582077026367188\n",
      "step 371 : loss 1.8525367975234985\n",
      "step 372 : loss 1.846865177154541\n",
      "step 373 : loss 1.841207504272461\n",
      "step 374 : loss 1.8355729579925537\n",
      "step 375 : loss 1.8299381732940674\n",
      "step 376 : loss 1.824310541152954\n",
      "step 377 : loss 1.8187021017074585\n",
      "step 378 : loss 1.813098669052124\n",
      "step 379 : loss 1.8075048923492432\n",
      "step 380 : loss 1.8019253015518188\n",
      "step 381 : loss 1.7963539361953735\n",
      "step 382 : loss 1.7907873392105103\n",
      "step 383 : loss 1.7852295637130737\n",
      "step 384 : loss 1.7797091007232666\n",
      "step 385 : loss 1.7741732597351074\n",
      "step 386 : loss 1.7686623334884644\n",
      "step 387 : loss 1.7631597518920898\n",
      "step 388 : loss 1.7576631307601929\n",
      "step 389 : loss 1.7521696090698242\n",
      "step 390 : loss 1.7466908693313599\n",
      "step 391 : loss 1.741222620010376\n",
      "step 392 : loss 1.7357654571533203\n",
      "step 393 : loss 1.730320692062378\n",
      "step 394 : loss 1.7248775959014893\n",
      "step 395 : loss 1.7194525003433228\n",
      "step 396 : loss 1.7140402793884277\n",
      "step 397 : loss 1.708633303642273\n",
      "step 398 : loss 1.703235387802124\n",
      "step 399 : loss 1.6978462934494019\n",
      "step 400 : loss 1.6924749612808228\n",
      "step 401 : loss 1.687114953994751\n",
      "step 402 : loss 1.6817514896392822\n",
      "step 403 : loss 1.67640221118927\n",
      "step 404 : loss 1.6710703372955322\n",
      "step 405 : loss 1.6657452583312988\n",
      "step 406 : loss 1.6604251861572266\n",
      "step 407 : loss 1.6551282405853271\n",
      "step 408 : loss 1.6498275995254517\n",
      "step 409 : loss 1.6445471048355103\n",
      "step 410 : loss 1.6392614841461182\n",
      "step 411 : loss 1.634004831314087\n",
      "step 412 : loss 1.6287472248077393\n",
      "step 413 : loss 1.623497486114502\n",
      "step 414 : loss 1.618260145187378\n",
      "step 415 : loss 1.6130329370498657\n",
      "step 416 : loss 1.6078189611434937\n",
      "step 417 : loss 1.6026158332824707\n",
      "step 418 : loss 1.597425103187561\n",
      "step 419 : loss 1.5922363996505737\n",
      "step 420 : loss 1.5870518684387207\n",
      "step 421 : loss 1.5818860530853271\n",
      "step 422 : loss 1.5767297744750977\n",
      "step 423 : loss 1.5715910196304321\n",
      "step 424 : loss 1.566440224647522\n",
      "step 425 : loss 1.5613139867782593\n",
      "step 426 : loss 1.556190848350525\n",
      "step 427 : loss 1.5510811805725098\n",
      "step 428 : loss 1.545984148979187\n",
      "step 429 : loss 1.5408855676651\n",
      "step 430 : loss 1.5358128547668457\n",
      "step 431 : loss 1.5307378768920898\n",
      "step 432 : loss 1.5256750583648682\n",
      "step 433 : loss 1.5206283330917358\n",
      "step 434 : loss 1.515580177307129\n",
      "step 435 : loss 1.5105491876602173\n",
      "step 436 : loss 1.5055279731750488\n",
      "step 437 : loss 1.500503659248352\n",
      "step 438 : loss 1.4955016374588013\n",
      "step 439 : loss 1.4905129671096802\n",
      "step 440 : loss 1.4855152368545532\n",
      "step 441 : loss 1.4805474281311035\n",
      "step 442 : loss 1.4755736589431763\n",
      "step 443 : loss 1.4706156253814697\n",
      "step 444 : loss 1.4656686782836914\n",
      "step 445 : loss 1.4607313871383667\n",
      "step 446 : loss 1.4557982683181763\n",
      "step 447 : loss 1.4508756399154663\n",
      "step 448 : loss 1.4459691047668457\n",
      "step 449 : loss 1.441070318222046\n",
      "step 450 : loss 1.4361768960952759\n",
      "step 451 : loss 1.4312876462936401\n",
      "step 452 : loss 1.4264130592346191\n",
      "step 453 : loss 1.42154061794281\n",
      "step 454 : loss 1.4166860580444336\n",
      "step 455 : loss 1.4118369817733765\n",
      "step 456 : loss 1.4069980382919312\n",
      "step 457 : loss 1.4021717309951782\n",
      "step 458 : loss 1.3973548412322998\n",
      "step 459 : loss 1.3925353288650513\n",
      "step 460 : loss 1.3877356052398682\n",
      "step 461 : loss 1.3829394578933716\n",
      "step 462 : loss 1.3781598806381226\n",
      "step 463 : loss 1.3733789920806885\n",
      "step 464 : loss 1.3686165809631348\n",
      "step 465 : loss 1.363856554031372\n",
      "step 466 : loss 1.3590987920761108\n",
      "step 467 : loss 1.3543709516525269\n",
      "step 468 : loss 1.3496379852294922\n",
      "step 469 : loss 1.344919204711914\n",
      "step 470 : loss 1.3402036428451538\n",
      "step 471 : loss 1.3354982137680054\n",
      "step 472 : loss 1.330801248550415\n",
      "step 473 : loss 1.326109766960144\n",
      "step 474 : loss 1.3214441537857056\n",
      "step 475 : loss 1.316766381263733\n",
      "step 476 : loss 1.3121076822280884\n",
      "step 477 : loss 1.3074605464935303\n",
      "step 478 : loss 1.3028132915496826\n",
      "step 479 : loss 1.298173189163208\n",
      "step 480 : loss 1.2935504913330078\n",
      "step 481 : loss 1.2889357805252075\n",
      "step 482 : loss 1.284327507019043\n",
      "step 483 : loss 1.2797303199768066\n",
      "step 484 : loss 1.2751469612121582\n",
      "step 485 : loss 1.2705625295639038\n",
      "step 486 : loss 1.2659838199615479\n",
      "step 487 : loss 1.2614190578460693\n",
      "step 488 : loss 1.2568631172180176\n",
      "step 489 : loss 1.2523161172866821\n",
      "step 490 : loss 1.2477796077728271\n",
      "step 491 : loss 1.24324369430542\n",
      "step 492 : loss 1.2387226819992065\n",
      "step 493 : loss 1.2342067956924438\n",
      "step 494 : loss 1.229703664779663\n",
      "step 495 : loss 1.225202202796936\n",
      "step 496 : loss 1.2207181453704834\n",
      "step 497 : loss 1.2162350416183472\n",
      "step 498 : loss 1.2117637395858765\n",
      "step 499 : loss 1.2073043584823608\n",
      "step 500 : loss 1.2028452157974243\n",
      "step 501 : loss 1.1983975172042847\n",
      "step 502 : loss 1.1939553022384644\n",
      "step 503 : loss 1.189527153968811\n",
      "step 504 : loss 1.185104489326477\n",
      "step 505 : loss 1.1806923151016235\n",
      "step 506 : loss 1.1762856245040894\n",
      "step 507 : loss 1.171891212463379\n",
      "step 508 : loss 1.1674989461898804\n",
      "step 509 : loss 1.1631237268447876\n",
      "step 510 : loss 1.1587510108947754\n",
      "step 511 : loss 1.154384970664978\n",
      "step 512 : loss 1.1500245332717896\n",
      "step 513 : loss 1.1456806659698486\n",
      "step 514 : loss 1.1413389444351196\n",
      "step 515 : loss 1.1370071172714233\n",
      "step 516 : loss 1.1326864957809448\n",
      "step 517 : loss 1.1283735036849976\n",
      "step 518 : loss 1.124067783355713\n",
      "step 519 : loss 1.119769811630249\n",
      "step 520 : loss 1.1154754161834717\n",
      "step 521 : loss 1.111189603805542\n",
      "step 522 : loss 1.1069202423095703\n",
      "step 523 : loss 1.1026452779769897\n",
      "step 524 : loss 1.0983836650848389\n",
      "step 525 : loss 1.0941356420516968\n",
      "step 526 : loss 1.0898948907852173\n",
      "step 527 : loss 1.0856587886810303\n",
      "step 528 : loss 1.081432819366455\n",
      "step 529 : loss 1.077211856842041\n",
      "step 530 : loss 1.0730000734329224\n",
      "step 531 : loss 1.068793535232544\n",
      "step 532 : loss 1.0646001100540161\n",
      "step 533 : loss 1.0604108572006226\n",
      "step 534 : loss 1.0562353134155273\n",
      "step 535 : loss 1.0520583391189575\n",
      "step 536 : loss 1.0478910207748413\n",
      "step 537 : loss 1.0437390804290771\n",
      "step 538 : loss 1.0395878553390503\n",
      "step 539 : loss 1.0354481935501099\n",
      "step 540 : loss 1.0313217639923096\n",
      "step 541 : loss 1.0271928310394287\n",
      "step 542 : loss 1.0230839252471924\n",
      "step 543 : loss 1.0189845561981201\n",
      "step 544 : loss 1.0148988962173462\n",
      "step 545 : loss 1.010832667350769\n",
      "step 546 : loss 1.0067957639694214\n",
      "step 547 : loss 1.0027992725372314\n",
      "step 548 : loss 0.9988873600959778\n",
      "step 549 : loss 0.9951019883155823\n",
      "step 550 : loss 0.9915550351142883\n",
      "step 551 : loss 0.9884337782859802\n",
      "step 552 : loss 0.9861090183258057\n",
      "step 553 : loss 0.9852467775344849\n",
      "step 554 : loss 0.9870471358299255\n",
      "step 555 : loss 0.9935027956962585\n",
      "step 556 : loss 1.0072177648544312\n",
      "step 557 : loss 1.0309584140777588\n",
      "step 558 : loss 1.0628812313079834\n",
      "step 559 : loss 1.0978665351867676\n",
      "step 560 : loss 1.1230539083480835\n",
      "step 561 : loss 1.1404229402542114\n",
      "step 562 : loss 1.144850730895996\n",
      "step 563 : loss 1.1501154899597168\n",
      "step 564 : loss 1.1478679180145264\n",
      "step 565 : loss 1.1508115530014038\n",
      "step 566 : loss 1.1473617553710938\n",
      "step 567 : loss 1.1501185894012451\n",
      "step 568 : loss 1.1464024782180786\n",
      "step 569 : loss 1.1492700576782227\n",
      "step 570 : loss 1.1454020738601685\n",
      "step 571 : loss 1.1484099626541138\n",
      "step 572 : loss 1.1443674564361572\n",
      "step 573 : loss 1.1475296020507812\n",
      "step 574 : loss 1.143354892730713\n",
      "step 575 : loss 1.1466811895370483\n",
      "step 576 : loss 1.1423566341400146\n",
      "step 577 : loss 1.1458386182785034\n",
      "step 578 : loss 1.1413192749023438\n",
      "step 579 : loss 1.1449824571609497\n",
      "step 580 : loss 1.1403635740280151\n",
      "step 581 : loss 1.1441409587860107\n",
      "step 582 : loss 1.1393245458602905\n",
      "step 583 : loss 1.1432745456695557\n",
      "step 584 : loss 1.1383100748062134\n",
      "step 585 : loss 1.1424509286880493\n",
      "step 586 : loss 1.1373519897460938\n",
      "step 587 : loss 1.1416348218917847\n",
      "step 588 : loss 1.1363554000854492\n",
      "step 589 : loss 1.1408101320266724\n",
      "step 590 : loss 1.13535475730896\n",
      "step 591 : loss 1.1399939060211182\n",
      "step 592 : loss 1.1343598365783691\n",
      "step 593 : loss 1.1391918659210205\n",
      "step 594 : loss 1.1334187984466553\n",
      "step 595 : loss 1.1383863687515259\n",
      "step 596 : loss 1.1324243545532227\n",
      "step 597 : loss 1.1375852823257446\n",
      "step 598 : loss 1.1314488649368286\n",
      "step 599 : loss 1.136805534362793\n",
      "step 600 : loss 1.130489468574524\n",
      "step 601 : loss 1.1360186338424683\n",
      "step 602 : loss 1.1295396089553833\n",
      "step 603 : loss 1.1352514028549194\n",
      "step 604 : loss 1.1285679340362549\n",
      "step 605 : loss 1.134462594985962\n",
      "step 606 : loss 1.127587914466858\n",
      "step 607 : loss 1.1337010860443115\n",
      "step 608 : loss 1.1266571283340454\n",
      "step 609 : loss 1.1329573392868042\n",
      "step 610 : loss 1.1257506608963013\n",
      "step 611 : loss 1.1322144269943237\n",
      "step 612 : loss 1.1247981786727905\n",
      "step 613 : loss 1.1314724683761597\n",
      "step 614 : loss 1.12386953830719\n",
      "step 615 : loss 1.1307144165039062\n",
      "step 616 : loss 1.1229078769683838\n",
      "step 617 : loss 1.1299954652786255\n",
      "step 618 : loss 1.122007966041565\n",
      "step 619 : loss 1.12930166721344\n",
      "step 620 : loss 1.1211186647415161\n",
      "step 621 : loss 1.1285806894302368\n",
      "step 622 : loss 1.1201897859573364\n",
      "step 623 : loss 1.1278866529464722\n",
      "step 624 : loss 1.1192814111709595\n",
      "step 625 : loss 1.1271721124649048\n",
      "step 626 : loss 1.118394374847412\n",
      "step 627 : loss 1.1264711618423462\n",
      "step 628 : loss 1.1174638271331787\n",
      "step 629 : loss 1.1258031129837036\n",
      "step 630 : loss 1.116589903831482\n",
      "step 631 : loss 1.125112533569336\n",
      "step 632 : loss 1.1157124042510986\n",
      "step 633 : loss 1.1244629621505737\n",
      "step 634 : loss 1.1148078441619873\n",
      "step 635 : loss 1.1237727403640747\n",
      "step 636 : loss 1.1139516830444336\n",
      "step 637 : loss 1.1231372356414795\n",
      "step 638 : loss 1.1130894422531128\n",
      "step 639 : loss 1.1224735975265503\n",
      "step 640 : loss 1.1122044324874878\n",
      "step 641 : loss 1.121838927268982\n",
      "step 642 : loss 1.111352801322937\n",
      "step 643 : loss 1.1211949586868286\n",
      "step 644 : loss 1.1104705333709717\n",
      "step 645 : loss 1.120546817779541\n",
      "step 646 : loss 1.109611988067627\n",
      "step 647 : loss 1.1199440956115723\n",
      "step 648 : loss 1.1087599992752075\n",
      "step 649 : loss 1.1193287372589111\n",
      "step 650 : loss 1.1079221963882446\n",
      "step 651 : loss 1.1186953783035278\n",
      "step 652 : loss 1.1070505380630493\n",
      "step 653 : loss 1.1181014776229858\n",
      "step 654 : loss 1.1062211990356445\n",
      "step 655 : loss 1.1175041198730469\n",
      "step 656 : loss 1.1054044961929321\n",
      "step 657 : loss 1.1169325113296509\n",
      "step 658 : loss 1.1045912504196167\n",
      "step 659 : loss 1.1163413524627686\n",
      "step 660 : loss 1.103755235671997\n",
      "step 661 : loss 1.1157475709915161\n",
      "step 662 : loss 1.1029282808303833\n",
      "step 663 : loss 1.1151847839355469\n",
      "step 664 : loss 1.1021008491516113\n",
      "step 665 : loss 1.1146118640899658\n",
      "step 666 : loss 1.1012808084487915\n",
      "step 667 : loss 1.1140271425247192\n",
      "step 668 : loss 1.1004722118377686\n",
      "step 669 : loss 1.113494873046875\n",
      "step 670 : loss 1.0996538400650024\n",
      "step 671 : loss 1.11294686794281\n",
      "step 672 : loss 1.0988603830337524\n",
      "step 673 : loss 1.1124223470687866\n",
      "step 674 : loss 1.0980873107910156\n",
      "step 675 : loss 1.1118674278259277\n",
      "step 676 : loss 1.0972644090652466\n",
      "step 677 : loss 1.111363172531128\n",
      "step 678 : loss 1.096454381942749\n",
      "step 679 : loss 1.110818862915039\n",
      "step 680 : loss 1.0957099199295044\n",
      "step 681 : loss 1.1103168725967407\n",
      "step 682 : loss 1.0949151515960693\n",
      "step 683 : loss 1.1097986698150635\n",
      "step 684 : loss 1.0941364765167236\n",
      "step 685 : loss 1.1093127727508545\n",
      "step 686 : loss 1.0933481454849243\n",
      "step 687 : loss 1.1087900400161743\n",
      "step 688 : loss 1.0925649404525757\n",
      "step 689 : loss 1.10829758644104\n",
      "step 690 : loss 1.0918262004852295\n",
      "step 691 : loss 1.107844591140747\n",
      "step 692 : loss 1.0910438299179077\n",
      "step 693 : loss 1.1073546409606934\n",
      "step 694 : loss 1.0902818441390991\n",
      "step 695 : loss 1.1068572998046875\n",
      "step 696 : loss 1.0895329713821411\n",
      "step 697 : loss 1.1064115762710571\n",
      "step 698 : loss 1.0887634754180908\n",
      "step 699 : loss 1.10593581199646\n",
      "step 700 : loss 1.0880063772201538\n",
      "step 701 : loss 1.105480670928955\n",
      "step 702 : loss 1.0872594118118286\n",
      "step 703 : loss 1.1050132513046265\n",
      "step 704 : loss 1.086505651473999\n",
      "step 705 : loss 1.1045938730239868\n",
      "step 706 : loss 1.0857807397842407\n",
      "step 707 : loss 1.1041626930236816\n",
      "step 708 : loss 1.08499014377594\n",
      "step 709 : loss 1.1036961078643799\n",
      "step 710 : loss 1.084308385848999\n",
      "step 711 : loss 1.1032772064208984\n",
      "step 712 : loss 1.0835459232330322\n",
      "step 713 : loss 1.1028438806533813\n",
      "step 714 : loss 1.0828043222427368\n",
      "step 715 : loss 1.1024028062820435\n",
      "step 716 : loss 1.0820757150650024\n",
      "step 717 : loss 1.101994276046753\n",
      "step 718 : loss 1.0813313722610474\n",
      "step 719 : loss 1.101600170135498\n",
      "step 720 : loss 1.0806443691253662\n",
      "step 721 : loss 1.101213812828064\n",
      "step 722 : loss 1.0799022912979126\n",
      "step 723 : loss 1.100817322731018\n",
      "step 724 : loss 1.0791795253753662\n",
      "step 725 : loss 1.1004046201705933\n",
      "step 726 : loss 1.0784770250320435\n",
      "step 727 : loss 1.1000494956970215\n",
      "step 728 : loss 1.077744960784912\n",
      "step 729 : loss 1.099661946296692\n",
      "step 730 : loss 1.0770317316055298\n",
      "step 731 : loss 1.0992728471755981\n",
      "step 732 : loss 1.076322317123413\n",
      "step 733 : loss 1.0989060401916504\n",
      "step 734 : loss 1.0756123065948486\n",
      "step 735 : loss 1.0985466241836548\n",
      "step 736 : loss 1.0749136209487915\n",
      "step 737 : loss 1.0981839895248413\n",
      "step 738 : loss 1.0742061138153076\n",
      "step 739 : loss 1.0978082418441772\n",
      "step 740 : loss 1.0734976530075073\n",
      "step 741 : loss 1.0974745750427246\n",
      "step 742 : loss 1.0727989673614502\n",
      "step 743 : loss 1.0971399545669556\n",
      "step 744 : loss 1.0720889568328857\n",
      "step 745 : loss 1.0967954397201538\n",
      "step 746 : loss 1.071426272392273\n",
      "step 747 : loss 1.096450686454773\n",
      "step 748 : loss 1.0707030296325684\n",
      "step 749 : loss 1.0961272716522217\n",
      "step 750 : loss 1.0700076818466187\n",
      "step 751 : loss 1.0958020687103271\n",
      "step 752 : loss 1.069326639175415\n",
      "step 753 : loss 1.0954904556274414\n",
      "step 754 : loss 1.0686156749725342\n",
      "step 755 : loss 1.0951757431030273\n",
      "step 756 : loss 1.0679198503494263\n",
      "step 757 : loss 1.0948615074157715\n",
      "step 758 : loss 1.067259430885315\n",
      "step 759 : loss 1.094563603401184\n",
      "step 760 : loss 1.0665819644927979\n",
      "step 761 : loss 1.0942386388778687\n",
      "step 762 : loss 1.0658799409866333\n",
      "step 763 : loss 1.0939637422561646\n",
      "step 764 : loss 1.0652308464050293\n",
      "step 765 : loss 1.0936665534973145\n",
      "step 766 : loss 1.0645267963409424\n",
      "step 767 : loss 1.0933618545532227\n",
      "step 768 : loss 1.0638604164123535\n",
      "step 769 : loss 1.093105673789978\n",
      "step 770 : loss 1.0631636381149292\n",
      "step 771 : loss 1.092817783355713\n",
      "step 772 : loss 1.0625121593475342\n",
      "step 773 : loss 1.0925554037094116\n",
      "step 774 : loss 1.0618231296539307\n",
      "step 775 : loss 1.0922845602035522\n",
      "step 776 : loss 1.0611774921417236\n",
      "step 777 : loss 1.0920157432556152\n",
      "step 778 : loss 1.0604654550552368\n",
      "step 779 : loss 1.0917741060256958\n",
      "step 780 : loss 1.0598053932189941\n",
      "step 781 : loss 1.091528058052063\n",
      "step 782 : loss 1.0591543912887573\n",
      "step 783 : loss 1.0912625789642334\n",
      "step 784 : loss 1.0584841966629028\n",
      "step 785 : loss 1.0909831523895264\n",
      "step 786 : loss 1.057845950126648\n",
      "step 787 : loss 1.0907611846923828\n",
      "step 788 : loss 1.057144284248352\n",
      "step 789 : loss 1.0905516147613525\n",
      "step 790 : loss 1.0565013885498047\n",
      "step 791 : loss 1.0902864933013916\n",
      "step 792 : loss 1.0558242797851562\n",
      "step 793 : loss 1.0900942087173462\n",
      "step 794 : loss 1.0551652908325195\n",
      "step 795 : loss 1.0898494720458984\n",
      "step 796 : loss 1.0545234680175781\n",
      "step 797 : loss 1.0896236896514893\n",
      "step 798 : loss 1.0538628101348877\n",
      "step 799 : loss 1.0894148349761963\n",
      "step 800 : loss 1.0531924962997437\n",
      "step 801 : loss 1.0892257690429688\n",
      "step 802 : loss 1.052546739578247\n",
      "step 803 : loss 1.0890107154846191\n",
      "step 804 : loss 1.0519124269485474\n",
      "step 805 : loss 1.0887868404388428\n",
      "step 806 : loss 1.0512659549713135\n",
      "step 807 : loss 1.088592290878296\n",
      "step 808 : loss 1.050634741783142\n",
      "step 809 : loss 1.0883945226669312\n",
      "step 810 : loss 1.0499451160430908\n",
      "step 811 : loss 1.088205337524414\n",
      "step 812 : loss 1.0492796897888184\n",
      "step 813 : loss 1.0880330801010132\n",
      "step 814 : loss 1.048652172088623\n",
      "step 815 : loss 1.0878490209579468\n",
      "step 816 : loss 1.0480073690414429\n",
      "step 817 : loss 1.087642788887024\n",
      "step 818 : loss 1.047362208366394\n",
      "step 819 : loss 1.0874890089035034\n",
      "step 820 : loss 1.0467041730880737\n",
      "step 821 : loss 1.0872920751571655\n",
      "step 822 : loss 1.0460542440414429\n",
      "step 823 : loss 1.087131142616272\n",
      "step 824 : loss 1.045406460762024\n",
      "step 825 : loss 1.0869884490966797\n",
      "step 826 : loss 1.0447508096694946\n",
      "step 827 : loss 1.0868364572525024\n",
      "step 828 : loss 1.0441343784332275\n",
      "step 829 : loss 1.0866926908493042\n",
      "step 830 : loss 1.0434727668762207\n",
      "step 831 : loss 1.086545705795288\n",
      "step 832 : loss 1.042830467224121\n",
      "step 833 : loss 1.086413025856018\n",
      "step 834 : loss 1.0421720743179321\n",
      "step 835 : loss 1.0862786769866943\n",
      "step 836 : loss 1.0415440797805786\n",
      "step 837 : loss 1.086129903793335\n",
      "step 838 : loss 1.0409096479415894\n",
      "step 839 : loss 1.0859853029251099\n",
      "step 840 : loss 1.0402647256851196\n",
      "step 841 : loss 1.0858443975448608\n",
      "step 842 : loss 1.039637565612793\n",
      "step 843 : loss 1.0857338905334473\n",
      "step 844 : loss 1.0389831066131592\n",
      "step 845 : loss 1.0855979919433594\n",
      "step 846 : loss 1.038339376449585\n",
      "step 847 : loss 1.085503101348877\n",
      "step 848 : loss 1.0376988649368286\n",
      "step 849 : loss 1.0853803157806396\n",
      "step 850 : loss 1.0370588302612305\n",
      "step 851 : loss 1.0852633714675903\n",
      "step 852 : loss 1.0364363193511963\n",
      "step 853 : loss 1.0851597785949707\n",
      "step 854 : loss 1.035762906074524\n",
      "step 855 : loss 1.0850579738616943\n",
      "step 856 : loss 1.0351297855377197\n",
      "step 857 : loss 1.0849695205688477\n",
      "step 858 : loss 1.0344947576522827\n",
      "step 859 : loss 1.084869384765625\n",
      "step 860 : loss 1.0338540077209473\n",
      "step 861 : loss 1.0847922563552856\n",
      "step 862 : loss 1.0332263708114624\n",
      "step 863 : loss 1.0846978425979614\n",
      "step 864 : loss 1.0325958728790283\n",
      "step 865 : loss 1.084628701210022\n",
      "step 866 : loss 1.0319480895996094\n",
      "step 867 : loss 1.0845651626586914\n",
      "step 868 : loss 1.0312838554382324\n",
      "step 869 : loss 1.0844976902008057\n",
      "step 870 : loss 1.0306307077407837\n",
      "step 871 : loss 1.0844333171844482\n",
      "step 872 : loss 1.0299817323684692\n",
      "step 873 : loss 1.0843671560287476\n",
      "step 874 : loss 1.0293524265289307\n",
      "step 875 : loss 1.0843040943145752\n",
      "step 876 : loss 1.0287092924118042\n",
      "step 877 : loss 1.0842281579971313\n",
      "step 878 : loss 1.0280840396881104\n",
      "step 879 : loss 1.084182620048523\n",
      "step 880 : loss 1.0274633169174194\n",
      "step 881 : loss 1.084120512008667\n",
      "step 882 : loss 1.0268280506134033\n",
      "step 883 : loss 1.084075689315796\n",
      "step 884 : loss 1.0261638164520264\n",
      "step 885 : loss 1.0840250253677368\n",
      "step 886 : loss 1.0255398750305176\n",
      "step 887 : loss 1.0840002298355103\n",
      "step 888 : loss 1.0249053239822388\n",
      "step 889 : loss 1.0839482545852661\n",
      "step 890 : loss 1.0242674350738525\n",
      "step 891 : loss 1.0839052200317383\n",
      "step 892 : loss 1.0236073732376099\n",
      "step 893 : loss 1.0838968753814697\n",
      "step 894 : loss 1.0229651927947998\n",
      "step 895 : loss 1.0838662385940552\n",
      "step 896 : loss 1.022347331047058\n",
      "step 897 : loss 1.0838360786437988\n",
      "step 898 : loss 1.0216953754425049\n",
      "step 899 : loss 1.0838165283203125\n",
      "step 900 : loss 1.021068811416626\n",
      "step 901 : loss 1.0838028192520142\n",
      "step 902 : loss 1.0204240083694458\n",
      "step 903 : loss 1.0837756395339966\n",
      "step 904 : loss 1.0198131799697876\n",
      "step 905 : loss 1.0837469100952148\n",
      "step 906 : loss 1.0191792249679565\n",
      "step 907 : loss 1.0837247371673584\n",
      "step 908 : loss 1.0185389518737793\n",
      "step 909 : loss 1.083733320236206\n",
      "step 910 : loss 1.0178807973861694\n",
      "step 911 : loss 1.0837178230285645\n",
      "step 912 : loss 1.0172507762908936\n",
      "step 913 : loss 1.0837434530258179\n",
      "step 914 : loss 1.0165894031524658\n",
      "step 915 : loss 1.0837403535842896\n",
      "step 916 : loss 1.0159521102905273\n",
      "step 917 : loss 1.0837666988372803\n",
      "step 918 : loss 1.0153391361236572\n",
      "step 919 : loss 1.0837640762329102\n",
      "step 920 : loss 1.0146831274032593\n",
      "step 921 : loss 1.0837490558624268\n",
      "step 922 : loss 1.0140734910964966\n",
      "step 923 : loss 1.0837781429290771\n",
      "step 924 : loss 1.013388991355896\n",
      "step 925 : loss 1.0838009119033813\n",
      "step 926 : loss 1.0127414464950562\n",
      "step 927 : loss 1.083822250366211\n",
      "step 928 : loss 1.012137770652771\n",
      "step 929 : loss 1.0838476419448853\n",
      "step 930 : loss 1.0114879608154297\n",
      "step 931 : loss 1.0838643312454224\n",
      "step 932 : loss 1.0108592510223389\n",
      "step 933 : loss 1.083890676498413\n",
      "step 934 : loss 1.0102126598358154\n",
      "step 935 : loss 1.0839409828186035\n",
      "step 936 : loss 1.0095624923706055\n",
      "step 937 : loss 1.0839887857437134\n",
      "step 938 : loss 1.0089472532272339\n",
      "step 939 : loss 1.0840264558792114\n",
      "step 940 : loss 1.0083023309707642\n",
      "step 941 : loss 1.0840781927108765\n",
      "step 942 : loss 1.0076464414596558\n",
      "step 943 : loss 1.0840972661972046\n",
      "step 944 : loss 1.0069828033447266\n",
      "step 945 : loss 1.0841649770736694\n",
      "step 946 : loss 1.006369709968567\n",
      "step 947 : loss 1.0842344760894775\n",
      "step 948 : loss 1.0056976079940796\n",
      "step 949 : loss 1.0842825174331665\n",
      "step 950 : loss 1.005092978477478\n",
      "step 951 : loss 1.0843422412872314\n",
      "step 952 : loss 1.0044472217559814\n",
      "step 953 : loss 1.0843924283981323\n",
      "step 954 : loss 1.0037685632705688\n",
      "step 955 : loss 1.0844637155532837\n",
      "step 956 : loss 1.0031400918960571\n",
      "step 957 : loss 1.0845555067062378\n",
      "step 958 : loss 1.002476453781128\n",
      "step 959 : loss 1.0846104621887207\n",
      "step 960 : loss 1.001833438873291\n",
      "step 961 : loss 1.0846943855285645\n",
      "step 962 : loss 1.0011870861053467\n",
      "step 963 : loss 1.0847946405410767\n",
      "step 964 : loss 1.0005398988723755\n",
      "step 965 : loss 1.084869146347046\n",
      "step 966 : loss 0.999880313873291\n",
      "step 967 : loss 1.08495032787323\n",
      "step 968 : loss 0.9992081522941589\n",
      "step 969 : loss 1.0850563049316406\n",
      "step 970 : loss 0.9985664486885071\n",
      "step 971 : loss 1.0851337909698486\n",
      "step 972 : loss 0.9979118704795837\n",
      "step 973 : loss 1.0852346420288086\n",
      "step 974 : loss 0.9972727298736572\n",
      "step 975 : loss 1.0853348970413208\n",
      "step 976 : loss 0.9966303706169128\n",
      "step 977 : loss 1.0854424238204956\n",
      "step 978 : loss 0.9959686398506165\n",
      "step 979 : loss 1.085508108139038\n",
      "step 980 : loss 0.995306134223938\n",
      "step 981 : loss 1.0856508016586304\n",
      "step 982 : loss 0.9946460127830505\n",
      "step 983 : loss 1.0857415199279785\n",
      "step 984 : loss 0.9939812421798706\n",
      "step 985 : loss 1.0858465433120728\n",
      "step 986 : loss 0.9933448433876038\n",
      "step 987 : loss 1.085957646369934\n",
      "step 988 : loss 0.9926947951316833\n",
      "step 989 : loss 1.0860559940338135\n",
      "step 990 : loss 0.9920527935028076\n",
      "step 991 : loss 1.0861752033233643\n",
      "step 992 : loss 0.9914097785949707\n",
      "step 993 : loss 1.086299180984497\n",
      "step 994 : loss 0.9907552003860474\n",
      "step 995 : loss 1.0863964557647705\n",
      "step 996 : loss 0.9901168942451477\n",
      "step 997 : loss 1.086514949798584\n",
      "step 998 : loss 0.9894612431526184\n",
      "step 999 : loss 1.0866281986236572\n"
     ]
    }
   ],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.SGD(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(init_tensor)\n",
    "    loss = (y-init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADC0lEQVR4Ae2dgW7iMBAFScX/f3EpNUREriy0MfJmMB2ku+bi1i+e8SMgpN5yOS2nU/lzLX/7yCZwY/14XO4HX49/+pUhsKw7v/ZgF8aqqHd9PfPP/VmnJl+PenwQgfNBOf8y5tner2HYgJoGcHxeLdWu6uP1ita7QnseuN6Pi7QBsNIOAaUH2wuk+hheweTxHQImX+mbXn73q6CtBGVB9XG9vhnvFu1a1lXU5zPWZQPqnQMcdzegvcaMfdGmZJ95topn50ddjw0YRfLFeboFlB2RvSleXMqcP9YtYM5lvu9VD7gH1K8T1oXOWJF2Fa20jHXZgJbzoWe6G7Bnp7QreO2n2nk+74wNgJ12N2DP9X7qft+zrt77hA3Ys6MSv0cBg+GWluwpypaqgA0FcxDcA7pkMiuYPNUGwAIVoACYABwfNKC8qu19YQsvaLb4QMBsy5nvehUAO1OAAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNgAWEPz2dPjqJozv/UXDNgCWrIDBAvwfNAYDzZ7OBmQTDuZXQAAoe1gB2YSD+RUQAMoeVkA24WB+BQSAsocVkE04mF8BAaDsYQVkEw7mV0AAKHtYAdmEg/kVEADKHlZANuFgfgUEgLKH/URsMGE/ERsMNHs6n4KyCQfzKyAAlD2sgMGE/Ux4MNDs6WxANuFgfgUEgLKHFZBNOJhfAQGg7GHfCQ8m7DvhwUCzp/MpKJtwML8CAkDZw94DBhMu74S3x577gQ3YcDEHCmC4b6kK2FAwB7vuAXuey5jLnz/VBsAOi4Cyv93imAYbgKFfg5fv+1c9HO/hcr29Z5D88eT/JP4CXKkjDX9mR4EAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(autoencoder(init_tensor).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(init_tensor.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "\n",
    "def generate_image(size, num_images):\n",
    "    for i in range(num_images):\n",
    "        # Create a new image with a random background color\n",
    "        img = Image.new(\"RGB\", size, color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "        # Get a drawing context\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Choose a random shape (circle, square, or triangle)\n",
    "        shape = random.choice([\"circle\", \"square\", \"triangle\"])\n",
    "\n",
    "        # Choose a random position\n",
    "        position = (random.randint(20, size[0]-20), random.randint(20, size[1]-20))\n",
    "\n",
    "        # Choose a random color for the shape\n",
    "        shape_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Draw the shape on the image\n",
    "        if shape == \"circle\":\n",
    "            draw.ellipse([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"square\":\n",
    "            draw.rectangle([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"triangle\":\n",
    "            draw.polygon([(position[0], position[1]-20), (position[0]-20, position[1]+20), (position[0]+20, position[1]+20)], fill=shape_color)\n",
    "\n",
    "        # Show the image\n",
    "        img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDmqKKK9Q+pCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKhuJmi27QOc9aqMXJ2Q4xcnZE1FUvtcnov5Ufa5PRfyrX6vM1+rzLtFUvtcnov5Ufa5PRfyo+rzD6vMu0VXgnaVyrAYxnirFZSi4uzM5RcXZhRRRUkhRRRQAUUUUAFFFFABVW8/g/GrVVbz+D8a2ofxEa0P4iKtFFFd53hRRRQBPaf60/7tXapWn+tP+7V2uHEfGcOI+MKKKKwMQooooAKKKKACiiigAqrefwfjVqqt5/B+NbUP4iNaH8RFWiiiu87wooooAntP9af92rtUrT/Wn/dq7XDiPjOHEfGFFFFYGIUUUUAFFFFABRRRQAVFND5u35sY9qloqoycXdDjJxd0Vfsf/TT9KPsf/TT9KtUVp7ep3NPb1O5V+x/9NP0o+x/9NP0q1RR7ep3D29TuQw2/lOW3Z4x0qaiis5ScndmcpOTuwoooqRBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAB2ElEQVR4Ae3bsQ2DQADFUILS0mWMVMzIFlmLMdJlAcQGNKdHJFOfsGTz6e7xWbepxxmYHTryaaAA+DsoQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAOOfI/jL/hrx2ivv/L2/V47d50y/INyiAAXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41sADjDkovbf3ZaGEVoAlH+iC1AAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA4w/fSgdgwr2/vAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDgaKKK4j9WCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopM10UcJXrK9ON0cmIx+Gw7tVmk/vf3IWikzS0q2FrUf4kbDw+Nw+I/hST/P7nqFFFFYHUFFFFABRRRQAUUUUAFFFFABRRSGurBUFXrxg9upw5jinhsNKot+nqxKKKK+zjFRSjFWSPzyc5Tk5Sd2wooopThGpFxkrplU6k6U1ODs0OopBS18ViqPsa0qfY/RcFiPrGHjV7r8dn+IUUUVgdQUUUUAFFFFABRRRQAUhpaDXZgK6oV4zlt1ODM8NLE4WVOO+6+Q2iiivsk01dH5604uz3CiiilKSinKTskEISnJRirtiiloor4rF1lWryqLZn6NgMO8PhoUnul+L1YUUUVznWFFFFABRRRQAUUUUAFFFFABikxS0V2UMfXoLlhLTscGJyzC4mXNUjr3WgmKWiipr42vXVpy07DwuXYbDO9OOvfdhRRRXKdwUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAC8ElEQVR4Ae2cYWrbYBQEm9C7tGcs9Fehl+sFepyqGIzAjpYEjTbvY/IjCOuxK83oOQScvPx5/fbFrx6B1161zf8JKKD8HChAAWUC5Xo3QAFlAuV6N0ABZQLlejdAAWUC5Xo3QAFlAuV6N0ABZQLlejdAAWUC5Xo3QAFlAuV6N0ABZQLlejdAAWUC5Xo3QAFlAuV6N0ABZQLlejdAAWUC5Xo3QAFlAuV6N0ABZQLlejdAAWUC5Xo3QAFlAuV6N0ABZQLlejdAAWUC5Xo3QAFlAuV6N6As4Gu5n6//8f3nWyW///5669Rlr7+s+s86Drg/wi2aWFDAu9DvZVQ0LCXgw+iLGtb5IXwK/c3EWTl7qQfHiwg4l9q5aQf0t1MrCCB4EZlPTYwXwJHikvcmZgugGdH5s9+CLqCzAaJbZm/AfpeHHk8VQD+Ye51o11QBe0Cjj0cKQB/Jpzq5xpECnjIa+qICyuLmCeDeDY5VQL3zBBxjGndWAWVlClBAmUC53g1QQJlAud4NUECZQLl+3gZUPjyyWYJ65wkoP7Fn1yvgbKLvzBspAHo3OEDHNY4UcEBq3KmpArhH8lEh2jVVwCOmoa8MFoA+mHeddMtgARsjmg6dv93CbAGogwvoryAAcnAN/UUEnO7gMvrrCDjRwZX0t8te6k+UtvvZvj788YWL0d+udkEBtxt7l4YK+sUF3G5v+35gosj9fnnLbsD9Dj/5wfjfAz4533h5CoiI2AEFsHxjugIiInZAASzfmK6AiIgdUADLN6YrICJiBxTA8o3pCoiI2AEFsHxjugIiInZAASzfmK6AiIgdUADLN6YrICJiBxTA8o3pCoiI2AEFsHxjugIiInZAASzfmK6AiIgdUADLN6YrICJiBxTA8o3pCoiI2AEFsHxjugIiInZAASzfmK6AiIgdUADLN6YrICJiBxTA8o3pCoiI2AEFsHxjugIiInZAASzfmK6AiIgdUADLN6YrICJiBxTA8o3pCoiI2IF/PRtTm6kxdtUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0aiiivmD5IKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACikBDDIII6cUtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABUN3cC2tnkOMjhQe5qasLV7nzZxCp+WPrju3+f61pShzysdWCw/t6yi9upPpF0WZ4JGJJ+ZST19f8fzrWrlIpWhlWRD8ynIrqIpVmiWRD8rDIrTEQtLmXU6s1w/s6ntI7P8AMfRRRXOeWFFFFABRRRQAUUUUAFFFFABRRRQBDd3AtrZ5DjI4UHua5gksxJJJPJJ71oavc+bOIVPyx9cd2/z/AFrOrvoQ5Y37n02WYf2VLme8v6QVr6Nc/et2P+0mf1H9fzrIp8UrQyrIh+ZTkVpUhzxsdWKoKvScP6udXRTIpVmiWRD8rDIp9eZsfINNOzCiiigQUUUUAFFFFABRRRQAVDd3AtrZ5DjI4UHuamopq19SoNKScldHJElmJJJJ5JPekrrqK6vrXkez/bP9z8f+AcjRXXUUfWvIf9s/3Px/4BkaNc/et2P+0mf1H9fzrXoornnJSldI8rE1lWqOaVrhRRRUGAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACx0lEQVR4Ae2Ty00EMRAFARENaZEENyIhLXLhggCttFoJeeRfu9o7xQnNzvi1q/o9vn+9PfjHEXjiok3+I6AAeA8UoACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATh+RQNePr7hWyaOXyEg8fX50cIFXNbfEpRUhwsoBfv8QiBWwO3i3/4v/SuBWAHXGP8pEQgU8H/l/z8pjXWe54ECzgNx5KZRAkrLXno+coetv40SsDWUlcOHCDhe8+NfV14+Q1aIgAwX22WG+QJqFrzmnV0IDs45X8DgQGf7fLKA+tWuf/O+lUwWcN+wIm43U0DrUre+H3F//MyZAvDL7DjANAF969z31Y6gSzNPE1AK8PkxgTkCRhZ55Nvju23x6xwBW1w155ATBIyv8PgJOeHWTDVBQE2M75QIjAqYtbyzzindM+3zUQFpL7bLYEMC5q7t3NNOIWCXS2aes78BEQsbcWZm+r+z9QtIfrFdxusUELeqcSfnVNIpIOdldpyqR0D0kkafn8pTj4BUF9h9mGYBa9ZzTUoGec0CMgx9TzO0CVi5mCuzQKNtAsBB7zW6QcD6lVyfuF5zg4D1w50hsVYAtYxU7jL3z5VJn6+1qioP9LULAbHCm6AABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgON/AKEhTLq+Vhx7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDWooor7Q/QAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiipobWWflVwv948ConUjTjzTdkTKUYq8nYhorQGlnAzNg98L/wDXqKXT5UBKEOB6dfyrljmGGk+VTMY4qjJ2UipRSkEEgjBHUGkrsOgKKKKYBRRRQAUUUUAFFFFABRRRQBas7Xz3LN9xevv7VrAAAADAHQCobRBHaxgdxuJx61PXyOPxMq1Z66LY8PE1XUm+yCiiiuI5ype2glUyIMSAf99Vk10NYl0gjuZFHTORgevNfQZRiZSToye2x6eBqt3g+hDRRRXtnohRRRQAUUUUAFFFFABRRRQBt2rB7WMj+7j8uKmrKsLlYmMbnCscg+hrVr5DHYeVGs09nqjwsTSdOo+wUUUVxmAVi3jB7uQj1x+XFaV3crBGQD+8I+UenvWNXvZPh5K9Z9dEelgKTV6jCiiivdPSCiiigAooooAKKKKACiiigAq1BfSQgKfnQdAeo/GqtFZVaMKseWoroidOM1aSuag1OLAyjg98YqKXUiQREmP9pv8ACqFFckcsw0Xe1zCODop3sOd2kcsxyx6mm0UV3pJKyOpK2iCiiimAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAC4ElEQVR4Ae2cUU7bQBRFAXUTXVV/yirLvlgEqlQjS1EUy69g+c55Y518oMRPzB2fMzfwEXj++PX3yQdH4IWLNvmTgALgc6AABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA2ABfyA88+If39921vm55/fe6Mm15/n/efdBfct3LYmphTwLfT3MhpqmEzAYfRtNcz0Q/gU+ouJs9a5l3r4+TQCzqV27mqH6S/fOIeABK/EmgdMTCAgRyq38tdNdBeQZpRe/78mWgsYQ2dMyp6J1gL2Nn2l630FjDyYI7MeTk9fAQ8bverLpgLGH8nxieuRairgqud9e18K2DIZeqWjAOrdAMntKGDoCaTDFAAbUIACYAJwvA1QAEwAjrcBCoAJwPEdG0B9eATJ7SgAPpNj4xUwlvcmramA8e8G4xNXF00FbA7KZS/0FTDySI7MejhKfQU8bPSqL1sLGHMwx6TsHaDWApZNp+mk19/jfrveXUDUAU5/ubsJBIQcdKA/jYDTHTShP5OAEx30ob/c1GR/orTseHkc/vhCK/TrvUwpYN36tzQ0RD+9gPUGlq+Fibbcb5ufuAG3e5j6yRy/hk6NuN68Amo+8akC4ojrAAXUfOJTBcQR1wEKqPnEpwqII64DFFDziU8VEEdcByig5hOfKiCOuA5QQM0nPlVAHHEdoICaT3yqgDjiOkABNZ/4VAFxxHWAAmo+8akC4ojrAAXUfOJTBcQR1wEKqPnEpwqII64DFFDziU8VEEdcByig5hOf/gOX/V4zEeN8qQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0OiiivyM88KKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopqOkkayRsrowBVlOQQe4NADqKKKACiiigAooooAKKKKACiiigAooooAKKKKAMPxXq39laM/lvtuJ/3cWDgj1bqDwO46EiqPgfVvtemtYSvma2+5k8tGenfPB49ANtcr4r1b+1dZfy33W8H7uLByD6t1I5PcdQBVPRdTfSNVhu13FFOJFH8SHqMZGfUe4Fe9DAXwnLb3nr/AMA7lQ/dW6nsFFNR0kjWSNldGAKspyCD3Bp1eCcIUUUUAFFFFABRRRQAUUUUAFFFFABWH4r1b+ytGfy323E/7uLBwR6t1B4HcdCRW5XlfivVv7V1l/Lfdbwfu4sHIPq3Ujk9x1AFduAw/tqyvstWbUKfPPyRh0UUV9SemejeB9W+16a1hK+Zrb7mTy0Z6d88Hj0A211VeP6Lqb6RqsN2u4opxIo/iQ9RjIz6j3Ar15HSSNZI2V0YAqynIIPcGvmcyw/sqvMtpf0zzsRT5ZXXUdRRRXnnOFFFFABRRRQAUUUUAFFFFAGH4r1b+ytGfy323E/7uLBwR6t1B4HcdCRXlde4UV6GEx6w8OVQu35/8A6KVdU1ax4fRXuFFdf9s/3Px/4Bp9b8jw+vRvA+rfa9NawlfM1t9zJ5aM9O+eDx6Aba6qiubFZgsRT5HC3z/wCARVrqpG1gooorzTmCiiigAooooAKKKKACiiigD//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACzElEQVR4Ae2TO3LCQBAFjUMu6rNxAx+M2GVTJBgksf/ekdqRS0j7ZrvnnS7X84d/HIFPLtrkGwEFwHugAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPHNGvD99QNfJWZ8MwExr89P3UbAff0tQYHPNgIKgv3kTqCBgMfFf/xfxCkEGghIifGdNQK1Al5X/vXJWrbP/wjUChBiJYEqAWvLvva8ctZdfl4lYJdEBl+qXMD2mm//OviSM8eVC5j5VoFmKxSQsuAp7wQi1WnUQgGdpjngsSUC0lc7/c0Dor9fuUTAYWH1uHi2gNylzn2/xyVnPjNbwMyXiThbnoCydS77KiLNgpnzBBQE+Mk2gQwBNYtc8+32BaL/miEg+lXnnD9VQP0K158wJ8HKqVIFVMb4+RqBJAGtlrfVOWuXifg8SUDEi0WZ+b2Atmvb9rQolDfmfC9g42N/qifwRkCPhe1xZj0I6oQ3AqixjpO7JaDfqvY7OZy5LQHhLhNx4FUBvZe09/lRZKwKiHKB6HMuCxiznmNSJje0LGDyofc03oKAkYs5MmtObQsC5hx0r1M9Cxi/kuMTp3L5LGCq4Y4wzD8B1DJSuTMIPl2u5xnmOOwM/xpwWArgxRUAwr9FK0ABMAE43gYoACYAx9sAWMAvCJla8r6BuGAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size = (128, 128)\n",
    "num_images = 5  # Change this to the number of images you want to generate\n",
    "\n",
    "generate_image(image_size, num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
