{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image: Image) -> torch.Tensor:\n",
    "    img_array = np.array(image)\n",
    "    img_array = img_array/255\n",
    "    img_array = img_array.transpose(2, 0, 1).astype(np.float32)\n",
    "    img_tensor = torch.from_numpy(img_array)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a Tensor to a PIL Image.\n",
    "\n",
    "    The tensor must have shape `[1, channels, height, width]` where the number of\n",
    "    channels is either 1 (grayscale) or 3 (RGB) or 4 (RGBA).\n",
    "\n",
    "    Expected values are in the range `[0, 1]` and are clamped to this range.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 4 and tensor.shape[0] == 1, f\"Unsupported tensor shape: {tensor.shape}\"\n",
    "    num_channels = tensor.shape[1]\n",
    "    tensor = tensor.clamp(0, 1).squeeze(0)\n",
    "\n",
    "    match num_channels:\n",
    "        case 1:\n",
    "            tensor = tensor.squeeze(0)\n",
    "        case 3 | 4:\n",
    "            tensor = tensor.permute(1, 2, 0)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported number of channels: {num_channels}\")\n",
    "\n",
    "    return Image.fromarray((tensor.cpu().numpy() * 255).astype(\"uint8\"))  # type: ignore[reportUnknownType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = tuple[int, int, int]\n",
    "def create_empty_image(resolution: int, color: color=(0,0,0)) -> Image:\n",
    "    return Image.new('RGB', (resolution, resolution), color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = image_to_tensor(create_empty_image(128, color=(255, 0, 0)))\n",
    "target_tensor = image_to_tensor(create_empty_image(128, color=(0, 255, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/laure/vton/laure_f/autoencoder.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     init_tensor \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr\u001b[39m*\u001b[39;49minit_tensor\u001b[39m.\u001b[39;49mgrad\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     init_tensor\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mzero_()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(3, 3, 3, padding=1)\n",
    "lr = 0.01\n",
    "min_steps = 100\n",
    "\n",
    "for step in range(min_steps):\n",
    "    y = conv(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        init_tensor -= lr*init_tensor.grad\n",
    "        init_tensor.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int=1, out_features: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "linear = Linear(in_features=3, out_features=3)\n",
    "x = torch.randn(1, 3)\n",
    "target_tensor = torch.randn(1, 3)\n",
    "y = linear(x)\n",
    "loss = ((y - target_tensor)**2).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resblock(nn.Module):\n",
    "    def __init__(self, in_channels: int=1, out_channels: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(x)\n",
    "        y = self.conv2(x)\n",
    "        return y+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/laure/vton/laure_f/autoencoder.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(min_steps):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     y \u001b[39m=\u001b[39m block(init_tensor)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m (y \u001b[39m-\u001b[39;49m target_tensor)\u001b[39m.\u001b[39mnorm()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "block = resblock(3, 3)\n",
    "\n",
    "lr = 1e-5\n",
    "min_steps = 10000\n",
    "for step in range(min_steps):\n",
    "    y = block(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step: {step}, loss: {loss.item()}')\n",
    "        for param in block.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr*param.grad\n",
    "\n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv1_1 = nn.Conv2d(256, 4, 1, padding=0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.conv1_1(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, output_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv1_1 = nn.Conv2d(4, 256, 1, padding=0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1_1(x)\n",
    "        y = self.conv1(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        return y\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 131.0380096435547\n",
      "step 1 : loss 130.60336303710938\n",
      "step 2 : loss 130.16302490234375\n",
      "step 3 : loss 129.71522521972656\n",
      "step 4 : loss 129.2603302001953\n",
      "step 5 : loss 128.79769897460938\n",
      "step 6 : loss 128.32713317871094\n",
      "step 7 : loss 127.85008239746094\n",
      "step 8 : loss 127.36390686035156\n",
      "step 9 : loss 126.8695068359375\n",
      "step 10 : loss 126.36640167236328\n",
      "step 11 : loss 125.85428619384766\n",
      "step 12 : loss 125.33370208740234\n",
      "step 13 : loss 124.80420684814453\n",
      "step 14 : loss 124.26411437988281\n",
      "step 15 : loss 123.71537017822266\n",
      "step 16 : loss 123.15577697753906\n",
      "step 17 : loss 122.58430480957031\n",
      "step 18 : loss 122.00279235839844\n",
      "step 19 : loss 121.40977478027344\n",
      "step 20 : loss 120.8049087524414\n",
      "step 21 : loss 120.185302734375\n",
      "step 22 : loss 119.55496215820312\n",
      "step 23 : loss 118.91166687011719\n",
      "step 24 : loss 118.2535171508789\n",
      "step 25 : loss 117.58138275146484\n",
      "step 26 : loss 116.89311981201172\n",
      "step 27 : loss 116.18940734863281\n",
      "step 28 : loss 115.46844482421875\n",
      "step 29 : loss 114.72876739501953\n",
      "step 30 : loss 113.97044372558594\n",
      "step 31 : loss 113.19154357910156\n",
      "step 32 : loss 112.3909912109375\n",
      "step 33 : loss 111.56721496582031\n",
      "step 34 : loss 110.71929931640625\n",
      "step 35 : loss 109.84455108642578\n",
      "step 36 : loss 108.94226837158203\n",
      "step 37 : loss 108.00536346435547\n",
      "step 38 : loss 107.03865814208984\n",
      "step 39 : loss 106.03437805175781\n",
      "step 40 : loss 104.99124145507812\n",
      "step 41 : loss 103.9009780883789\n",
      "step 42 : loss 102.76588439941406\n",
      "step 43 : loss 101.57574462890625\n",
      "step 44 : loss 100.32518768310547\n",
      "step 45 : loss 99.00629425048828\n",
      "step 46 : loss 97.6090316772461\n",
      "step 47 : loss 96.1221923828125\n",
      "step 48 : loss 94.53021240234375\n",
      "step 49 : loss 92.8163070678711\n",
      "step 50 : loss 90.95709991455078\n",
      "step 51 : loss 88.91817474365234\n",
      "step 52 : loss 86.67134857177734\n",
      "step 53 : loss 84.15937805175781\n",
      "step 54 : loss 81.3180923461914\n",
      "step 55 : loss 78.05142974853516\n",
      "step 56 : loss 74.2228012084961\n",
      "step 57 : loss 69.64976501464844\n",
      "step 58 : loss 64.06269073486328\n",
      "step 59 : loss 57.093345642089844\n",
      "step 60 : loss 48.35422134399414\n",
      "step 61 : loss 38.14299774169922\n",
      "step 62 : loss 30.38098907470703\n",
      "step 63 : loss 29.383169174194336\n",
      "step 64 : loss 29.051340103149414\n",
      "step 65 : loss 28.756372451782227\n",
      "step 66 : loss 28.472412109375\n",
      "step 67 : loss 28.192472457885742\n",
      "step 68 : loss 27.914735794067383\n",
      "step 69 : loss 27.639636993408203\n",
      "step 70 : loss 27.366031646728516\n",
      "step 71 : loss 27.094276428222656\n",
      "step 72 : loss 26.824527740478516\n",
      "step 73 : loss 26.55750846862793\n",
      "step 74 : loss 26.292436599731445\n",
      "step 75 : loss 26.02864646911621\n",
      "step 76 : loss 25.76750373840332\n",
      "step 77 : loss 25.508163452148438\n",
      "step 78 : loss 25.250768661499023\n",
      "step 79 : loss 24.995277404785156\n",
      "step 80 : loss 24.741880416870117\n",
      "step 81 : loss 24.490453720092773\n",
      "step 82 : loss 24.240842819213867\n",
      "step 83 : loss 23.992467880249023\n",
      "step 84 : loss 23.74664878845215\n",
      "step 85 : loss 23.50286865234375\n",
      "step 86 : loss 23.260833740234375\n",
      "step 87 : loss 23.020666122436523\n",
      "step 88 : loss 22.782445907592773\n",
      "step 89 : loss 22.546056747436523\n",
      "step 90 : loss 22.31125259399414\n",
      "step 91 : loss 22.078548431396484\n",
      "step 92 : loss 21.847631454467773\n",
      "step 93 : loss 21.61851692199707\n",
      "step 94 : loss 21.39118766784668\n",
      "step 95 : loss 21.165836334228516\n",
      "step 96 : loss 20.942115783691406\n",
      "step 97 : loss 20.720216751098633\n",
      "step 98 : loss 20.500017166137695\n",
      "step 99 : loss 20.281652450561523\n",
      "step 100 : loss 20.06490707397461\n",
      "step 101 : loss 19.85003089904785\n",
      "step 102 : loss 19.63690185546875\n",
      "step 103 : loss 19.425352096557617\n",
      "step 104 : loss 19.215572357177734\n",
      "step 105 : loss 19.007375717163086\n",
      "step 106 : loss 18.800872802734375\n",
      "step 107 : loss 18.59614372253418\n",
      "step 108 : loss 18.39297866821289\n",
      "step 109 : loss 18.191364288330078\n",
      "step 110 : loss 17.991554260253906\n",
      "step 111 : loss 17.793283462524414\n",
      "step 112 : loss 17.59648895263672\n",
      "step 113 : loss 17.401403427124023\n",
      "step 114 : loss 17.207874298095703\n",
      "step 115 : loss 17.01584815979004\n",
      "step 116 : loss 16.825366973876953\n",
      "step 117 : loss 16.636449813842773\n",
      "step 118 : loss 16.448986053466797\n",
      "step 119 : loss 16.26315689086914\n",
      "step 120 : loss 16.07872772216797\n",
      "step 121 : loss 15.895843505859375\n",
      "step 122 : loss 15.714430809020996\n",
      "step 123 : loss 15.53443717956543\n",
      "step 124 : loss 15.355990409851074\n",
      "step 125 : loss 15.178929328918457\n",
      "step 126 : loss 15.003195762634277\n",
      "step 127 : loss 14.829000473022461\n",
      "step 128 : loss 14.65619945526123\n",
      "step 129 : loss 14.48485279083252\n",
      "step 130 : loss 14.314827919006348\n",
      "step 131 : loss 14.146221160888672\n",
      "step 132 : loss 13.97898006439209\n",
      "step 133 : loss 13.81308650970459\n",
      "step 134 : loss 13.648612022399902\n",
      "step 135 : loss 13.485380172729492\n",
      "step 136 : loss 13.323593139648438\n",
      "step 137 : loss 13.163143157958984\n",
      "step 138 : loss 13.00399112701416\n",
      "step 139 : loss 12.846221923828125\n",
      "step 140 : loss 12.689659118652344\n",
      "step 141 : loss 12.534467697143555\n",
      "step 142 : loss 12.380603790283203\n",
      "step 143 : loss 12.227989196777344\n",
      "step 144 : loss 12.076700210571289\n",
      "step 145 : loss 11.926658630371094\n",
      "step 146 : loss 11.777904510498047\n",
      "step 147 : loss 11.6304931640625\n",
      "step 148 : loss 11.484243392944336\n",
      "step 149 : loss 11.339357376098633\n",
      "step 150 : loss 11.195712089538574\n",
      "step 151 : loss 11.053328514099121\n",
      "step 152 : loss 10.912202835083008\n",
      "step 153 : loss 10.77233600616455\n",
      "step 154 : loss 10.633769035339355\n",
      "step 155 : loss 10.49638557434082\n",
      "step 156 : loss 10.36032772064209\n",
      "step 157 : loss 10.225494384765625\n",
      "step 158 : loss 10.09189510345459\n",
      "step 159 : loss 9.959567070007324\n",
      "step 160 : loss 9.828489303588867\n",
      "step 161 : loss 9.698662757873535\n",
      "step 162 : loss 9.570063591003418\n",
      "step 163 : loss 9.442787170410156\n",
      "step 164 : loss 9.316751480102539\n",
      "step 165 : loss 9.192069053649902\n",
      "step 166 : loss 9.068559646606445\n",
      "step 167 : loss 8.946345329284668\n",
      "step 168 : loss 8.825398445129395\n",
      "step 169 : loss 8.70572566986084\n",
      "step 170 : loss 8.587357521057129\n",
      "step 171 : loss 8.470254898071289\n",
      "step 172 : loss 8.354491233825684\n",
      "step 173 : loss 8.240018844604492\n",
      "step 174 : loss 8.126872062683105\n",
      "step 175 : loss 8.015095710754395\n",
      "step 176 : loss 7.904671669006348\n",
      "step 177 : loss 7.795571327209473\n",
      "step 178 : loss 7.6877851486206055\n",
      "step 179 : loss 7.581442356109619\n",
      "step 180 : loss 7.476447582244873\n",
      "step 181 : loss 7.372871398925781\n",
      "step 182 : loss 7.270803928375244\n",
      "step 183 : loss 7.170053482055664\n",
      "step 184 : loss 7.0707621574401855\n",
      "step 185 : loss 6.9729204177856445\n",
      "step 186 : loss 6.876589298248291\n",
      "step 187 : loss 6.781734943389893\n",
      "step 188 : loss 6.68837833404541\n",
      "step 189 : loss 6.596588134765625\n",
      "step 190 : loss 6.50640869140625\n",
      "step 191 : loss 6.4177165031433105\n",
      "step 192 : loss 6.330581188201904\n",
      "step 193 : loss 6.2450714111328125\n",
      "step 194 : loss 6.161159992218018\n",
      "step 195 : loss 6.078866958618164\n",
      "step 196 : loss 5.998286724090576\n",
      "step 197 : loss 5.919316291809082\n",
      "step 198 : loss 5.8420329093933105\n",
      "step 199 : loss 5.766437530517578\n",
      "step 200 : loss 5.692537784576416\n",
      "step 201 : loss 5.620361804962158\n",
      "step 202 : loss 5.549979209899902\n",
      "step 203 : loss 5.481254577636719\n",
      "step 204 : loss 5.414233684539795\n",
      "step 205 : loss 5.348954677581787\n",
      "step 206 : loss 5.285409450531006\n",
      "step 207 : loss 5.223626136779785\n",
      "step 208 : loss 5.163524150848389\n",
      "step 209 : loss 5.105157852172852\n",
      "step 210 : loss 5.048503875732422\n",
      "step 211 : loss 4.993492603302002\n",
      "step 212 : loss 4.9401774406433105\n",
      "step 213 : loss 4.888521194458008\n",
      "step 214 : loss 4.838476181030273\n",
      "step 215 : loss 4.790029048919678\n",
      "step 216 : loss 4.743165016174316\n",
      "step 217 : loss 4.6978349685668945\n",
      "step 218 : loss 4.653997421264648\n",
      "step 219 : loss 4.611659049987793\n",
      "step 220 : loss 4.57072639465332\n",
      "step 221 : loss 4.531212329864502\n",
      "step 222 : loss 4.493036270141602\n",
      "step 223 : loss 4.45621919631958\n",
      "step 224 : loss 4.420626163482666\n",
      "step 225 : loss 4.386277675628662\n",
      "step 226 : loss 4.353073596954346\n",
      "step 227 : loss 4.321015357971191\n",
      "step 228 : loss 4.2900238037109375\n",
      "step 229 : loss 4.260080814361572\n",
      "step 230 : loss 4.23114538192749\n",
      "step 231 : loss 4.2031402587890625\n",
      "step 232 : loss 4.176042556762695\n",
      "step 233 : loss 4.149792194366455\n",
      "step 234 : loss 4.124364852905273\n",
      "step 235 : loss 4.099716663360596\n",
      "step 236 : loss 4.075809478759766\n",
      "step 237 : loss 4.052587985992432\n",
      "step 238 : loss 4.030038833618164\n",
      "step 239 : loss 4.008118629455566\n",
      "step 240 : loss 3.9867751598358154\n",
      "step 241 : loss 3.965975284576416\n",
      "step 242 : loss 3.945709466934204\n",
      "step 243 : loss 3.925945997238159\n",
      "step 244 : loss 3.9066412448883057\n",
      "step 245 : loss 3.8877909183502197\n",
      "step 246 : loss 3.869333028793335\n",
      "step 247 : loss 3.8512589931488037\n",
      "step 248 : loss 3.8335723876953125\n",
      "step 249 : loss 3.816232681274414\n",
      "step 250 : loss 3.79919171333313\n",
      "step 251 : loss 3.7824716567993164\n",
      "step 252 : loss 3.7660303115844727\n",
      "step 253 : loss 3.7498793601989746\n",
      "step 254 : loss 3.7339513301849365\n",
      "step 255 : loss 3.718320369720459\n",
      "step 256 : loss 3.702890157699585\n",
      "step 257 : loss 3.6876566410064697\n",
      "step 258 : loss 3.672640800476074\n",
      "step 259 : loss 3.657820224761963\n",
      "step 260 : loss 3.643164873123169\n",
      "step 261 : loss 3.6286888122558594\n",
      "step 262 : loss 3.6143531799316406\n",
      "step 263 : loss 3.6001806259155273\n",
      "step 264 : loss 3.586168050765991\n",
      "step 265 : loss 3.572300910949707\n",
      "step 266 : loss 3.5585598945617676\n",
      "step 267 : loss 3.5449631214141846\n",
      "step 268 : loss 3.5314762592315674\n",
      "step 269 : loss 3.518099784851074\n",
      "step 270 : loss 3.5048532485961914\n",
      "step 271 : loss 3.491703748703003\n",
      "step 272 : loss 3.4786460399627686\n",
      "step 273 : loss 3.4657018184661865\n",
      "step 274 : loss 3.4528238773345947\n",
      "step 275 : loss 3.4400534629821777\n",
      "step 276 : loss 3.4273810386657715\n",
      "step 277 : loss 3.4147982597351074\n",
      "step 278 : loss 3.4022932052612305\n",
      "step 279 : loss 3.389878749847412\n",
      "step 280 : loss 3.3775293827056885\n",
      "step 281 : loss 3.3652565479278564\n",
      "step 282 : loss 3.3530495166778564\n",
      "step 283 : loss 3.3409183025360107\n",
      "step 284 : loss 3.328855037689209\n",
      "step 285 : loss 3.3168718814849854\n",
      "step 286 : loss 3.304953098297119\n",
      "step 287 : loss 3.2931013107299805\n",
      "step 288 : loss 3.281297206878662\n",
      "step 289 : loss 3.269561290740967\n",
      "step 290 : loss 3.257880210876465\n",
      "step 291 : loss 3.246253490447998\n",
      "step 292 : loss 3.2347092628479004\n",
      "step 293 : loss 3.223207712173462\n",
      "step 294 : loss 3.2117528915405273\n",
      "step 295 : loss 3.200368642807007\n",
      "step 296 : loss 3.1890408992767334\n",
      "step 297 : loss 3.177767515182495\n",
      "step 298 : loss 3.1665468215942383\n",
      "step 299 : loss 3.1553709506988525\n",
      "step 300 : loss 3.144245147705078\n",
      "step 301 : loss 3.133167028427124\n",
      "step 302 : loss 3.1221415996551514\n",
      "step 303 : loss 3.1111679077148438\n",
      "step 304 : loss 3.1002559661865234\n",
      "step 305 : loss 3.089364767074585\n",
      "step 306 : loss 3.078533887863159\n",
      "step 307 : loss 3.0677390098571777\n",
      "step 308 : loss 3.0569961071014404\n",
      "step 309 : loss 3.0463006496429443\n",
      "step 310 : loss 3.0356311798095703\n",
      "step 311 : loss 3.025015115737915\n",
      "step 312 : loss 3.0144290924072266\n",
      "step 313 : loss 3.0038998126983643\n",
      "step 314 : loss 2.993410110473633\n",
      "step 315 : loss 2.98297381401062\n",
      "step 316 : loss 2.9725940227508545\n",
      "step 317 : loss 2.9622344970703125\n",
      "step 318 : loss 2.951911211013794\n",
      "step 319 : loss 2.941654682159424\n",
      "step 320 : loss 2.9314167499542236\n",
      "step 321 : loss 2.9212169647216797\n",
      "step 322 : loss 2.911074638366699\n",
      "step 323 : loss 2.9009740352630615\n",
      "step 324 : loss 2.8909332752227783\n",
      "step 325 : loss 2.8809049129486084\n",
      "step 326 : loss 2.870973587036133\n",
      "step 327 : loss 2.8610305786132812\n",
      "step 328 : loss 2.8512136936187744\n",
      "step 329 : loss 2.841373920440674\n",
      "step 330 : loss 2.8317155838012695\n",
      "step 331 : loss 2.8220467567443848\n",
      "step 332 : loss 2.812577247619629\n",
      "step 333 : loss 2.8031814098358154\n",
      "step 334 : loss 2.794023275375366\n",
      "step 335 : loss 2.7851133346557617\n",
      "step 336 : loss 2.7765564918518066\n",
      "step 337 : loss 2.7685554027557373\n",
      "step 338 : loss 2.7612547874450684\n",
      "step 339 : loss 2.7549819946289062\n",
      "step 340 : loss 2.749990224838257\n",
      "step 341 : loss 2.7471797466278076\n",
      "step 342 : loss 2.746608018875122\n",
      "step 343 : loss 2.75014591217041\n",
      "step 344 : loss 2.7569336891174316\n",
      "step 345 : loss 2.7704474925994873\n",
      "step 346 : loss 2.7861058712005615\n",
      "step 347 : loss 2.810364007949829\n",
      "step 348 : loss 2.830073356628418\n",
      "step 349 : loss 2.858475923538208\n",
      "step 350 : loss 2.870875358581543\n",
      "step 351 : loss 2.8942596912384033\n",
      "step 352 : loss 2.8928709030151367\n",
      "step 353 : loss 2.908846616744995\n",
      "step 354 : loss 2.896663188934326\n",
      "step 355 : loss 2.9077959060668945\n",
      "step 356 : loss 2.889904260635376\n",
      "step 357 : loss 2.8988780975341797\n",
      "step 358 : loss 2.878566265106201\n",
      "step 359 : loss 2.8868117332458496\n",
      "step 360 : loss 2.8654167652130127\n",
      "step 361 : loss 2.873624324798584\n",
      "step 362 : loss 2.851799249649048\n",
      "step 363 : loss 2.8601250648498535\n",
      "step 364 : loss 2.8380959033966064\n",
      "step 365 : loss 2.8466360569000244\n",
      "step 366 : loss 2.824431896209717\n",
      "step 367 : loss 2.8332250118255615\n",
      "step 368 : loss 2.8109309673309326\n",
      "step 369 : loss 2.819976568222046\n",
      "step 370 : loss 2.7975358963012695\n",
      "step 371 : loss 2.806853771209717\n",
      "step 372 : loss 2.784306287765503\n",
      "step 373 : loss 2.793900728225708\n",
      "step 374 : loss 2.771205425262451\n",
      "step 375 : loss 2.781100034713745\n",
      "step 376 : loss 2.758234739303589\n",
      "step 377 : loss 2.7684249877929688\n",
      "step 378 : loss 2.7454216480255127\n",
      "step 379 : loss 2.7558889389038086\n",
      "step 380 : loss 2.732755422592163\n",
      "step 381 : loss 2.7435121536254883\n",
      "step 382 : loss 2.7202248573303223\n",
      "step 383 : loss 2.731271505355835\n",
      "step 384 : loss 2.7078380584716797\n",
      "step 385 : loss 2.719114065170288\n",
      "step 386 : loss 2.6955606937408447\n",
      "step 387 : loss 2.7071123123168945\n",
      "step 388 : loss 2.683431625366211\n",
      "step 389 : loss 2.6952826976776123\n",
      "step 390 : loss 2.671393632888794\n",
      "step 391 : loss 2.683581829071045\n",
      "step 392 : loss 2.6595406532287598\n",
      "step 393 : loss 2.6720027923583984\n",
      "step 394 : loss 2.6477949619293213\n",
      "step 395 : loss 2.660529851913452\n",
      "step 396 : loss 2.636153221130371\n",
      "step 397 : loss 2.649174690246582\n",
      "step 398 : loss 2.6246252059936523\n",
      "step 399 : loss 2.637942314147949\n",
      "step 400 : loss 2.6131985187530518\n",
      "step 401 : loss 2.6268293857574463\n",
      "step 402 : loss 2.601900815963745\n",
      "step 403 : loss 2.615851640701294\n",
      "step 404 : loss 2.590735912322998\n",
      "step 405 : loss 2.604937791824341\n",
      "step 406 : loss 2.5797040462493896\n",
      "step 407 : loss 2.594179153442383\n",
      "step 408 : loss 2.568751096725464\n",
      "step 409 : loss 2.583550453186035\n",
      "step 410 : loss 2.557896375656128\n",
      "step 411 : loss 2.573051691055298\n",
      "step 412 : loss 2.5471808910369873\n",
      "step 413 : loss 2.5626091957092285\n",
      "step 414 : loss 2.53655743598938\n",
      "step 415 : loss 2.5523171424865723\n",
      "step 416 : loss 2.5260465145111084\n",
      "step 417 : loss 2.542104721069336\n",
      "step 418 : loss 2.5155885219573975\n",
      "step 419 : loss 2.5319600105285645\n",
      "step 420 : loss 2.5053062438964844\n",
      "step 421 : loss 2.5219690799713135\n",
      "step 422 : loss 2.4951114654541016\n",
      "step 423 : loss 2.5120699405670166\n",
      "step 424 : loss 2.4849510192871094\n",
      "step 425 : loss 2.5022389888763428\n",
      "step 426 : loss 2.474952459335327\n",
      "step 427 : loss 2.492579698562622\n",
      "step 428 : loss 2.4650144577026367\n",
      "step 429 : loss 2.482914447784424\n",
      "step 430 : loss 2.4552032947540283\n",
      "step 431 : loss 2.4734227657318115\n",
      "step 432 : loss 2.445408582687378\n",
      "step 433 : loss 2.4640002250671387\n",
      "step 434 : loss 2.435800790786743\n",
      "step 435 : loss 2.454699754714966\n",
      "step 436 : loss 2.426262378692627\n",
      "step 437 : loss 2.4454691410064697\n",
      "step 438 : loss 2.4168014526367188\n",
      "step 439 : loss 2.436335563659668\n",
      "step 440 : loss 2.407419443130493\n",
      "step 441 : loss 2.427255153656006\n",
      "step 442 : loss 2.3981542587280273\n",
      "step 443 : loss 2.418288469314575\n",
      "step 444 : loss 2.388962507247925\n",
      "step 445 : loss 2.4094414710998535\n",
      "step 446 : loss 2.3798487186431885\n",
      "step 447 : loss 2.4006972312927246\n",
      "step 448 : loss 2.3708090782165527\n",
      "step 449 : loss 2.3919620513916016\n",
      "step 450 : loss 2.3618879318237305\n",
      "step 451 : loss 2.383329391479492\n",
      "step 452 : loss 2.3530142307281494\n",
      "step 453 : loss 2.3747925758361816\n",
      "step 454 : loss 2.3442654609680176\n",
      "step 455 : loss 2.366323232650757\n",
      "step 456 : loss 2.3355712890625\n",
      "step 457 : loss 2.3579304218292236\n",
      "step 458 : loss 2.3269472122192383\n",
      "step 459 : loss 2.3496360778808594\n",
      "step 460 : loss 2.3183963298797607\n",
      "step 461 : loss 2.341466188430786\n",
      "step 462 : loss 2.30993914604187\n",
      "step 463 : loss 2.333303213119507\n",
      "step 464 : loss 2.30153226852417\n",
      "step 465 : loss 2.3252182006835938\n",
      "step 466 : loss 2.2932543754577637\n",
      "step 467 : loss 2.3172447681427\n",
      "step 468 : loss 2.2850396633148193\n",
      "step 469 : loss 2.3093576431274414\n",
      "step 470 : loss 2.276864528656006\n",
      "step 471 : loss 2.301539659500122\n",
      "step 472 : loss 2.268791675567627\n",
      "step 473 : loss 2.293792486190796\n",
      "step 474 : loss 2.2607555389404297\n",
      "step 475 : loss 2.286098003387451\n",
      "step 476 : loss 2.252819299697876\n",
      "step 477 : loss 2.2785141468048096\n",
      "step 478 : loss 2.2449519634246826\n",
      "step 479 : loss 2.2709543704986572\n",
      "step 480 : loss 2.237133741378784\n",
      "step 481 : loss 2.2634775638580322\n",
      "step 482 : loss 2.2293894290924072\n",
      "step 483 : loss 2.256035804748535\n",
      "step 484 : loss 2.22176194190979\n",
      "step 485 : loss 2.2487149238586426\n",
      "step 486 : loss 2.2141802310943604\n",
      "step 487 : loss 2.241441488265991\n",
      "step 488 : loss 2.2066099643707275\n",
      "step 489 : loss 2.234238386154175\n",
      "step 490 : loss 2.1991565227508545\n",
      "step 491 : loss 2.2271342277526855\n",
      "step 492 : loss 2.191763401031494\n",
      "step 493 : loss 2.2200567722320557\n",
      "step 494 : loss 2.1843984127044678\n",
      "step 495 : loss 2.213066577911377\n",
      "step 496 : loss 2.1771304607391357\n",
      "step 497 : loss 2.206115484237671\n",
      "step 498 : loss 2.1698970794677734\n",
      "step 499 : loss 2.19924259185791\n",
      "step 500 : loss 2.162768602371216\n",
      "step 501 : loss 2.192446708679199\n",
      "step 502 : loss 2.1556718349456787\n",
      "step 503 : loss 2.185683488845825\n",
      "step 504 : loss 2.148618698120117\n",
      "step 505 : loss 2.1789965629577637\n",
      "step 506 : loss 2.141641139984131\n",
      "step 507 : loss 2.1723554134368896\n",
      "step 508 : loss 2.1346991062164307\n",
      "step 509 : loss 2.165820837020874\n",
      "step 510 : loss 2.1278738975524902\n",
      "step 511 : loss 2.159318685531616\n",
      "step 512 : loss 2.121040105819702\n",
      "step 513 : loss 2.152916193008423\n",
      "step 514 : loss 2.114286184310913\n",
      "step 515 : loss 2.1465392112731934\n",
      "step 516 : loss 2.1075520515441895\n",
      "step 517 : loss 2.1401960849761963\n",
      "step 518 : loss 2.1009879112243652\n",
      "step 519 : loss 2.13391375541687\n",
      "step 520 : loss 2.094391345977783\n",
      "step 521 : loss 2.1276819705963135\n",
      "step 522 : loss 2.0878517627716064\n",
      "step 523 : loss 2.121515989303589\n",
      "step 524 : loss 2.081395387649536\n",
      "step 525 : loss 2.1153738498687744\n",
      "step 526 : loss 2.075031280517578\n",
      "step 527 : loss 2.1093485355377197\n",
      "step 528 : loss 2.0686516761779785\n",
      "step 529 : loss 2.1033177375793457\n",
      "step 530 : loss 2.0623371601104736\n",
      "step 531 : loss 2.0973544120788574\n",
      "step 532 : loss 2.056126594543457\n",
      "step 533 : loss 2.0914647579193115\n",
      "step 534 : loss 2.049905300140381\n",
      "step 535 : loss 2.0855939388275146\n",
      "step 536 : loss 2.0437355041503906\n",
      "step 537 : loss 2.0798189640045166\n",
      "step 538 : loss 2.0376439094543457\n",
      "step 539 : loss 2.0741074085235596\n",
      "step 540 : loss 2.0315887928009033\n",
      "step 541 : loss 2.068378448486328\n",
      "step 542 : loss 2.0255441665649414\n",
      "step 543 : loss 2.0627105236053467\n",
      "step 544 : loss 2.019617795944214\n",
      "step 545 : loss 2.057128667831421\n",
      "step 546 : loss 2.013697862625122\n",
      "step 547 : loss 2.0515618324279785\n",
      "step 548 : loss 2.007859468460083\n",
      "step 549 : loss 2.046062707901001\n",
      "step 550 : loss 2.0020639896392822\n",
      "step 551 : loss 2.0405771732330322\n",
      "step 552 : loss 1.9963207244873047\n",
      "step 553 : loss 2.0351474285125732\n",
      "step 554 : loss 1.9905487298965454\n",
      "step 555 : loss 2.0297911167144775\n",
      "step 556 : loss 1.9849467277526855\n",
      "step 557 : loss 2.0244383811950684\n",
      "step 558 : loss 1.97929847240448\n",
      "step 559 : loss 2.0191869735717773\n",
      "step 560 : loss 1.973734974861145\n",
      "step 561 : loss 2.0139482021331787\n",
      "step 562 : loss 1.9681651592254639\n",
      "step 563 : loss 2.00876784324646\n",
      "step 564 : loss 1.9626551866531372\n",
      "step 565 : loss 2.003653049468994\n",
      "step 566 : loss 1.9571630954742432\n",
      "step 567 : loss 1.9985634088516235\n",
      "step 568 : loss 1.9517533779144287\n",
      "step 569 : loss 1.9935176372528076\n",
      "step 570 : loss 1.9464023113250732\n",
      "step 571 : loss 1.9885064363479614\n",
      "step 572 : loss 1.94105064868927\n",
      "step 573 : loss 1.9835336208343506\n",
      "step 574 : loss 1.9357571601867676\n",
      "step 575 : loss 1.978599190711975\n",
      "step 576 : loss 1.9305261373519897\n",
      "step 577 : loss 1.973713994026184\n",
      "step 578 : loss 1.9253069162368774\n",
      "step 579 : loss 1.9688496589660645\n",
      "step 580 : loss 1.9201539754867554\n",
      "step 581 : loss 1.964086890220642\n",
      "step 582 : loss 1.915022611618042\n",
      "step 583 : loss 1.9592686891555786\n",
      "step 584 : loss 1.9099112749099731\n",
      "step 585 : loss 1.9545408487319946\n",
      "step 586 : loss 1.9048759937286377\n",
      "step 587 : loss 1.949859619140625\n",
      "step 588 : loss 1.8998758792877197\n",
      "step 589 : loss 1.9451903104782104\n",
      "step 590 : loss 1.8948886394500732\n",
      "step 591 : loss 1.940588116645813\n",
      "step 592 : loss 1.8899554014205933\n",
      "step 593 : loss 1.9360109567642212\n",
      "step 594 : loss 1.8850454092025757\n",
      "step 595 : loss 1.9314826726913452\n",
      "step 596 : loss 1.8801652193069458\n",
      "step 597 : loss 1.9269920587539673\n",
      "step 598 : loss 1.8753358125686646\n",
      "step 599 : loss 1.9225140810012817\n",
      "step 600 : loss 1.8705377578735352\n",
      "step 601 : loss 1.9180816411972046\n",
      "step 602 : loss 1.865837812423706\n",
      "step 603 : loss 1.913688063621521\n",
      "step 604 : loss 1.8611098527908325\n",
      "step 605 : loss 1.9093244075775146\n",
      "step 606 : loss 1.8564283847808838\n",
      "step 607 : loss 1.9050194025039673\n",
      "step 608 : loss 1.8517637252807617\n",
      "step 609 : loss 1.9007278680801392\n",
      "step 610 : loss 1.8471496105194092\n",
      "step 611 : loss 1.89647376537323\n",
      "step 612 : loss 1.842572569847107\n",
      "step 613 : loss 1.8922698497772217\n",
      "step 614 : loss 1.8380013704299927\n",
      "step 615 : loss 1.8880698680877686\n",
      "step 616 : loss 1.8335201740264893\n",
      "step 617 : loss 1.8839343786239624\n",
      "step 618 : loss 1.8290326595306396\n",
      "step 619 : loss 1.8797695636749268\n",
      "step 620 : loss 1.8245904445648193\n",
      "step 621 : loss 1.8756959438323975\n",
      "step 622 : loss 1.8201671838760376\n",
      "step 623 : loss 1.8716700077056885\n",
      "step 624 : loss 1.8157856464385986\n",
      "step 625 : loss 1.867641806602478\n",
      "step 626 : loss 1.8114150762557983\n",
      "step 627 : loss 1.8636844158172607\n",
      "step 628 : loss 1.8070827722549438\n",
      "step 629 : loss 1.8597222566604614\n",
      "step 630 : loss 1.802799940109253\n",
      "step 631 : loss 1.8557941913604736\n",
      "step 632 : loss 1.7984998226165771\n",
      "step 633 : loss 1.8519357442855835\n",
      "step 634 : loss 1.7942850589752197\n",
      "step 635 : loss 1.8480665683746338\n",
      "step 636 : loss 1.7900702953338623\n",
      "step 637 : loss 1.844207763671875\n",
      "step 638 : loss 1.7859077453613281\n",
      "step 639 : loss 1.8404128551483154\n",
      "step 640 : loss 1.7817281484603882\n",
      "step 641 : loss 1.8366203308105469\n",
      "step 642 : loss 1.7776498794555664\n",
      "step 643 : loss 1.8329006433486938\n",
      "step 644 : loss 1.7735497951507568\n",
      "step 645 : loss 1.8291728496551514\n",
      "step 646 : loss 1.7694915533065796\n",
      "step 647 : loss 1.825495958328247\n",
      "step 648 : loss 1.7654775381088257\n",
      "step 649 : loss 1.8218393325805664\n",
      "step 650 : loss 1.7614578008651733\n",
      "step 651 : loss 1.818177580833435\n",
      "step 652 : loss 1.757511019706726\n",
      "step 653 : loss 1.8145782947540283\n",
      "step 654 : loss 1.7535600662231445\n",
      "step 655 : loss 1.8110241889953613\n",
      "step 656 : loss 1.7496137619018555\n",
      "step 657 : loss 1.8074949979782104\n",
      "step 658 : loss 1.7457373142242432\n",
      "step 659 : loss 1.8039772510528564\n",
      "step 660 : loss 1.741838812828064\n",
      "step 661 : loss 1.8004709482192993\n",
      "step 662 : loss 1.738021731376648\n",
      "step 663 : loss 1.796983242034912\n",
      "step 664 : loss 1.734255075454712\n",
      "step 665 : loss 1.7935477495193481\n",
      "step 666 : loss 1.730439305305481\n",
      "step 667 : loss 1.7901400327682495\n",
      "step 668 : loss 1.726678729057312\n",
      "step 669 : loss 1.786766767501831\n",
      "step 670 : loss 1.72292959690094\n",
      "step 671 : loss 1.7834240198135376\n",
      "step 672 : loss 1.7192189693450928\n",
      "step 673 : loss 1.7800779342651367\n",
      "step 674 : loss 1.7154994010925293\n",
      "step 675 : loss 1.776807427406311\n",
      "step 676 : loss 1.7118420600891113\n",
      "step 677 : loss 1.7735146284103394\n",
      "step 678 : loss 1.7082034349441528\n",
      "step 679 : loss 1.7702287435531616\n",
      "step 680 : loss 1.7046089172363281\n",
      "step 681 : loss 1.766996145248413\n",
      "step 682 : loss 1.7010339498519897\n",
      "step 683 : loss 1.7637908458709717\n",
      "step 684 : loss 1.6974551677703857\n",
      "step 685 : loss 1.7606086730957031\n",
      "step 686 : loss 1.6939128637313843\n",
      "step 687 : loss 1.7574119567871094\n",
      "step 688 : loss 1.6904056072235107\n",
      "step 689 : loss 1.754269003868103\n",
      "step 690 : loss 1.6869182586669922\n",
      "step 691 : loss 1.7511595487594604\n",
      "step 692 : loss 1.6834715604782104\n",
      "step 693 : loss 1.748070240020752\n",
      "step 694 : loss 1.679982304573059\n",
      "step 695 : loss 1.7449896335601807\n",
      "step 696 : loss 1.67657470703125\n",
      "step 697 : loss 1.741973638534546\n",
      "step 698 : loss 1.6731420755386353\n",
      "step 699 : loss 1.7389326095581055\n",
      "step 700 : loss 1.6697356700897217\n",
      "step 701 : loss 1.73594331741333\n",
      "step 702 : loss 1.6663930416107178\n",
      "step 703 : loss 1.732998013496399\n",
      "step 704 : loss 1.6630332469940186\n",
      "step 705 : loss 1.730053424835205\n",
      "step 706 : loss 1.659691333770752\n",
      "step 707 : loss 1.727095127105713\n",
      "step 708 : loss 1.656410574913025\n",
      "step 709 : loss 1.7242182493209839\n",
      "step 710 : loss 1.6530773639678955\n",
      "step 711 : loss 1.7213399410247803\n",
      "step 712 : loss 1.6498510837554932\n",
      "step 713 : loss 1.7184720039367676\n",
      "step 714 : loss 1.646593689918518\n",
      "step 715 : loss 1.7156386375427246\n",
      "step 716 : loss 1.643357515335083\n",
      "step 717 : loss 1.7127734422683716\n",
      "step 718 : loss 1.6402270793914795\n",
      "step 719 : loss 1.7099124193191528\n",
      "step 720 : loss 1.6370830535888672\n",
      "step 721 : loss 1.7071231603622437\n",
      "step 722 : loss 1.6339315176010132\n",
      "step 723 : loss 1.7043514251708984\n",
      "step 724 : loss 1.6307969093322754\n",
      "step 725 : loss 1.7016162872314453\n",
      "step 726 : loss 1.6276835203170776\n",
      "step 727 : loss 1.6988751888275146\n",
      "step 728 : loss 1.6245776414871216\n",
      "step 729 : loss 1.696184754371643\n",
      "step 730 : loss 1.6215038299560547\n",
      "step 731 : loss 1.6935235261917114\n",
      "step 732 : loss 1.6184097528457642\n",
      "step 733 : loss 1.690852165222168\n",
      "step 734 : loss 1.615357756614685\n",
      "step 735 : loss 1.688210368156433\n",
      "step 736 : loss 1.612343668937683\n",
      "step 737 : loss 1.685598611831665\n",
      "step 738 : loss 1.6093195676803589\n",
      "step 739 : loss 1.6830002069473267\n",
      "step 740 : loss 1.6063281297683716\n",
      "step 741 : loss 1.680395245552063\n",
      "step 742 : loss 1.6033700704574585\n",
      "step 743 : loss 1.6778489351272583\n",
      "step 744 : loss 1.6003977060317993\n",
      "step 745 : loss 1.6752909421920776\n",
      "step 746 : loss 1.5974397659301758\n",
      "step 747 : loss 1.672798752784729\n",
      "step 748 : loss 1.5944987535476685\n",
      "step 749 : loss 1.6702756881713867\n",
      "step 750 : loss 1.5916074514389038\n",
      "step 751 : loss 1.6677769422531128\n",
      "step 752 : loss 1.588692545890808\n",
      "step 753 : loss 1.6653262376785278\n",
      "step 754 : loss 1.5858498811721802\n",
      "step 755 : loss 1.6628299951553345\n",
      "step 756 : loss 1.5829999446868896\n",
      "step 757 : loss 1.6603807210922241\n",
      "step 758 : loss 1.5801700353622437\n",
      "step 759 : loss 1.6579428911209106\n",
      "step 760 : loss 1.5773802995681763\n",
      "step 761 : loss 1.6555157899856567\n",
      "step 762 : loss 1.574615240097046\n",
      "step 763 : loss 1.653145432472229\n",
      "step 764 : loss 1.5718001127243042\n",
      "step 765 : loss 1.6507717370986938\n",
      "step 766 : loss 1.569040298461914\n",
      "step 767 : loss 1.6484215259552002\n",
      "step 768 : loss 1.5663100481033325\n",
      "step 769 : loss 1.6460256576538086\n",
      "step 770 : loss 1.5636101961135864\n",
      "step 771 : loss 1.6437190771102905\n",
      "step 772 : loss 1.560893177986145\n",
      "step 773 : loss 1.6414198875427246\n",
      "step 774 : loss 1.5581589937210083\n",
      "step 775 : loss 1.6391103267669678\n",
      "step 776 : loss 1.5554624795913696\n",
      "step 777 : loss 1.6368465423583984\n",
      "step 778 : loss 1.5528028011322021\n",
      "step 779 : loss 1.6345698833465576\n",
      "step 780 : loss 1.5501489639282227\n",
      "step 781 : loss 1.63236665725708\n",
      "step 782 : loss 1.547512173652649\n",
      "step 783 : loss 1.6301100254058838\n",
      "step 784 : loss 1.544846773147583\n",
      "step 785 : loss 1.6279271841049194\n",
      "step 786 : loss 1.5422487258911133\n",
      "step 787 : loss 1.6257250308990479\n",
      "step 788 : loss 1.5396679639816284\n",
      "step 789 : loss 1.6235461235046387\n",
      "step 790 : loss 1.5370577573776245\n",
      "step 791 : loss 1.6213995218276978\n",
      "step 792 : loss 1.5344842672348022\n",
      "step 793 : loss 1.619255542755127\n",
      "step 794 : loss 1.5319267511367798\n",
      "step 795 : loss 1.617114782333374\n",
      "step 796 : loss 1.5293821096420288\n",
      "step 797 : loss 1.614990472793579\n",
      "step 798 : loss 1.5268491506576538\n",
      "step 799 : loss 1.6128689050674438\n",
      "step 800 : loss 1.5243414640426636\n",
      "step 801 : loss 1.6108121871948242\n",
      "step 802 : loss 1.521851897239685\n",
      "step 803 : loss 1.608762264251709\n",
      "step 804 : loss 1.5193110704421997\n",
      "step 805 : loss 1.6066973209381104\n",
      "step 806 : loss 1.516871690750122\n",
      "step 807 : loss 1.6046233177185059\n",
      "step 808 : loss 1.514451026916504\n",
      "step 809 : loss 1.6025580167770386\n",
      "step 810 : loss 1.5120190382003784\n",
      "step 811 : loss 1.6005163192749023\n",
      "step 812 : loss 1.5096006393432617\n",
      "step 813 : loss 1.5985093116760254\n",
      "step 814 : loss 1.5071961879730225\n",
      "step 815 : loss 1.5965486764907837\n",
      "step 816 : loss 1.504758596420288\n",
      "step 817 : loss 1.5945638418197632\n",
      "step 818 : loss 1.5023839473724365\n",
      "step 819 : loss 1.5925990343093872\n",
      "step 820 : loss 1.5000063180923462\n",
      "step 821 : loss 1.5906506776809692\n",
      "step 822 : loss 1.4976516962051392\n",
      "step 823 : loss 1.588720679283142\n",
      "step 824 : loss 1.495261311531067\n",
      "step 825 : loss 1.5868054628372192\n",
      "step 826 : loss 1.4929156303405762\n",
      "step 827 : loss 1.58491849899292\n",
      "step 828 : loss 1.4905706644058228\n",
      "step 829 : loss 1.583014965057373\n",
      "step 830 : loss 1.4882680177688599\n",
      "step 831 : loss 1.5811079740524292\n",
      "step 832 : loss 1.4859758615493774\n",
      "step 833 : loss 1.5792236328125\n",
      "step 834 : loss 1.4837323427200317\n",
      "step 835 : loss 1.577351450920105\n",
      "step 836 : loss 1.4814401865005493\n",
      "step 837 : loss 1.5755003690719604\n",
      "step 838 : loss 1.4791767597198486\n",
      "step 839 : loss 1.573662519454956\n",
      "step 840 : loss 1.4769350290298462\n",
      "step 841 : loss 1.5718426704406738\n",
      "step 842 : loss 1.4746681451797485\n",
      "step 843 : loss 1.5700435638427734\n",
      "step 844 : loss 1.472426414489746\n",
      "step 845 : loss 1.5682507753372192\n",
      "step 846 : loss 1.4701783657073975\n",
      "step 847 : loss 1.56648850440979\n",
      "step 848 : loss 1.467980980873108\n",
      "step 849 : loss 1.5647224187850952\n",
      "step 850 : loss 1.4658100605010986\n",
      "step 851 : loss 1.5629631280899048\n",
      "step 852 : loss 1.4636141061782837\n",
      "step 853 : loss 1.5612077713012695\n",
      "step 854 : loss 1.4614241123199463\n",
      "step 855 : loss 1.5594865083694458\n",
      "step 856 : loss 1.4592264890670776\n",
      "step 857 : loss 1.5577757358551025\n",
      "step 858 : loss 1.4570791721343994\n",
      "step 859 : loss 1.556053638458252\n",
      "step 860 : loss 1.4549381732940674\n",
      "step 861 : loss 1.5543510913848877\n",
      "step 862 : loss 1.452796220779419\n",
      "step 863 : loss 1.5526844263076782\n",
      "step 864 : loss 1.450674295425415\n",
      "step 865 : loss 1.5510002374649048\n",
      "step 866 : loss 1.4485433101654053\n",
      "step 867 : loss 1.5493375062942505\n",
      "step 868 : loss 1.4464526176452637\n",
      "step 869 : loss 1.5476912260055542\n",
      "step 870 : loss 1.4443564414978027\n",
      "step 871 : loss 1.5460301637649536\n",
      "step 872 : loss 1.4423004388809204\n",
      "step 873 : loss 1.5443388223648071\n",
      "step 874 : loss 1.440276026725769\n",
      "step 875 : loss 1.5427356958389282\n",
      "step 876 : loss 1.4382026195526123\n",
      "step 877 : loss 1.541098713874817\n",
      "step 878 : loss 1.4361547231674194\n",
      "step 879 : loss 1.539502501487732\n",
      "step 880 : loss 1.4341529607772827\n",
      "step 881 : loss 1.537873387336731\n",
      "step 882 : loss 1.432119607925415\n",
      "step 883 : loss 1.5363187789916992\n",
      "step 884 : loss 1.430107831954956\n",
      "step 885 : loss 1.5347492694854736\n",
      "step 886 : loss 1.4280844926834106\n",
      "step 887 : loss 1.533207654953003\n",
      "step 888 : loss 1.426078200340271\n",
      "step 889 : loss 1.5316554307937622\n",
      "step 890 : loss 1.4240854978561401\n",
      "step 891 : loss 1.530128836631775\n",
      "step 892 : loss 1.42207932472229\n",
      "step 893 : loss 1.5286043882369995\n",
      "step 894 : loss 1.4200688600540161\n",
      "step 895 : loss 1.5271220207214355\n",
      "step 896 : loss 1.418149709701538\n",
      "step 897 : loss 1.5255907773971558\n",
      "step 898 : loss 1.4161947965621948\n",
      "step 899 : loss 1.5241035223007202\n",
      "step 900 : loss 1.414223074913025\n",
      "step 901 : loss 1.5226109027862549\n",
      "step 902 : loss 1.412302851676941\n",
      "step 903 : loss 1.5211509466171265\n",
      "step 904 : loss 1.4103708267211914\n",
      "step 905 : loss 1.5196880102157593\n",
      "step 906 : loss 1.4084185361862183\n",
      "step 907 : loss 1.5182570219039917\n",
      "step 908 : loss 1.406526803970337\n",
      "step 909 : loss 1.516805648803711\n",
      "step 910 : loss 1.4045988321304321\n",
      "step 911 : loss 1.5153769254684448\n",
      "step 912 : loss 1.4027271270751953\n",
      "step 913 : loss 1.5139520168304443\n",
      "step 914 : loss 1.4008272886276245\n",
      "step 915 : loss 1.5125311613082886\n",
      "step 916 : loss 1.3989531993865967\n",
      "step 917 : loss 1.5111151933670044\n",
      "step 918 : loss 1.397100567817688\n",
      "step 919 : loss 1.5097074508666992\n",
      "step 920 : loss 1.3952479362487793\n",
      "step 921 : loss 1.5082998275756836\n",
      "step 922 : loss 1.3934377431869507\n",
      "step 923 : loss 1.5068843364715576\n",
      "step 924 : loss 1.3916274309158325\n",
      "step 925 : loss 1.505467176437378\n",
      "step 926 : loss 1.3898072242736816\n",
      "step 927 : loss 1.5041183233261108\n",
      "step 928 : loss 1.3879915475845337\n",
      "step 929 : loss 1.5027722120285034\n",
      "step 930 : loss 1.386183738708496\n",
      "step 931 : loss 1.5014129877090454\n",
      "step 932 : loss 1.3843683004379272\n",
      "step 933 : loss 1.5000885725021362\n",
      "step 934 : loss 1.3825784921646118\n",
      "step 935 : loss 1.4987019300460815\n",
      "step 936 : loss 1.380805253982544\n",
      "step 937 : loss 1.4974254369735718\n",
      "step 938 : loss 1.3789907693862915\n",
      "step 939 : loss 1.4961236715316772\n",
      "step 940 : loss 1.3771981000900269\n",
      "step 941 : loss 1.4948468208312988\n",
      "step 942 : loss 1.375433325767517\n",
      "step 943 : loss 1.4935646057128906\n",
      "step 944 : loss 1.3736625909805298\n",
      "step 945 : loss 1.492282509803772\n",
      "step 946 : loss 1.3719291687011719\n",
      "step 947 : loss 1.491007685661316\n",
      "step 948 : loss 1.3701425790786743\n",
      "step 949 : loss 1.4897319078445435\n",
      "step 950 : loss 1.3684364557266235\n",
      "step 951 : loss 1.4884549379348755\n",
      "step 952 : loss 1.3666884899139404\n",
      "step 953 : loss 1.487200140953064\n",
      "step 954 : loss 1.3649921417236328\n",
      "step 955 : loss 1.485959768295288\n",
      "step 956 : loss 1.3632960319519043\n",
      "step 957 : loss 1.4847294092178345\n",
      "step 958 : loss 1.3615633249282837\n",
      "step 959 : loss 1.4835001230239868\n",
      "step 960 : loss 1.359876036643982\n",
      "step 961 : loss 1.4822722673416138\n",
      "step 962 : loss 1.3581879138946533\n",
      "step 963 : loss 1.4810600280761719\n",
      "step 964 : loss 1.3564765453338623\n",
      "step 965 : loss 1.4798506498336792\n",
      "step 966 : loss 1.3548558950424194\n",
      "step 967 : loss 1.4786373376846313\n",
      "step 968 : loss 1.3531816005706787\n",
      "step 969 : loss 1.4774802923202515\n",
      "step 970 : loss 1.3515198230743408\n",
      "step 971 : loss 1.4762465953826904\n",
      "step 972 : loss 1.3498578071594238\n",
      "step 973 : loss 1.4750938415527344\n",
      "step 974 : loss 1.3481982946395874\n",
      "step 975 : loss 1.4739444255828857\n",
      "step 976 : loss 1.3465709686279297\n",
      "step 977 : loss 1.472739577293396\n",
      "step 978 : loss 1.3449337482452393\n",
      "step 979 : loss 1.4715827703475952\n",
      "step 980 : loss 1.3433576822280884\n",
      "step 981 : loss 1.4704324007034302\n",
      "step 982 : loss 1.3417574167251587\n",
      "step 983 : loss 1.4692943096160889\n",
      "step 984 : loss 1.340134859085083\n",
      "step 985 : loss 1.4681484699249268\n",
      "step 986 : loss 1.3385138511657715\n",
      "step 987 : loss 1.4670180082321167\n",
      "step 988 : loss 1.3369371891021729\n",
      "step 989 : loss 1.4658942222595215\n",
      "step 990 : loss 1.335360050201416\n",
      "step 991 : loss 1.464779257774353\n",
      "step 992 : loss 1.333767056465149\n",
      "step 993 : loss 1.4637067317962646\n",
      "step 994 : loss 1.332190752029419\n",
      "step 995 : loss 1.462591528892517\n",
      "step 996 : loss 1.3306013345718384\n",
      "step 997 : loss 1.4614918231964111\n",
      "step 998 : loss 1.3290263414382935\n",
      "step 999 : loss 1.460400104522705\n"
     ]
    }
   ],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.SGD(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(init_tensor)\n",
    "    loss = (y-init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAC70lEQVR4Ae2d0W6iUAAFpaH7/3+73T7oAjbbMSWupnonaca05nARTjvjBeHF6fd0WB6vp/V5ix/P6/Ll47z2cqylTwIbws/FJX0d4eo/28ILh8rjCUzHaX1nT6frtsb/YT+/8bj9i80A2XQCEiATkOubAQmQCcj1c5/uLQNn8h2CLP4fvQlIgExArm8GJEAmINfP/7llJ/95P7++Q5Ds+GW9/d+1gGehGeCx35o7B2gCuhLW0LO4QxBpCDkBAnRWJoA0hJwAATorE0AaQk6AAJ2VXQmThpCbAQJ0ViaANIScAAE6KxNAGkJOgACdlQkgDSEnQIDOygSQhpATIEBnZQJIQ8gJEKCzMgGkIeQECNBZmQDSEHICBOisTABpCDkBAnRWJoA0hJwAATorE0AaQk6AAJ2VCSANISdAgM7KBJCGkBMgQGdlAkhDyAkQoLMyAaQh5AQI0FmZANIQcgIE6KxMAGkIOQECdFYmgDSEnAABOisTQBpCToAAnZUJIA0hJ0CAzsoEkIaQEyBAZ2UCSEPICRCgszIBpCHkBAjQWZkA0hByAgTorEwAaQg5AQJ0ViaANIScAAE6KxNAGkJOgACdlQkgDSEnQIDOygSQhpATIEBnZQJIQ8gJEKCzMgGkIeQECNBZmQDSEHICBOisTABpCDkBAnRWJoA0hJwAATorE0AaQk6AAJ2VCSANISdAgM7KBJCGkBMgQGdl36hNGkJuBgjQWdl3SZKGkJsBAnRWJoA0hJwAATorZy6URxI4bWXNgJHMd7oSsANl5FACRtLe6epKeAfKyKFmwEjaO10J2IEycigBI2nvdCVgB8rIoQSMpL3TtVwJn6/IdtY1NIBAM2AA5GsVi4Bp+7n2otY9j0Az4Hlsb9pzd0NvwvSMF3U39BlU795nn4LuRvbYDToHPJbn3XubT+unoOW3q4G72X1zg84B3wT4mM3nt8N6FPp1OP7b3zojLh+3jFxu0dJKgEeVr/l9w9o5QH6v/AUxwRsD8JfnNQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(autoencoder(init_tensor).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(init_tensor.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_images(size, num_images, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(1, num_images + 1):\n",
    "        # Create a new image with a random background color\n",
    "        img = Image.new(\"RGB\", size, color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "        # Get a drawing context\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Choose a random shape (circle, square, or triangle)\n",
    "        # shape = random.choice([\"circle\", \"square\", \"triangle\"])\n",
    "        shape = \"circle\"\n",
    "\n",
    "        # Choose a random position\n",
    "        position = (random.randint(20, size[0]-20), random.randint(20, size[1]-20))\n",
    "\n",
    "        # Choose a random color for the shape\n",
    "        shape_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        # Draw the shape on the image\n",
    "        if shape == \"circle\":\n",
    "            draw.ellipse([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"square\":\n",
    "            draw.rectangle([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "        elif shape == \"triangle\":\n",
    "            draw.polygon([(position[0], position[1]-20), (position[0]-20, position[1]+20), (position[0]+20, position[1]+20)], fill=shape_color)\n",
    "\n",
    "        # Save the image to the output folder\n",
    "        img.save(os.path.join(output_folder, f\"image_{i}.png\"))\n",
    "\n",
    "# Example usage\n",
    "generate_images((128, 128), 100, \"../data/dataset_train\")\n",
    "generate_images((128, 128), 20, \"../data/dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, path) -> None:\n",
    "        self.data = list(range(100))\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Dataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "    \n",
    "    def __getitem__(self, key : str|int) -> int:\n",
    "        match key:\n",
    "            case key if isinstance(key, str):\n",
    "                raise ValueError('Dataset does not take string as index.')\n",
    "            case _:\n",
    "                return self.data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageDataset:\n",
    "    def __init__(self, path) -> None:\n",
    "        self.path = path\n",
    "        self.image_files = [f for f in os.listdir(path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.data = [self.load_image(file) for file in self.image_files]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'ImageDataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __getitem__(self, key: int) -> Image.Image:\n",
    "        return self.data[key]\n",
    "\n",
    "    def load_image(self, file: str) -> Image.Image:\n",
    "        image_path = os.path.join(self.path, file)\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image '{file}': {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 121.1850357055664\n",
      "step 1 : loss 120.30896759033203\n",
      "step 2 : loss 119.3756332397461\n",
      "step 3 : loss 118.31941223144531\n",
      "step 4 : loss 117.07115936279297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/laure/vton/laure_f/autoencoder.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     loss_iter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_iter\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(dataset_train)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvton/home/laure/vton/laure_f/autoencoder.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/vton/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/vton/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_dataset_train = \"../data/dataset_train/\"\n",
    "path_dataset_test = \"../data/dataset_test/\"\n",
    "dataset_train = ImageDataset(path_dataset_train).data\n",
    "dataset_test = ImageDataset(path_dataset_test).data\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.train()\n",
    "lr = 1e-4\n",
    "num_steps = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    loss_iter = 0\n",
    "    for image in dataset_train:\n",
    "        image = image_to_tensor(image)\n",
    "        y = autoencoder(image)\n",
    "        loss = (y-image).norm()\n",
    "        loss_iter += loss\n",
    "    loss = loss_iter/len(dataset_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:88mswdn4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr><tr><td>step</td><td></td></tr><tr><td>test_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>111.20601</td></tr><tr><td>step</td><td>9</td></tr><tr><td>test_loss</td><td>113.12148</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-smoke-20</strong> at: <a href='https://wandb.ai/finegrain-cs/finegrain-cs/runs/88mswdn4' target=\"_blank\">https://wandb.ai/finegrain-cs/finegrain-cs/runs/88mswdn4</a><br/>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231116_103514-88mswdn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:88mswdn4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/julen/vton/julen/wandb/run-20231116_103757-n87e3w04</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/finegrain-cs/finegrain-cs/runs/n87e3w04' target=\"_blank\">1000_step_100_img</a></strong> to <a href='https://wandb.ai/finegrain-cs/finegrain-cs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/finegrain-cs/finegrain-cs' target=\"_blank\">https://wandb.ai/finegrain-cs/finegrain-cs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/finegrain-cs/finegrain-cs/runs/n87e3w04' target=\"_blank\">https://wandb.ai/finegrain-cs/finegrain-cs/runs/n87e3w04</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 127.8471908569336\n",
      "step 1 : loss 126.98172760009766\n",
      "step 2 : loss 126.02904510498047\n",
      "step 3 : loss 124.91508483886719\n",
      "step 4 : loss 123.57235717773438\n",
      "step 5 : loss 121.93327331542969\n",
      "step 6 : loss 119.92459106445312\n",
      "step 7 : loss 117.46563720703125\n",
      "step 8 : loss 114.46715545654297\n",
      "step 9 : loss 110.8344955444336\n",
      "step 10 : loss 106.47940063476562\n",
      "step 11 : loss 101.34246063232422\n",
      "step 12 : loss 95.4439468383789\n",
      "step 13 : loss 88.98930358886719\n",
      "step 14 : loss 82.57461547851562\n",
      "step 15 : loss 77.53211212158203\n",
      "step 16 : loss 75.98564147949219\n",
      "step 17 : loss 78.50843048095703\n",
      "step 18 : loss 80.99042510986328\n",
      "step 19 : loss 80.5259780883789\n",
      "step 20 : loss 78.02440643310547\n",
      "step 21 : loss 75.19908905029297\n",
      "step 22 : loss 73.1849594116211\n",
      "step 23 : loss 72.30620574951172\n",
      "step 24 : loss 72.31853485107422\n",
      "step 25 : loss 72.77925109863281\n",
      "step 26 : loss 73.30518341064453\n",
      "step 27 : loss 73.65884399414062\n",
      "step 28 : loss 73.7333755493164\n",
      "step 29 : loss 73.5112075805664\n",
      "step 30 : loss 73.02991485595703\n",
      "step 31 : loss 72.36077117919922\n",
      "step 32 : loss 71.59542083740234\n",
      "step 33 : loss 70.83423614501953\n",
      "step 34 : loss 70.17149353027344\n",
      "step 35 : loss 69.67530059814453\n",
      "step 36 : loss 69.36608123779297\n",
      "step 37 : loss 69.20472717285156\n",
      "step 38 : loss 69.10310363769531\n",
      "step 39 : loss 68.96078491210938\n",
      "step 40 : loss 68.71089935302734\n",
      "step 41 : loss 68.3472671508789\n",
      "step 42 : loss 67.91796875\n",
      "step 43 : loss 67.49281311035156\n",
      "step 44 : loss 67.12645721435547\n",
      "step 45 : loss 66.83634185791016\n",
      "step 46 : loss 66.60064697265625\n",
      "step 47 : loss 66.37115478515625\n",
      "step 48 : loss 66.09304809570312\n",
      "step 49 : loss 65.72380828857422\n",
      "step 50 : loss 65.24906158447266\n",
      "step 51 : loss 64.69511413574219\n",
      "step 52 : loss 64.13540649414062\n",
      "step 53 : loss 63.67593765258789\n",
      "step 54 : loss 63.37909698486328\n",
      "step 55 : loss 63.11099624633789\n",
      "step 56 : loss 62.570404052734375\n",
      "step 57 : loss 61.79511260986328\n",
      "step 58 : loss 61.191062927246094\n",
      "step 59 : loss 60.79970169067383\n",
      "step 60 : loss 60.30564880371094\n",
      "step 61 : loss 59.76759338378906\n",
      "step 62 : loss 59.7066535949707\n",
      "step 63 : loss 59.496368408203125\n",
      "step 64 : loss 59.197940826416016\n",
      "step 65 : loss 59.241458892822266\n",
      "step 66 : loss 58.92363357543945\n",
      "step 67 : loss 58.83224105834961\n",
      "step 68 : loss 58.673030853271484\n",
      "step 69 : loss 58.37001419067383\n",
      "step 70 : loss 58.22699737548828\n",
      "step 71 : loss 57.797119140625\n",
      "step 72 : loss 57.564510345458984\n",
      "step 73 : loss 57.21815872192383\n",
      "step 74 : loss 56.906646728515625\n",
      "step 75 : loss 56.6943244934082\n",
      "step 76 : loss 56.375850677490234\n",
      "step 77 : loss 56.19093704223633\n",
      "step 78 : loss 55.940372467041016\n",
      "step 79 : loss 55.701663970947266\n",
      "step 80 : loss 55.52777862548828\n",
      "step 81 : loss 55.28028869628906\n",
      "step 82 : loss 55.10079574584961\n",
      "step 83 : loss 54.86083984375\n",
      "step 84 : loss 54.62815856933594\n",
      "step 85 : loss 54.406497955322266\n",
      "step 86 : loss 54.15436553955078\n",
      "step 87 : loss 53.95169448852539\n",
      "step 88 : loss 53.712894439697266\n",
      "step 89 : loss 53.527061462402344\n",
      "step 90 : loss 53.304725646972656\n",
      "step 91 : loss 53.116371154785156\n",
      "step 92 : loss 52.93544006347656\n",
      "step 93 : loss 52.735939025878906\n",
      "step 94 : loss 52.57854461669922\n",
      "step 95 : loss 52.402584075927734\n",
      "step 96 : loss 52.22291564941406\n",
      "step 97 : loss 52.087615966796875\n",
      "step 98 : loss 51.97739791870117\n",
      "step 99 : loss 51.858741760253906\n",
      "step 100 : loss 51.737709045410156\n",
      "step 101 : loss 51.61626434326172\n",
      "step 102 : loss 51.49945831298828\n",
      "step 103 : loss 51.38380432128906\n",
      "step 104 : loss 51.27494812011719\n",
      "step 105 : loss 51.176544189453125\n",
      "step 106 : loss 51.09624099731445\n",
      "step 107 : loss 51.067108154296875\n",
      "step 108 : loss 51.10493469238281\n",
      "step 109 : loss 51.142494201660156\n",
      "step 110 : loss 50.851016998291016\n",
      "step 111 : loss 50.698787689208984\n",
      "step 112 : loss 50.77827453613281\n",
      "step 113 : loss 50.637184143066406\n",
      "step 114 : loss 50.48774337768555\n",
      "step 115 : loss 50.519954681396484\n",
      "step 116 : loss 50.41850280761719\n",
      "step 117 : loss 50.31563949584961\n",
      "step 118 : loss 50.322227478027344\n",
      "step 119 : loss 50.227882385253906\n",
      "step 120 : loss 50.14906311035156\n",
      "step 121 : loss 50.133182525634766\n",
      "step 122 : loss 50.04047393798828\n",
      "step 123 : loss 49.96491622924805\n",
      "step 124 : loss 49.931884765625\n",
      "step 125 : loss 49.84480285644531\n",
      "step 126 : loss 49.757171630859375\n",
      "step 127 : loss 49.704559326171875\n",
      "step 128 : loss 49.624900817871094\n",
      "step 129 : loss 49.519065856933594\n",
      "step 130 : loss 49.427154541015625\n",
      "step 131 : loss 49.33928298950195\n",
      "step 132 : loss 49.224063873291016\n",
      "step 133 : loss 49.07966613769531\n",
      "step 134 : loss 48.928409576416016\n",
      "step 135 : loss 48.767127990722656\n",
      "step 136 : loss 48.57322692871094\n",
      "step 137 : loss 48.33466339111328\n",
      "step 138 : loss 48.035919189453125\n",
      "step 139 : loss 47.675865173339844\n",
      "step 140 : loss 47.2330436706543\n",
      "step 141 : loss 46.686363220214844\n",
      "step 142 : loss 45.99995422363281\n",
      "step 143 : loss 45.14152526855469\n",
      "step 144 : loss 44.08705520629883\n",
      "step 145 : loss 42.860294342041016\n",
      "step 146 : loss 42.2911262512207\n",
      "step 147 : loss 47.88886642456055\n",
      "step 148 : loss 41.6656494140625\n",
      "step 149 : loss 41.719703674316406\n",
      "step 150 : loss 44.11797332763672\n",
      "step 151 : loss 41.07791519165039\n",
      "step 152 : loss 44.19233322143555\n",
      "step 153 : loss 40.820980072021484\n",
      "step 154 : loss 41.6489372253418\n",
      "step 155 : loss 41.001808166503906\n",
      "step 156 : loss 40.62112045288086\n",
      "step 157 : loss 40.444313049316406\n",
      "step 158 : loss 40.67683410644531\n",
      "step 159 : loss 39.400936126708984\n",
      "step 160 : loss 40.719207763671875\n",
      "step 161 : loss 39.1662712097168\n",
      "step 162 : loss 39.79465866088867\n",
      "step 163 : loss 39.79374313354492\n",
      "step 164 : loss 38.69313049316406\n",
      "step 165 : loss 39.60517883300781\n",
      "step 166 : loss 38.723411560058594\n",
      "step 167 : loss 38.81859588623047\n",
      "step 168 : loss 38.87272262573242\n",
      "step 169 : loss 38.42658615112305\n",
      "step 170 : loss 38.462589263916016\n",
      "step 171 : loss 38.41572189331055\n",
      "step 172 : loss 38.105316162109375\n",
      "step 173 : loss 38.201568603515625\n",
      "step 174 : loss 38.05647277832031\n",
      "step 175 : loss 37.83721923828125\n",
      "step 176 : loss 38.010433197021484\n",
      "step 177 : loss 37.635196685791016\n",
      "step 178 : loss 37.79743576049805\n",
      "step 179 : loss 37.590030670166016\n",
      "step 180 : loss 37.500850677490234\n",
      "step 181 : loss 37.53679656982422\n",
      "step 182 : loss 37.28254699707031\n",
      "step 183 : loss 37.396018981933594\n",
      "step 184 : loss 37.175682067871094\n",
      "step 185 : loss 37.19025802612305\n",
      "step 186 : loss 37.0979118347168\n",
      "step 187 : loss 36.99807357788086\n",
      "step 188 : loss 36.98563766479492\n",
      "step 189 : loss 36.84968566894531\n",
      "step 190 : loss 36.836570739746094\n",
      "step 191 : loss 36.73664474487305\n",
      "step 192 : loss 36.6830940246582\n",
      "step 193 : loss 36.633506774902344\n",
      "step 194 : loss 36.54545593261719\n",
      "step 195 : loss 36.51849365234375\n",
      "step 196 : loss 36.42601013183594\n",
      "step 197 : loss 36.39088439941406\n",
      "step 198 : loss 36.31837463378906\n",
      "step 199 : loss 36.26258087158203\n",
      "step 200 : loss 36.21409225463867\n",
      "step 201 : loss 36.143028259277344\n",
      "step 202 : loss 36.10981750488281\n",
      "step 203 : loss 36.033050537109375\n",
      "step 204 : loss 36.004764556884766\n",
      "step 205 : loss 35.93109130859375\n",
      "step 206 : loss 35.899253845214844\n",
      "step 207 : loss 35.834312438964844\n",
      "step 208 : loss 35.79674530029297\n",
      "step 209 : loss 35.739810943603516\n",
      "step 210 : loss 35.697486877441406\n",
      "step 211 : loss 35.647220611572266\n",
      "step 212 : loss 35.60211181640625\n",
      "step 213 : loss 35.555912017822266\n",
      "step 214 : loss 35.5098762512207\n",
      "step 215 : loss 35.465816497802734\n",
      "step 216 : loss 35.419857025146484\n",
      "step 217 : loss 35.37702178955078\n",
      "step 218 : loss 35.33170700073242\n",
      "step 219 : loss 35.28968811035156\n",
      "step 220 : loss 35.245277404785156\n",
      "step 221 : loss 35.20400619506836\n",
      "step 222 : loss 35.16006088256836\n",
      "step 223 : loss 35.119232177734375\n",
      "step 224 : loss 35.075565338134766\n",
      "step 225 : loss 35.03535461425781\n",
      "step 226 : loss 34.991943359375\n",
      "step 227 : loss 34.952274322509766\n",
      "step 228 : loss 34.90924835205078\n",
      "step 229 : loss 34.869956970214844\n",
      "step 230 : loss 34.827415466308594\n",
      "step 231 : loss 34.78815460205078\n",
      "step 232 : loss 34.74633026123047\n",
      "step 233 : loss 34.70698165893555\n",
      "step 234 : loss 34.66594314575195\n",
      "step 235 : loss 34.626468658447266\n",
      "step 236 : loss 34.586158752441406\n",
      "step 237 : loss 34.546730041503906\n",
      "step 238 : loss 34.50686264038086\n",
      "step 239 : loss 34.4677619934082\n",
      "step 240 : loss 34.42821502685547\n",
      "step 241 : loss 34.38963317871094\n",
      "step 242 : loss 34.350345611572266\n",
      "step 243 : loss 34.31220626831055\n",
      "step 244 : loss 34.27335739135742\n",
      "step 245 : loss 34.235477447509766\n",
      "step 246 : loss 34.197242736816406\n",
      "step 247 : loss 34.159515380859375\n",
      "step 248 : loss 34.121944427490234\n",
      "step 249 : loss 34.084495544433594\n",
      "step 250 : loss 34.047420501708984\n",
      "step 251 : loss 34.01042175292969\n",
      "step 252 : loss 33.9737434387207\n",
      "step 253 : loss 33.93722915649414\n",
      "step 254 : loss 33.901004791259766\n",
      "step 255 : loss 33.86488723754883\n",
      "step 256 : loss 33.82916259765625\n",
      "step 257 : loss 33.79344940185547\n",
      "step 258 : loss 33.75813293457031\n",
      "step 259 : loss 33.7228889465332\n",
      "step 260 : loss 33.687896728515625\n",
      "step 261 : loss 33.65312576293945\n",
      "step 262 : loss 33.618465423583984\n",
      "step 263 : loss 33.584049224853516\n",
      "step 264 : loss 33.549739837646484\n",
      "step 265 : loss 33.515628814697266\n",
      "step 266 : loss 33.481590270996094\n",
      "step 267 : loss 33.447696685791016\n",
      "step 268 : loss 33.41383361816406\n",
      "step 269 : loss 33.38005447387695\n",
      "step 270 : loss 33.3463020324707\n",
      "step 271 : loss 33.3124885559082\n",
      "step 272 : loss 33.2786750793457\n",
      "step 273 : loss 33.244747161865234\n",
      "step 274 : loss 33.210693359375\n",
      "step 275 : loss 33.17646026611328\n",
      "step 276 : loss 33.1419792175293\n",
      "step 277 : loss 33.107215881347656\n",
      "step 278 : loss 33.072059631347656\n",
      "step 279 : loss 33.036476135253906\n",
      "step 280 : loss 33.0003662109375\n",
      "step 281 : loss 32.96363830566406\n",
      "step 282 : loss 32.92625427246094\n",
      "step 283 : loss 32.88801574707031\n",
      "step 284 : loss 32.84886169433594\n",
      "step 285 : loss 32.808677673339844\n",
      "step 286 : loss 32.76734924316406\n",
      "step 287 : loss 32.724761962890625\n",
      "step 288 : loss 32.680721282958984\n",
      "step 289 : loss 32.635009765625\n",
      "step 290 : loss 32.58745574951172\n",
      "step 291 : loss 32.53778839111328\n",
      "step 292 : loss 32.485740661621094\n",
      "step 293 : loss 32.4310302734375\n",
      "step 294 : loss 32.37324905395508\n",
      "step 295 : loss 32.31207275390625\n",
      "step 296 : loss 32.2470588684082\n",
      "step 297 : loss 32.17776870727539\n",
      "step 298 : loss 32.10371017456055\n",
      "step 299 : loss 32.02412414550781\n",
      "step 300 : loss 31.938203811645508\n",
      "step 301 : loss 31.845050811767578\n",
      "step 302 : loss 31.74356460571289\n",
      "step 303 : loss 31.63247299194336\n",
      "step 304 : loss 31.510263442993164\n",
      "step 305 : loss 31.375102996826172\n",
      "step 306 : loss 31.224834442138672\n",
      "step 307 : loss 31.056865692138672\n",
      "step 308 : loss 30.868112564086914\n",
      "step 309 : loss 30.654821395874023\n",
      "step 310 : loss 30.412466049194336\n",
      "step 311 : loss 30.1356201171875\n",
      "step 312 : loss 29.817739486694336\n",
      "step 313 : loss 29.451112747192383\n",
      "step 314 : loss 29.02672576904297\n",
      "step 315 : loss 28.5340518951416\n",
      "step 316 : loss 27.961767196655273\n",
      "step 317 : loss 27.298290252685547\n",
      "step 318 : loss 26.535938262939453\n",
      "step 319 : loss 25.682838439941406\n",
      "step 320 : loss 24.938085556030273\n",
      "step 321 : loss 27.05978012084961\n",
      "step 322 : loss 38.708988189697266\n",
      "step 323 : loss 32.220863342285156\n",
      "step 324 : loss 31.584552764892578\n",
      "step 325 : loss 27.760604858398438\n",
      "step 326 : loss 24.829866409301758\n",
      "step 327 : loss 28.06863021850586\n",
      "step 328 : loss 26.264545440673828\n",
      "step 329 : loss 24.687788009643555\n",
      "step 330 : loss 24.518091201782227\n",
      "step 331 : loss 23.9666805267334\n",
      "step 332 : loss 24.69652557373047\n",
      "step 333 : loss 22.063371658325195\n",
      "step 334 : loss 24.37030792236328\n",
      "step 335 : loss 21.675485610961914\n",
      "step 336 : loss 23.201805114746094\n",
      "step 337 : loss 22.057903289794922\n",
      "step 338 : loss 22.04880142211914\n",
      "step 339 : loss 21.978574752807617\n",
      "step 340 : loss 21.42967987060547\n",
      "step 341 : loss 21.859661102294922\n",
      "step 342 : loss 20.945110321044922\n",
      "step 343 : loss 21.562761306762695\n",
      "step 344 : loss 20.833498001098633\n",
      "step 345 : loss 21.024513244628906\n",
      "step 346 : loss 20.905424118041992\n",
      "step 347 : loss 20.522258758544922\n",
      "step 348 : loss 20.788602828979492\n",
      "step 349 : loss 20.200820922851562\n",
      "step 350 : loss 20.62860107421875\n",
      "step 351 : loss 19.984294891357422\n",
      "step 352 : loss 20.42469596862793\n",
      "step 353 : loss 19.87921714782715\n",
      "step 354 : loss 20.171968460083008\n",
      "step 355 : loss 19.84026527404785\n",
      "step 356 : loss 19.940214157104492\n",
      "step 357 : loss 19.770130157470703\n",
      "step 358 : loss 19.745277404785156\n",
      "step 359 : loss 19.701766967773438\n",
      "step 360 : loss 19.56279754638672\n",
      "step 361 : loss 19.601673126220703\n",
      "step 362 : loss 19.430749893188477\n",
      "step 363 : loss 19.48700523376465\n",
      "step 364 : loss 19.30356788635254\n",
      "step 365 : loss 19.375625610351562\n",
      "step 366 : loss 19.1989803314209\n",
      "step 367 : loss 19.25769805908203\n",
      "step 368 : loss 19.10114288330078\n",
      "step 369 : loss 19.146854400634766\n",
      "step 370 : loss 19.012048721313477\n",
      "step 371 : loss 19.03011131286621\n",
      "step 372 : loss 18.926546096801758\n",
      "step 373 : loss 18.922895431518555\n",
      "step 374 : loss 18.835161209106445\n",
      "step 375 : loss 18.822402954101562\n",
      "step 376 : loss 18.73992347717285\n",
      "step 377 : loss 18.72846794128418\n",
      "step 378 : loss 18.646459579467773\n",
      "step 379 : loss 18.632736206054688\n",
      "step 380 : loss 18.55895233154297\n",
      "step 381 : loss 18.538949966430664\n",
      "step 382 : loss 18.471511840820312\n",
      "step 383 : loss 18.447893142700195\n",
      "step 384 : loss 18.385238647460938\n",
      "step 385 : loss 18.358116149902344\n",
      "step 386 : loss 18.300514221191406\n",
      "step 387 : loss 18.26949119567871\n",
      "step 388 : loss 18.214786529541016\n",
      "step 389 : loss 18.183935165405273\n",
      "step 390 : loss 18.130666732788086\n",
      "step 391 : loss 18.096435546875\n",
      "step 392 : loss 18.049901962280273\n",
      "step 393 : loss 18.009737014770508\n",
      "step 394 : loss 17.968582153320312\n",
      "step 395 : loss 17.92604637145996\n",
      "step 396 : loss 17.88759422302246\n",
      "step 397 : loss 17.843320846557617\n",
      "step 398 : loss 17.80718231201172\n",
      "step 399 : loss 17.76297378540039\n",
      "step 400 : loss 17.727123260498047\n",
      "step 401 : loss 17.683988571166992\n",
      "step 402 : loss 17.647472381591797\n",
      "step 403 : loss 17.607757568359375\n",
      "step 404 : loss 17.568580627441406\n",
      "step 405 : loss 17.5317325592041\n",
      "step 406 : loss 17.4923095703125\n",
      "step 407 : loss 17.45659828186035\n",
      "step 408 : loss 17.41765785217285\n",
      "step 409 : loss 17.38190460205078\n",
      "step 410 : loss 17.34465980529785\n",
      "step 411 : loss 17.308250427246094\n",
      "step 412 : loss 17.273170471191406\n",
      "step 413 : loss 17.236190795898438\n",
      "step 414 : loss 17.201658248901367\n",
      "step 415 : loss 17.165966033935547\n",
      "step 416 : loss 17.13113784790039\n",
      "step 417 : loss 17.096927642822266\n",
      "step 418 : loss 17.062040328979492\n",
      "step 419 : loss 17.028621673583984\n",
      "step 420 : loss 16.994422912597656\n",
      "step 421 : loss 16.96078109741211\n",
      "step 422 : loss 16.927810668945312\n",
      "step 423 : loss 16.89430046081543\n",
      "step 424 : loss 16.861675262451172\n",
      "step 425 : loss 16.829069137573242\n",
      "step 426 : loss 16.796607971191406\n",
      "step 427 : loss 16.765039443969727\n",
      "step 428 : loss 16.73370361328125\n",
      "step 429 : loss 16.704185485839844\n",
      "step 430 : loss 16.678808212280273\n",
      "step 431 : loss 16.664592742919922\n",
      "step 432 : loss 16.68120765686035\n",
      "step 433 : loss 16.781017303466797\n",
      "step 434 : loss 16.992677688598633\n",
      "step 435 : loss 17.300552368164062\n",
      "step 436 : loss 17.044099807739258\n",
      "step 437 : loss 16.676658630371094\n",
      "step 438 : loss 16.44118881225586\n",
      "step 439 : loss 16.512052536010742\n",
      "step 440 : loss 16.733192443847656\n",
      "step 441 : loss 16.70146942138672\n",
      "step 442 : loss 16.499353408813477\n",
      "step 443 : loss 16.311290740966797\n",
      "step 444 : loss 16.329696655273438\n",
      "step 445 : loss 16.45758819580078\n",
      "step 446 : loss 16.444900512695312\n",
      "step 447 : loss 16.31424331665039\n",
      "step 448 : loss 16.180156707763672\n",
      "step 449 : loss 16.170652389526367\n",
      "step 450 : loss 16.238317489624023\n",
      "step 451 : loss 16.245765686035156\n",
      "step 452 : loss 16.175630569458008\n",
      "step 453 : loss 16.068361282348633\n",
      "step 454 : loss 16.019733428955078\n",
      "step 455 : loss 16.032787322998047\n",
      "step 456 : loss 16.054737091064453\n",
      "step 457 : loss 16.04741859436035\n",
      "step 458 : loss 15.986865997314453\n",
      "step 459 : loss 15.921921730041504\n",
      "step 460 : loss 15.875452041625977\n",
      "step 461 : loss 15.854981422424316\n",
      "step 462 : loss 15.849278450012207\n",
      "step 463 : loss 15.842082977294922\n",
      "step 464 : loss 15.830755233764648\n",
      "step 465 : loss 15.808484077453613\n",
      "step 466 : loss 15.789649963378906\n",
      "step 467 : loss 15.770956039428711\n",
      "step 468 : loss 15.762184143066406\n",
      "step 469 : loss 15.757347106933594\n",
      "step 470 : loss 15.756991386413574\n",
      "step 471 : loss 15.75521469116211\n",
      "step 472 : loss 15.748592376708984\n",
      "step 473 : loss 15.732699394226074\n",
      "step 474 : loss 15.712152481079102\n",
      "step 475 : loss 15.680815696716309\n",
      "step 476 : loss 15.658977508544922\n",
      "step 477 : loss 15.627979278564453\n",
      "step 478 : loss 15.618305206298828\n",
      "step 479 : loss 15.592171669006348\n",
      "step 480 : loss 15.585031509399414\n",
      "step 481 : loss 15.547370910644531\n",
      "step 482 : loss 15.519135475158691\n",
      "step 483 : loss 15.462667465209961\n",
      "step 484 : loss 15.414896011352539\n",
      "step 485 : loss 15.35937213897705\n",
      "step 486 : loss 15.315157890319824\n",
      "step 487 : loss 15.275177001953125\n",
      "step 488 : loss 15.244571685791016\n",
      "step 489 : loss 15.220612525939941\n",
      "step 490 : loss 15.207619667053223\n",
      "step 491 : loss 15.211106300354004\n",
      "step 492 : loss 15.251482009887695\n",
      "step 493 : loss 15.359572410583496\n",
      "step 494 : loss 15.571294784545898\n",
      "step 495 : loss 15.817744255065918\n",
      "step 496 : loss 15.805480003356934\n",
      "step 497 : loss 15.537239074707031\n",
      "step 498 : loss 15.198381423950195\n",
      "step 499 : loss 15.088823318481445\n",
      "step 500 : loss 15.168710708618164\n",
      "step 501 : loss 15.342582702636719\n",
      "step 502 : loss 15.338345527648926\n",
      "step 503 : loss 15.174152374267578\n",
      "step 504 : loss 14.960712432861328\n",
      "step 505 : loss 14.874670028686523\n",
      "step 506 : loss 14.929848670959473\n",
      "step 507 : loss 15.049888610839844\n",
      "step 508 : loss 15.138998031616211\n",
      "step 509 : loss 15.056985855102539\n",
      "step 510 : loss 14.949666976928711\n",
      "step 511 : loss 14.890064239501953\n",
      "step 512 : loss 14.969286918640137\n",
      "step 513 : loss 15.08897590637207\n",
      "step 514 : loss 15.186667442321777\n",
      "step 515 : loss 15.05450439453125\n",
      "step 516 : loss 14.897470474243164\n",
      "step 517 : loss 14.766121864318848\n",
      "step 518 : loss 14.739696502685547\n",
      "step 519 : loss 14.737740516662598\n",
      "step 520 : loss 14.723729133605957\n",
      "step 521 : loss 14.654755592346191\n",
      "step 522 : loss 14.582380294799805\n",
      "step 523 : loss 14.53539752960205\n",
      "step 524 : loss 14.531658172607422\n",
      "step 525 : loss 14.57386589050293\n",
      "step 526 : loss 14.641789436340332\n",
      "step 527 : loss 14.746294021606445\n",
      "step 528 : loss 14.792204856872559\n",
      "step 529 : loss 14.859606742858887\n",
      "step 530 : loss 14.792171478271484\n",
      "step 531 : loss 14.779080390930176\n",
      "step 532 : loss 14.688345909118652\n",
      "step 533 : loss 14.629289627075195\n",
      "step 534 : loss 14.535440444946289\n",
      "step 535 : loss 14.436232566833496\n",
      "step 536 : loss 14.350833892822266\n",
      "step 537 : loss 14.292281150817871\n",
      "step 538 : loss 14.271934509277344\n",
      "step 539 : loss 14.290129661560059\n",
      "step 540 : loss 14.357810020446777\n",
      "step 541 : loss 14.454773902893066\n",
      "step 542 : loss 14.59317398071289\n",
      "step 543 : loss 14.612283706665039\n",
      "step 544 : loss 14.58313274383545\n",
      "step 545 : loss 14.402134895324707\n",
      "step 546 : loss 14.268580436706543\n",
      "step 547 : loss 14.167353630065918\n",
      "step 548 : loss 14.12537956237793\n",
      "step 549 : loss 14.114570617675781\n",
      "step 550 : loss 14.126813888549805\n",
      "step 551 : loss 14.141840934753418\n",
      "step 552 : loss 14.162693977355957\n",
      "step 553 : loss 14.17166519165039\n",
      "step 554 : loss 14.176814079284668\n",
      "step 555 : loss 14.181537628173828\n",
      "step 556 : loss 14.181987762451172\n",
      "step 557 : loss 14.223213195800781\n",
      "step 558 : loss 14.248390197753906\n",
      "step 559 : loss 14.343631744384766\n",
      "step 560 : loss 14.316407203674316\n",
      "step 561 : loss 14.310016632080078\n",
      "step 562 : loss 14.150376319885254\n",
      "step 563 : loss 14.03075885772705\n",
      "step 564 : loss 13.914384841918945\n",
      "step 565 : loss 13.84918212890625\n",
      "step 566 : loss 13.818262100219727\n",
      "step 567 : loss 13.821988105773926\n",
      "step 568 : loss 13.864677429199219\n",
      "step 569 : loss 13.943931579589844\n",
      "step 570 : loss 14.087435722351074\n",
      "step 571 : loss 14.160425186157227\n",
      "step 572 : loss 14.219796180725098\n",
      "step 573 : loss 14.043858528137207\n",
      "step 574 : loss 13.89317512512207\n",
      "step 575 : loss 13.74443244934082\n",
      "step 576 : loss 13.66303825378418\n",
      "step 577 : loss 13.621959686279297\n",
      "step 578 : loss 13.60992431640625\n",
      "step 579 : loss 13.618719100952148\n",
      "step 580 : loss 13.648536682128906\n",
      "step 581 : loss 13.707530975341797\n",
      "step 582 : loss 13.780518531799316\n",
      "step 583 : loss 13.879602432250977\n",
      "step 584 : loss 13.888101577758789\n",
      "step 585 : loss 13.875044822692871\n",
      "step 586 : loss 13.74695110321045\n",
      "step 587 : loss 13.667092323303223\n",
      "step 588 : loss 13.615017890930176\n",
      "step 589 : loss 13.664806365966797\n",
      "step 590 : loss 13.774993896484375\n",
      "step 591 : loss 13.989465713500977\n",
      "step 592 : loss 13.96849250793457\n",
      "step 593 : loss 13.8714599609375\n",
      "step 594 : loss 13.620399475097656\n",
      "step 595 : loss 13.480664253234863\n",
      "step 596 : loss 13.478411674499512\n",
      "step 597 : loss 13.588150978088379\n",
      "step 598 : loss 13.788631439208984\n",
      "step 599 : loss 13.813699722290039\n",
      "step 600 : loss 13.764805793762207\n",
      "step 601 : loss 13.552223205566406\n",
      "step 602 : loss 13.476543426513672\n",
      "step 603 : loss 13.497203826904297\n",
      "step 604 : loss 13.60824966430664\n",
      "step 605 : loss 13.613176345825195\n",
      "step 606 : loss 13.559544563293457\n",
      "step 607 : loss 13.429779052734375\n",
      "step 608 : loss 13.406347274780273\n",
      "step 609 : loss 13.457691192626953\n",
      "step 610 : loss 13.573577880859375\n",
      "step 611 : loss 13.541242599487305\n",
      "step 612 : loss 13.440267562866211\n",
      "step 613 : loss 13.282208442687988\n",
      "step 614 : loss 13.200379371643066\n",
      "step 615 : loss 13.189383506774902\n",
      "step 616 : loss 13.219267845153809\n",
      "step 617 : loss 13.236303329467773\n",
      "step 618 : loss 13.216789245605469\n",
      "step 619 : loss 13.16494369506836\n",
      "step 620 : loss 13.12927532196045\n",
      "step 621 : loss 13.133216857910156\n",
      "step 622 : loss 13.225341796875\n",
      "step 623 : loss 13.373477935791016\n",
      "step 624 : loss 13.615147590637207\n",
      "step 625 : loss 13.593951225280762\n",
      "step 626 : loss 13.53381633758545\n",
      "step 627 : loss 13.282848358154297\n",
      "step 628 : loss 13.142029762268066\n",
      "step 629 : loss 13.046797752380371\n",
      "step 630 : loss 13.010258674621582\n",
      "step 631 : loss 12.997967720031738\n",
      "step 632 : loss 13.00967788696289\n",
      "step 633 : loss 13.063451766967773\n",
      "step 634 : loss 13.165410995483398\n",
      "step 635 : loss 13.34062385559082\n",
      "step 636 : loss 13.434475898742676\n",
      "step 637 : loss 13.421514511108398\n",
      "step 638 : loss 13.204418182373047\n",
      "step 639 : loss 13.000378608703613\n",
      "step 640 : loss 12.874238014221191\n",
      "step 641 : loss 12.828788757324219\n",
      "step 642 : loss 12.857418060302734\n",
      "step 643 : loss 12.951800346374512\n",
      "step 644 : loss 13.133910179138184\n",
      "step 645 : loss 13.250347137451172\n",
      "step 646 : loss 13.335315704345703\n",
      "step 647 : loss 13.134486198425293\n",
      "step 648 : loss 12.983804702758789\n",
      "step 649 : loss 12.865344047546387\n",
      "step 650 : loss 12.850295066833496\n",
      "step 651 : loss 12.887659072875977\n",
      "step 652 : loss 12.980572700500488\n",
      "step 653 : loss 13.034246444702148\n",
      "step 654 : loss 13.039271354675293\n",
      "step 655 : loss 12.9471435546875\n",
      "step 656 : loss 12.843720436096191\n",
      "step 657 : loss 12.75650405883789\n",
      "step 658 : loss 12.713122367858887\n",
      "step 659 : loss 12.705768585205078\n",
      "step 660 : loss 12.719097137451172\n",
      "step 661 : loss 12.747834205627441\n",
      "step 662 : loss 12.760467529296875\n",
      "step 663 : loss 12.763272285461426\n",
      "step 664 : loss 12.72970199584961\n",
      "step 665 : loss 12.691615104675293\n",
      "step 666 : loss 12.643470764160156\n",
      "step 667 : loss 12.616761207580566\n",
      "step 668 : loss 12.611310958862305\n",
      "step 669 : loss 12.665610313415527\n",
      "step 670 : loss 12.79389762878418\n",
      "step 671 : loss 13.076830863952637\n",
      "step 672 : loss 13.251961708068848\n",
      "step 673 : loss 13.34544563293457\n",
      "step 674 : loss 12.974817276000977\n",
      "step 675 : loss 12.751367568969727\n",
      "step 676 : loss 12.63748550415039\n",
      "step 677 : loss 12.685466766357422\n",
      "step 678 : loss 12.809041976928711\n",
      "step 679 : loss 12.894269943237305\n",
      "step 680 : loss 12.882898330688477\n",
      "step 681 : loss 12.692025184631348\n",
      "step 682 : loss 12.536999702453613\n",
      "step 683 : loss 12.439108848571777\n",
      "step 684 : loss 12.423264503479004\n",
      "step 685 : loss 12.472476959228516\n",
      "step 686 : loss 12.595544815063477\n",
      "step 687 : loss 12.717961311340332\n",
      "step 688 : loss 12.851003646850586\n",
      "step 689 : loss 12.771446228027344\n",
      "step 690 : loss 12.710711479187012\n",
      "step 691 : loss 12.613930702209473\n",
      "step 692 : loss 12.634177207946777\n",
      "step 693 : loss 12.641034126281738\n",
      "step 694 : loss 12.6590576171875\n",
      "step 695 : loss 12.554906845092773\n",
      "step 696 : loss 12.453315734863281\n",
      "step 697 : loss 12.354541778564453\n",
      "step 698 : loss 12.306950569152832\n",
      "step 699 : loss 12.300134658813477\n",
      "step 700 : loss 12.337371826171875\n",
      "step 701 : loss 12.402749061584473\n",
      "step 702 : loss 12.516470909118652\n",
      "step 703 : loss 12.579080581665039\n",
      "step 704 : loss 12.655559539794922\n",
      "step 705 : loss 12.583802223205566\n",
      "step 706 : loss 12.555290222167969\n",
      "step 707 : loss 12.485201835632324\n",
      "step 708 : loss 12.481952667236328\n",
      "step 709 : loss 12.442230224609375\n",
      "step 710 : loss 12.425477027893066\n",
      "step 711 : loss 12.366474151611328\n",
      "step 712 : loss 12.339923858642578\n",
      "step 713 : loss 12.330728530883789\n",
      "step 714 : loss 12.406965255737305\n",
      "step 715 : loss 12.517553329467773\n",
      "step 716 : loss 12.717672348022461\n",
      "step 717 : loss 12.681086540222168\n",
      "step 718 : loss 12.595096588134766\n",
      "step 719 : loss 12.362561225891113\n",
      "step 720 : loss 12.214509010314941\n",
      "step 721 : loss 12.124960899353027\n",
      "step 722 : loss 12.086137771606445\n",
      "step 723 : loss 12.074431419372559\n",
      "step 724 : loss 12.079867362976074\n",
      "step 725 : loss 12.098920822143555\n",
      "step 726 : loss 12.132166862487793\n",
      "step 727 : loss 12.183446884155273\n",
      "step 728 : loss 12.250349044799805\n",
      "step 729 : loss 12.334349632263184\n",
      "step 730 : loss 12.383893013000488\n",
      "step 731 : loss 12.41395378112793\n",
      "step 732 : loss 12.349234580993652\n",
      "step 733 : loss 12.299554824829102\n",
      "step 734 : loss 12.226673126220703\n",
      "step 735 : loss 12.204401016235352\n",
      "step 736 : loss 12.182190895080566\n",
      "step 737 : loss 12.181617736816406\n",
      "step 738 : loss 12.166202545166016\n",
      "step 739 : loss 12.174046516418457\n",
      "step 740 : loss 12.197793960571289\n",
      "step 741 : loss 12.343740463256836\n",
      "step 742 : loss 12.577336311340332\n",
      "step 743 : loss 12.960992813110352\n",
      "step 744 : loss 12.72979736328125\n",
      "step 745 : loss 12.478379249572754\n",
      "step 746 : loss 12.141430854797363\n",
      "step 747 : loss 11.999555587768555\n",
      "step 748 : loss 11.958264350891113\n",
      "step 749 : loss 11.987191200256348\n",
      "step 750 : loss 12.066350936889648\n",
      "step 751 : loss 12.171356201171875\n",
      "step 752 : loss 12.287006378173828\n",
      "step 753 : loss 12.282001495361328\n",
      "step 754 : loss 12.270252227783203\n",
      "step 755 : loss 12.188739776611328\n",
      "step 756 : loss 12.213116645812988\n",
      "step 757 : loss 12.252102851867676\n",
      "step 758 : loss 12.34667682647705\n",
      "step 759 : loss 12.262813568115234\n",
      "step 760 : loss 12.167834281921387\n",
      "step 761 : loss 12.011957168579102\n",
      "step 762 : loss 11.924644470214844\n",
      "step 763 : loss 11.876891136169434\n",
      "step 764 : loss 11.866101264953613\n",
      "step 765 : loss 11.870348930358887\n",
      "step 766 : loss 11.882804870605469\n",
      "step 767 : loss 11.888667106628418\n",
      "step 768 : loss 11.889803886413574\n",
      "step 769 : loss 11.878281593322754\n",
      "step 770 : loss 11.865199089050293\n",
      "step 771 : loss 11.848676681518555\n",
      "step 772 : loss 11.847127914428711\n",
      "step 773 : loss 11.868059158325195\n",
      "step 774 : loss 11.962693214416504\n",
      "step 775 : loss 12.155378341674805\n",
      "step 776 : loss 12.510489463806152\n",
      "step 777 : loss 12.55756950378418\n",
      "step 778 : loss 12.428299903869629\n",
      "step 779 : loss 12.043248176574707\n",
      "step 780 : loss 11.85057258605957\n",
      "step 781 : loss 11.79195785522461\n",
      "step 782 : loss 11.8403959274292\n",
      "step 783 : loss 11.954970359802246\n",
      "step 784 : loss 12.102662086486816\n",
      "step 785 : loss 12.160126686096191\n",
      "step 786 : loss 12.073975563049316\n",
      "step 787 : loss 11.915485382080078\n",
      "step 788 : loss 11.785843849182129\n",
      "step 789 : loss 11.721488952636719\n",
      "step 790 : loss 11.720584869384766\n",
      "step 791 : loss 11.774989128112793\n",
      "step 792 : loss 11.903464317321777\n",
      "step 793 : loss 12.051105499267578\n",
      "step 794 : loss 12.239468574523926\n",
      "step 795 : loss 12.20154094696045\n",
      "step 796 : loss 12.173104286193848\n",
      "step 797 : loss 12.046329498291016\n",
      "step 798 : loss 12.024436950683594\n",
      "step 799 : loss 11.950727462768555\n",
      "step 800 : loss 11.913930892944336\n",
      "step 801 : loss 11.81962776184082\n",
      "step 802 : loss 11.748730659484863\n",
      "step 803 : loss 11.685527801513672\n",
      "step 804 : loss 11.646648406982422\n",
      "step 805 : loss 11.626908302307129\n",
      "step 806 : loss 11.623430252075195\n",
      "step 807 : loss 11.6400785446167\n",
      "step 808 : loss 11.68216323852539\n",
      "step 809 : loss 11.776444435119629\n",
      "step 810 : loss 11.913848876953125\n",
      "step 811 : loss 12.122344017028809\n",
      "step 812 : loss 12.174368858337402\n",
      "step 813 : loss 12.175429344177246\n",
      "step 814 : loss 11.972694396972656\n",
      "step 815 : loss 11.863529205322266\n",
      "step 816 : loss 11.765289306640625\n",
      "step 817 : loss 11.745288848876953\n",
      "step 818 : loss 11.725223541259766\n",
      "step 819 : loss 11.720388412475586\n",
      "step 820 : loss 11.702103614807129\n",
      "step 821 : loss 11.67579460144043\n",
      "step 822 : loss 11.652338981628418\n",
      "step 823 : loss 11.62915325164795\n",
      "step 824 : loss 11.629115104675293\n",
      "step 825 : loss 11.643479347229004\n",
      "step 826 : loss 11.705131530761719\n",
      "step 827 : loss 11.782835960388184\n",
      "step 828 : loss 11.917261123657227\n",
      "step 829 : loss 11.941756248474121\n",
      "step 830 : loss 11.954947471618652\n",
      "step 831 : loss 11.810635566711426\n",
      "step 832 : loss 11.720848083496094\n",
      "step 833 : loss 11.645638465881348\n",
      "step 834 : loss 11.661373138427734\n",
      "step 835 : loss 11.74957275390625\n",
      "step 836 : loss 11.958281517028809\n",
      "step 837 : loss 12.08960247039795\n",
      "step 838 : loss 12.083227157592773\n",
      "step 839 : loss 11.802323341369629\n",
      "step 840 : loss 11.589696884155273\n",
      "step 841 : loss 11.475358963012695\n",
      "step 842 : loss 11.452619552612305\n",
      "step 843 : loss 11.492340087890625\n",
      "step 844 : loss 11.575726509094238\n",
      "step 845 : loss 11.678487777709961\n",
      "step 846 : loss 11.723328590393066\n",
      "step 847 : loss 11.709774017333984\n",
      "step 848 : loss 11.617605209350586\n",
      "step 849 : loss 11.558021545410156\n",
      "step 850 : loss 11.544777870178223\n",
      "step 851 : loss 11.621337890625\n",
      "step 852 : loss 11.746243476867676\n",
      "step 853 : loss 11.915677070617676\n",
      "step 854 : loss 11.867203712463379\n",
      "step 855 : loss 11.779916763305664\n",
      "step 856 : loss 11.615333557128906\n",
      "step 857 : loss 11.540290832519531\n",
      "step 858 : loss 11.515900611877441\n",
      "step 859 : loss 11.550554275512695\n",
      "step 860 : loss 11.589825630187988\n",
      "step 861 : loss 11.641810417175293\n",
      "step 862 : loss 11.633864402770996\n",
      "step 863 : loss 11.61609172821045\n",
      "step 864 : loss 11.557412147521973\n",
      "step 865 : loss 11.530404090881348\n",
      "step 866 : loss 11.517099380493164\n",
      "step 867 : loss 11.557425498962402\n",
      "step 868 : loss 11.611551284790039\n",
      "step 869 : loss 11.703088760375977\n",
      "step 870 : loss 11.708183288574219\n",
      "step 871 : loss 11.697410583496094\n",
      "step 872 : loss 11.58966064453125\n",
      "step 873 : loss 11.512177467346191\n",
      "step 874 : loss 11.441198348999023\n",
      "step 875 : loss 11.409573554992676\n",
      "step 876 : loss 11.39811897277832\n",
      "step 877 : loss 11.416698455810547\n",
      "step 878 : loss 11.454302787780762\n",
      "step 879 : loss 11.531328201293945\n",
      "step 880 : loss 11.602115631103516\n",
      "step 881 : loss 11.68375015258789\n",
      "step 882 : loss 11.655614852905273\n",
      "step 883 : loss 11.605416297912598\n",
      "step 884 : loss 11.49618911743164\n",
      "step 885 : loss 11.422698974609375\n",
      "step 886 : loss 11.367406845092773\n",
      "step 887 : loss 11.345176696777344\n",
      "step 888 : loss 11.342917442321777\n",
      "step 889 : loss 11.367898941040039\n",
      "step 890 : loss 11.413280487060547\n",
      "step 891 : loss 11.4963960647583\n",
      "step 892 : loss 11.562424659729004\n",
      "step 893 : loss 11.632694244384766\n",
      "step 894 : loss 11.58386516571045\n",
      "step 895 : loss 11.528519630432129\n",
      "step 896 : loss 11.43044662475586\n",
      "step 897 : loss 11.374279022216797\n",
      "step 898 : loss 11.339643478393555\n",
      "step 899 : loss 11.347146034240723\n",
      "step 900 : loss 11.387770652770996\n",
      "step 901 : loss 11.498848915100098\n",
      "step 902 : loss 11.61655044555664\n",
      "step 903 : loss 11.781695365905762\n",
      "step 904 : loss 11.69961929321289\n",
      "step 905 : loss 11.606284141540527\n",
      "step 906 : loss 11.430859565734863\n",
      "step 907 : loss 11.334146499633789\n",
      "step 908 : loss 11.280793190002441\n",
      "step 909 : loss 11.273667335510254\n",
      "step 910 : loss 11.311661720275879\n",
      "step 911 : loss 11.405969619750977\n",
      "step 912 : loss 11.578812599182129\n",
      "step 913 : loss 11.7175874710083\n",
      "step 914 : loss 11.756278991699219\n",
      "step 915 : loss 11.562273979187012\n",
      "step 916 : loss 11.37788200378418\n",
      "step 917 : loss 11.250444412231445\n",
      "step 918 : loss 11.197600364685059\n",
      "step 919 : loss 11.193462371826172\n",
      "step 920 : loss 11.22726058959961\n",
      "step 921 : loss 11.293464660644531\n",
      "step 922 : loss 11.379654884338379\n",
      "step 923 : loss 11.450918197631836\n",
      "step 924 : loss 11.463504791259766\n",
      "step 925 : loss 11.406632423400879\n",
      "step 926 : loss 11.334551811218262\n",
      "step 927 : loss 11.277385711669922\n",
      "step 928 : loss 11.281895637512207\n",
      "step 929 : loss 11.344958305358887\n",
      "step 930 : loss 11.4990816116333\n",
      "step 931 : loss 11.628973007202148\n",
      "step 932 : loss 11.715271949768066\n",
      "step 933 : loss 11.560962677001953\n",
      "step 934 : loss 11.419777870178223\n",
      "step 935 : loss 11.296865463256836\n",
      "step 936 : loss 11.25204086303711\n",
      "step 937 : loss 11.239934921264648\n",
      "step 938 : loss 11.262822151184082\n",
      "step 939 : loss 11.285747528076172\n",
      "step 940 : loss 11.323233604431152\n",
      "step 941 : loss 11.332552909851074\n",
      "step 942 : loss 11.347029685974121\n",
      "step 943 : loss 11.328117370605469\n",
      "step 944 : loss 11.322759628295898\n",
      "step 945 : loss 11.305419921875\n",
      "step 946 : loss 11.304115295410156\n",
      "step 947 : loss 11.300304412841797\n",
      "step 948 : loss 11.298354148864746\n",
      "step 949 : loss 11.290558815002441\n",
      "step 950 : loss 11.273155212402344\n",
      "step 951 : loss 11.256002426147461\n",
      "step 952 : loss 11.233697891235352\n",
      "step 953 : loss 11.228933334350586\n",
      "step 954 : loss 11.230892181396484\n",
      "step 955 : loss 11.277360916137695\n",
      "step 956 : loss 11.34424114227295\n",
      "step 957 : loss 11.487370491027832\n",
      "step 958 : loss 11.569231033325195\n",
      "step 959 : loss 11.636028289794922\n",
      "step 960 : loss 11.502223014831543\n",
      "step 961 : loss 11.38072395324707\n",
      "step 962 : loss 11.253440856933594\n",
      "step 963 : loss 11.190177917480469\n",
      "step 964 : loss 11.154437065124512\n",
      "step 965 : loss 11.147726058959961\n",
      "step 966 : loss 11.153027534484863\n",
      "step 967 : loss 11.170928001403809\n",
      "step 968 : loss 11.19491958618164\n",
      "step 969 : loss 11.217650413513184\n",
      "step 970 : loss 11.236547470092773\n",
      "step 971 : loss 11.234831809997559\n",
      "step 972 : loss 11.223555564880371\n",
      "step 973 : loss 11.193697929382324\n",
      "step 974 : loss 11.166521072387695\n",
      "step 975 : loss 11.13838005065918\n",
      "step 976 : loss 11.121464729309082\n",
      "step 977 : loss 11.1106595993042\n",
      "step 978 : loss 11.111522674560547\n",
      "step 979 : loss 11.118746757507324\n",
      "step 980 : loss 11.139086723327637\n",
      "step 981 : loss 11.163247108459473\n",
      "step 982 : loss 11.205192565917969\n",
      "step 983 : loss 11.240344047546387\n",
      "step 984 : loss 11.30157470703125\n",
      "step 985 : loss 11.341486930847168\n",
      "step 986 : loss 11.425226211547852\n",
      "step 987 : loss 11.458227157592773\n",
      "step 988 : loss 11.526213645935059\n",
      "step 989 : loss 11.440421104431152\n",
      "step 990 : loss 11.382887840270996\n",
      "step 991 : loss 11.245613098144531\n",
      "step 992 : loss 11.167862892150879\n",
      "step 993 : loss 11.09980583190918\n",
      "step 994 : loss 11.066956520080566\n",
      "step 995 : loss 11.047952651977539\n",
      "step 996 : loss 11.047162055969238\n",
      "step 997 : loss 11.057944297790527\n",
      "step 998 : loss 11.088854789733887\n",
      "step 999 : loss 11.131056785583496\n",
      "Test Loss at the end of training: 12.800069808959961\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='finegrain-cs', config={\n",
    "    'lr': lr,\n",
    "    'num_steps': num_steps\n",
    "})\n",
    "\n",
    "path_dataset_train = \"../data/dataset_train/\"\n",
    "path_dataset_test = \"../data/dataset_test/\"\n",
    "dataset_train = ImageDataset(path_dataset_train).data\n",
    "dataset_test = ImageDataset(path_dataset_test).data\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.train()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "\n",
    "# Convert your datasets to DataLoader for easy batch processing\n",
    "# train_dataloader = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "\n",
    "# Modify your training loop\n",
    "for step in range(num_steps):\n",
    "    loss_iter = 0\n",
    "    for images in dataset_train:\n",
    "        images = image_to_tensor(images)\n",
    "        y = autoencoder(images)\n",
    "        loss = (y - images).norm()\n",
    "        loss_iter += loss\n",
    "\n",
    "    loss = loss_iter / len(dataset_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Log the loss to wandb\n",
    "    wandb.log({'step': step, 'loss': loss.item()})\n",
    "\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "# Testing at the end of training\n",
    "loss_iter_test = 0\n",
    "reconstructed_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_test in dataset_test:\n",
    "        images_test = image_to_tensor(images_test)\n",
    "        y_test = autoencoder(images_test)\n",
    "        loss_test = (y_test - images_test).norm()\n",
    "        loss_iter_test += loss_test\n",
    "\n",
    "        # # Append the reconstructed images for visualization\n",
    "        concat = Image.new('RGB', (256, 128))\n",
    "        concat.paste(tensor_to_image(images_test.data), (0, 0))\n",
    "        concat.paste(tensor_to_image(y_test.data), (128, 0))\n",
    "        \n",
    "        reconstructed_images.append(concat)\n",
    "\n",
    "images = [PIL.Image.fromarray(np.array(image)) for image in reconstructed_images]\n",
    "\n",
    "wandb.log({\"reconstructed_images\": [wandb.Image(image) for image in images]})\n",
    "\n",
    "loss_test = loss_iter_test / len(dataset_test)\n",
    "\n",
    "# Log the test loss to wandb\n",
    "wandb.log({'test_loss': loss_test.item()})\n",
    "\n",
    "print(f\"Test Loss at the end of training: {loss_test.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "for image in dataset_test:\n",
    "    image = image_to_tensor(image)\n",
    "    result = autoencoder(image)\n",
    "    tensor_to_image(image.data).show()\n",
    "    tensor_to_image(result.data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tensor = image_to_tensor(dataset_test[0])\n",
    "latent = autoencoder.encoder(tensor)\n",
    "latent = (latent - latent.min())/(latent.max() - latent.min())\n",
    "\n",
    "\n",
    "#split latent into 4 channels\n",
    "channels = latent.squeeze(0).split(1)\n",
    "\n",
    "images = [tensor_to_image(channel.unsqueeze(0).detach()) for channel in channels]\n",
    "\n",
    "#write function to make grid using matplotlib in grayscale colorscheme\n",
    "def make_grid_using_matplotlib(images):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    columns = 2\n",
    "    rows = 2\n",
    "    for i in range(1, columns*rows +1):\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "\n",
    "        plt.imshow(images[i-1], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "make_grid_using_matplotlib(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
