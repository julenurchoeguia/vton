{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_dataset import ImageDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from refiners.fluxion import layers as fl\n",
    "from refiners.fluxion.adapters.adapter import Adapter\n",
    "from refiners.fluxion import utils\n",
    "import PIL\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = ImageDataset('/var/hub/datasets/zalando-hd-resized/train/image')\n",
    "test_images = ImageDataset('/var/hub/datasets/zalando-hd-resized/test/image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resblock\n",
    "class Resblock(fl.Sum):\n",
    "    def __init__(self, in_channels: int=1, out_channels: int=1) -> None:\n",
    "        super().__init__(\n",
    "            fl.Chain(\n",
    "                fl.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                fl.SiLU(),\n",
    "                fl.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            ),\n",
    "            fl.Conv2d(in_channels, out_channels, 1, padding=0),\n",
    "        )\n",
    "\n",
    "\n",
    "class Dropout(nn.Dropout, fl.Module):\n",
    "    def __init__(self, probability: float = 0.5, inplace: bool = False) -> None:\n",
    "        super().__init__(p=probability, inplace=inplace)\n",
    "\n",
    "class MaxPool2d(nn.MaxPool2d, fl.Module):\n",
    "    def __init__(self, factor: int = 2) -> None:\n",
    "        super().__init__(factor)\n",
    "\n",
    "class DropoutAdapter(Adapter[fl.SiLU], fl.Chain):\n",
    "    def __init__(self, target: fl.SiLU, dropout: float = 0.5):\n",
    "        self.dropout = dropout\n",
    "        with self.setup_adapter(target):\n",
    "            super().__init__(target)\n",
    "\n",
    "    def inject(self, parent: fl.Chain | None = None):\n",
    "        self.append(Dropout(self.dropout))\n",
    "        super().inject(parent)\n",
    "\n",
    "    def eject(self):\n",
    "        dropout = self.ensure_find(Dropout)\n",
    "        #  ensure_find : meme chose que find mais ne peut pas renovyer None\n",
    "        self.remove(dropout)\n",
    "        super().eject()\n",
    "\n",
    "class Flatten(fl.Module):\n",
    "    def __init__(self, start_dim: int = 0, end_dim: int = -1) -> None:\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.flatten(self.start_dim, self.end_dim)\n",
    "\n",
    "\n",
    "class Unflatten(fl.Module):\n",
    "    def __init__(self, dim: int, sizes : torch.Size) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.sizes = sizes\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.unflatten(self.dim, self.sizes)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(fl.Chain):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__(\n",
    "            fl.Conv2d(input_channels, 32, 1, padding=0),\n",
    "            Resblock(32, 64),\n",
    "            MaxPool2d(2),\n",
    "            Resblock(64, 128),\n",
    "            MaxPool2d(2),\n",
    "            Resblock(128, 256),\n",
    "            MaxPool2d(2),\n",
    "            fl.Conv2d(256, 8, 1, padding=0),\n",
    "            Flatten(),\n",
    "        )\n",
    "\n",
    "class Decoder(fl.Chain):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__(\n",
    "            Unflatten(dim=0, sizes=torch.Size([1,8, 128, 96])),\n",
    "            fl.Conv2d(8, 256, 1, padding=0),\n",
    "            Resblock(256, 128),\n",
    "            fl.Upsample(channels=128,upsample_factor=2),\n",
    "            Resblock(128, 64),\n",
    "            fl.Upsample(channels=64,upsample_factor=2),\n",
    "            Resblock(64, 32),\n",
    "            fl.Upsample(channels = 32,upsample_factor=2),\n",
    "            Resblock(32, output_channels),\n",
    "            fl.Conv2d(output_channels, output_channels, 1, padding=0),\n",
    "        )\n",
    "\n",
    "class VAE(fl.Module):\n",
    "    def __init__(self,latent_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.fc1 = fl.Linear(8*128*96, latent_dim)\n",
    "        self.fc2 = fl.Linear(8*128*96, latent_dim)\n",
    "        self.fc3 = fl.Linear(latent_dim, 8*128*96)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.randn(*mu.size()).to(device)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        z = self.fc3(z)\n",
    "        return self.decoder(z), mu, logvar\n",
    "    \n",
    "\n",
    "\n",
    "def load_dropout(chain : fl.Chain, dropout : float = 0.5):\n",
    "    for silu, parent in chain.walk(fl.SiLU):\n",
    "        DropoutAdapter(silu, dropout).inject(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-4\n",
    "num_epochs = 1\n",
    "vae = VAE(latent_dim=16)\n",
    "vae.to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters() , lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     vae\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mlen\u001b[39m(train_images)):\n\u001b[1;32m      5\u001b[0m         image \u001b[38;5;241m=\u001b[39m train_images[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m         y,mu,logvar \u001b[38;5;241m=\u001b[39m vae(image)\n",
      "File \u001b[0;32m~/work/vton/.venv/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    for i in tqdm.tqdm(range(len(train_images))):\n",
    "        image = train_images[i].unsqueeze(0).to(device)\n",
    "        y,mu,logvar = vae(image)\n",
    "        loss = (y-image).norm() + 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"epoch : {epoch} ,loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "    # Testing at the end of training\n",
    "    loss_val = 0\n",
    "    reconstructed_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        for j in range(len(test_images)):\n",
    "            images_val = test_images[j].unsqueeze(0).to(device)\n",
    "            result_val = vae(images_val)\n",
    "            y_val, mu_val, logvar_val = result_val\n",
    "            l = (y_val - images_val).norm() + 0.5 * torch.sum(logvar_val.exp() + mu_val.pow(2) - 1 - logvar_val)\n",
    "            loss_val += l\n",
    "\n",
    "            # # Append the reconstructed images for visualization\n",
    "            if j < 10:\n",
    "                image_shape = images_val.shape\n",
    "                concat = PIL.Image.new('RGB', (image_shape[-1]*2, image_shape[-2]))\n",
    "                concat.paste(utils.tensor_to_image(images_val.data), (0, 0))\n",
    "                concat.paste(utils.tensor_to_image(y_val.data), (image_shape[-1], 0))\n",
    "                reconstructed_images.append(concat)\n",
    "\n",
    "    images = [PIL.Image.fromarray(np.array(image)) for image in reconstructed_images]\n",
    "    for image in images:\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
