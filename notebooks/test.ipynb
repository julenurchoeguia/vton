{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python imports ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Local imports ###\n",
    "import sys\n",
    "sys.path.append('../') # define relative path for local imports\n",
    "from src.models.scm import SCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 384]) torch.Size([3, 512, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laure/vton/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load image from /var/hub/VITON-HD/train/image/03308_00.jpg\n",
    "from PIL import Image\n",
    "from refiners.fluxion.utils import image_to_tensor\n",
    "def load_image(image_path: str) -> Image.Image:\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = image_to_tensor(image).squeeze(0)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image '{file}': {e}\")\n",
    "            return None\n",
    "\n",
    "path_gt = '/var/hub/VITON-HD/test/image/00006_00.jpg'\n",
    "path_result = '/var/hub/VITON-HD-results-ladi-vton/paired/upper_body/00006_00.jpg'\n",
    "path_clothed = '/var/hub/VITON-HD/test/cloth/00006_00.jpg'\n",
    "image_gt = load_image(path_gt)\n",
    "image_result = load_image(path_result)\n",
    "image_cloth = load_image(path_clothed)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 384))  # Resize to 512x384\n",
    "])\n",
    "\n",
    "# resized_gt = transform(image_gt)\n",
    "resized_cloth = transform(image_cloth)\n",
    "# resized_cloth = image_cloth\n",
    "\n",
    "print(resized_cloth.shape,  image_result.shape)\n",
    "concat_image = torch.cat((resized_cloth, image_result),0)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.unsqueeze(concat_image, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Slicing.__init__() got an unexpected keyword argument 'end'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m middle_blk_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m      6\u001b[0m dec_blks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mSCM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmiddle_blk_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmiddle_blk_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43menc_blk_nums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_blks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_blk_nums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_blks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vton/notebooks/../src/models/scm.py:291\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, img_channel, width, middle_blk_num, enc_blk_nums, dec_blk_nums, scm_weight)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m, img_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, middle_blk_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, enc_blk_nums\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m], dec_blk_nums\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m], scm_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    289\u001b[0m         fl\u001b[38;5;241m.\u001b[39mChain(\n\u001b[1;32m    290\u001b[0m             fl\u001b[38;5;241m.\u001b[39mConv2d(in_channels\u001b[38;5;241m=\u001b[39mimg_channel, out_channels\u001b[38;5;241m=\u001b[39mwidth, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m--> 291\u001b[0m             NAFNet_UNet(in_channels\u001b[38;5;241m=\u001b[39mwidth, middle_blk_num\u001b[38;5;241m=\u001b[39mmiddle_blk_num, enc_blk_nums\u001b[38;5;241m=\u001b[39menc_blk_nums, dec_blk_nums\u001b[38;5;241m=\u001b[39mdec_blk_nums),\n\u001b[1;32m    292\u001b[0m             fl\u001b[38;5;241m.\u001b[39mConv2d(in_channels\u001b[38;5;241m=\u001b[39mwidth, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),              \n\u001b[1;32m    293\u001b[0m         ),\n\u001b[1;32m    294\u001b[0m         fl\u001b[38;5;241m.\u001b[39mChain(\n\u001b[1;32m    295\u001b[0m             fl\u001b[38;5;241m.\u001b[39mSlicing(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m),\n\u001b[1;32m    296\u001b[0m             fl\u001b[38;5;241m.\u001b[39mMultiply(scale \u001b[38;5;241m=\u001b[39m scm_weight),\n\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Slicing.__init__() got an unexpected keyword argument 'end'"
     ]
    }
   ],
   "source": [
    "img_channel = 6\n",
    "width = 32\n",
    "\n",
    "enc_blks = [2, 2, 4, 8]\n",
    "middle_blk_num = 12\n",
    "dec_blks = [2, 2, 2, 2]\n",
    "\n",
    "net = SCM(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = net(batch)\n",
    "    print(out.shape)\n",
    "\n",
    "out = out.squeeze(0) # remove batch dimension\n",
    "print(out.shape)\n",
    "out = out.permute(1, 2, 0) # rearrange dimensions from (height, width, channels) to (channels, height, width)\n",
    "print(out.shape)\n",
    "\n",
    "plt.imshow(out)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image_gt.squeeze(0).permute(1, 2, 0))\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
