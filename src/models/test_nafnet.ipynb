{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python import ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### Refiner import ###\n",
    "from refiners.fluxion import layers as fl\n",
    "from refiners.training_utils.trainer import seed_everything\n",
    "### Local import ###\n",
    "from models.architecture_utils import (\n",
    "    LayerNorm2d, # function to rewrite using refiners\n",
    "    Local_Base, # function to rewrite using refiners\n",
    "    AdaptiveAvgPool2d,\n",
    "    Dropout\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGate_base(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        # the .chunk() method splits a tensor into a specified number of chunks along a given dimension\n",
    "        return x1 * x2\n",
    "\n",
    "class SimplifiedChannelAttention(fl.Module):\n",
    "    def __init__(self, c, DW_Expand = 2) -> None:\n",
    "        super().__init__(\n",
    "            AdaptiveAvgPool2d(1),\n",
    "            fl.Conv2d(in_channels=(c*DW_Expand)//2, out_channels=(c*DW_Expand)//2, kernel_size=1, padding=0, stride=1, groups=1, bias=True),\n",
    "        )\n",
    "\n",
    "class MultiplyLayers(fl.Module):\n",
    "    def forward(self, x, layer):\n",
    "        new_x = x * layer(x)\n",
    "        return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NAFBlock_debase(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n",
    "        super().__init__()\n",
    "        dw_channel = c * DW_Expand\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
    "                               bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        \n",
    "        # Simplified Channel Attention\n",
    "        self.sca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
    "                      groups=1, bias=True),\n",
    "        )\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate_base()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c)\n",
    "        self.norm2 = LayerNorm2d(c)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sg(x)\n",
    "        x = x * self.sca(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout1(x)\n",
    "        y = inp + x * self.beta\n",
    "        x = self.conv4(self.norm2(y))\n",
    "        x = self.sg(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout2(x)\n",
    "        return y + x * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGate(fl.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)\n",
    "        # the .chunk() method splits a tensor into a specified number of chunks along a given dimension\n",
    "        return x1 * x2\n",
    "\n",
    "class CustomConditionedDropout(fl.Module):\n",
    "    def __init__(self, drop_out_rate) -> None:\n",
    "        super().__init__()\n",
    "        self.drop_out_rate = drop_out_rate\n",
    "    def forward(self, x):\n",
    "        if self.drop_out_rate > 0.:\n",
    "            x = Dropout(x)\n",
    "        else :\n",
    "            x = fl.Identity(x)\n",
    "        return x\n",
    "\n",
    "class NAFBlock(fl.Chain):\n",
    "    def __init__(self, c, DW_Expand = 2, FFN_Expand = 2, drop_out_rate = 0.) -> None:\n",
    "        super().__init__(\n",
    "            # TODO : x = inp\n",
    "            # x = self.norm1(x)\n",
    "            # LayerNorm2d(c),\n",
    "\n",
    "            fl.Conv2d(in_channels=c, out_channels=c*DW_Expand, kernel_size=1, padding=0, stride=1, groups=1, use_bias=True),\n",
    "            fl.Conv2d(in_channels=c*DW_Expand, out_channels=c*DW_Expand, kernel_size=3, padding=1, stride=1, groups=c*DW_Expand, use_bias=True),\n",
    "            SimpleGate(),\n",
    "\n",
    "            # x = x * self.sca(x) (sca : simplified channel attention)\n",
    "            # try with fl.Matmul() ? cf chain.py in refiners repo\n",
    "            # MultiplyLayers(SimplifiedChannelAttention(c, DW_Expand)), # ??\n",
    "\n",
    "            fl.Conv2d(in_channels=(c*DW_Expand)//2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, use_bias=True),\n",
    "            CustomConditionedDropout(drop_out_rate),\n",
    "\n",
    "            # TODO :  y = inp + x * self.beta\n",
    "            #         x = self.conv4(self.norm2(y))\n",
    "            # LayerNorm2d(inp + x * self.beta),\n",
    "\n",
    "            SimpleGate(),\n",
    "            fl.Conv2d(in_channels=FFN_Expand*c, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, use_bias=True),\n",
    "            CustomConditionedDropout(drop_out_rate)\n",
    "\n",
    "            # TODO : return y + x * self.gamma\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NAFBlock_debase(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=6)\n",
       "  (conv3): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (sca): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sg): SimpleGate_base()\n",
       "  (conv4): Conv2d(3, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv5): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (norm1): LayerNorm2d()\n",
       "  (norm2): LayerNorm2d()\n",
       "  (dropout1): Identity()\n",
       "  (dropout2): Identity()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(CHAIN) NAFBlock()\n",
       "    ├── Conv2d(in_channels=3, out_channels=6, kernel_size=(1, 1)) #1\n",
       "    ├── Conv2d(in_channels=6, out_channels=6, kernel_size=(3, 3), padding=(1, 1), groups=6) #2\n",
       "    ├── SimpleGate() #1\n",
       "    ├── Conv2d(in_channels=3, out_channels=3, kernel_size=(1, 1)) #3\n",
       "    ├── CustomConditionedDropout(drop_out_rate=0.0) #1\n",
       "    ├── SimpleGate() #2\n",
       "    ├── Conv2d(in_channels=6, out_channels=3, kernel_size=(1, 1)) #4\n",
       "    └── CustomConditionedDropout(drop_out_rate=0.0) #2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the NAFBlock\n",
    "naf_base = NAFBlock_debase(3)\n",
    "display(naf_base)\n",
    "# print(naf_base.forward(torch.rand(1, 3, 32, 32)))\n",
    "\n",
    "naf = NAFBlock(3, drop_out_rate=0.)\n",
    "display(naf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
