{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.new('RGB', (128, 128), color = 'red')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array(img)\n",
    "img_array = img_array/255\n",
    "img_array = img_array.transpose(2, 0, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = torch.from_numpy(img_array)\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image: Image) -> torch.Tensor:\n",
    "    img_array = np.array(image)\n",
    "    img_array = img_array/255\n",
    "    img_array = img_array.transpose(2, 0, 1).astype(np.float32)\n",
    "    img_tensor = torch.from_numpy(img_array)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    return img_tensor\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image:\n",
    "    img_array = tensor.squeeze(0).numpy()\n",
    "    img_array = img_array.transpose(1, 2, 0)\n",
    "    img_array = img_array*255\n",
    "    img_array = img_array.astype(np.uint8)\n",
    "    image = Image.fromarray(img_array)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = tuple[int, int, int]\n",
    "def create_empty_image(resolution: int, color: color=(0,0,0)) -> Image:\n",
    "    return Image.new('RGB', (resolution, resolution), color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = image_to_tensor(create_empty_image(128, color=(255, 0, 0)))\n",
    "target_tensor = image_to_tensor(create_empty_image(128, color=(0, 255, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, loss :  197.142807)\n",
      "step : 1, loss :  195.537323)\n",
      "step : 2, loss :  192.305786)\n",
      "step : 3, loss :  187.487595)\n",
      "step : 4, loss :  181.057846)\n",
      "step : 5, loss :  173.021378)\n",
      "step : 6, loss :  163.378113)\n",
      "step : 7, loss :  152.129456)\n",
      "step : 8, loss :  139.289001)\n",
      "step : 9, loss :  124.844475)\n",
      "step : 10, loss :  108.810753)\n",
      "step : 11, loss :  91.206413)\n",
      "step : 12, loss :  72.061035)\n",
      "step : 13, loss :  51.459583)\n",
      "step : 14, loss :  29.770758)\n",
      "step : 15, loss :  10.977227)\n",
      "step : 16, loss :  23.008425)\n",
      "step : 17, loss :  44.916439)\n",
      "step : 18, loss :  66.090446)\n",
      "step : 19, loss :  85.873528)\n",
      "step : 20, loss :  104.130600)\n",
      "step : 21, loss :  120.815468)\n",
      "step : 22, loss :  135.920227)\n",
      "step : 23, loss :  149.437439)\n",
      "step : 24, loss :  161.338669)\n",
      "step : 25, loss :  171.651688)\n",
      "step : 26, loss :  180.352875)\n",
      "step : 27, loss :  187.449570)\n",
      "step : 28, loss :  192.937790)\n",
      "step : 29, loss :  196.819717)\n",
      "step : 30, loss :  199.093430)\n",
      "step : 31, loss :  199.759125)\n",
      "step : 32, loss :  198.824387)\n",
      "step : 33, loss :  196.264404)\n",
      "step : 34, loss :  192.103668)\n",
      "step : 35, loss :  186.341339)\n",
      "step : 36, loss :  178.965225)\n",
      "step : 37, loss :  169.987442)\n",
      "step : 38, loss :  159.404053)\n",
      "step : 39, loss :  147.205688)\n",
      "step : 40, loss :  133.422577)\n",
      "step : 41, loss :  118.043686)\n",
      "step : 42, loss :  101.069473)\n",
      "step : 43, loss :  82.544098)\n",
      "step : 44, loss :  62.494877)\n",
      "step : 45, loss :  41.085533)\n",
      "step : 46, loss :  19.229834)\n",
      "step : 47, loss :  12.948452)\n",
      "step : 48, loss :  33.555077)\n",
      "step : 49, loss :  55.155762)\n",
      "step : 50, loss :  75.524307)\n",
      "step : 51, loss :  94.413635)\n",
      "step : 52, loss :  111.744873)\n",
      "step : 53, loss :  127.500313)\n",
      "step : 54, loss :  141.656815)\n",
      "step : 55, loss :  154.214371)\n",
      "step : 56, loss :  165.181488)\n",
      "step : 57, loss :  174.535019)\n",
      "step : 58, loss :  182.281769)\n",
      "step : 59, loss :  188.422913)\n",
      "step : 60, loss :  192.957214)\n",
      "step : 61, loss :  195.892212)\n",
      "step : 62, loss :  197.216446)\n",
      "step : 63, loss :  196.916489)\n",
      "step : 64, loss :  195.018875)\n",
      "step : 65, loss :  191.519241)\n",
      "step : 66, loss :  186.402374)\n",
      "step : 67, loss :  179.680130)\n",
      "step : 68, loss :  171.345917)\n",
      "step : 69, loss :  161.417542)\n",
      "step : 70, loss :  149.876022)\n",
      "step : 71, loss :  136.729507)\n",
      "step : 72, loss :  121.992676)\n",
      "step : 73, loss :  105.660957)\n",
      "step : 74, loss :  87.756508)\n",
      "step : 75, loss :  68.304848)\n",
      "step : 76, loss :  47.399555)\n",
      "step : 77, loss :  25.471483)\n",
      "step : 78, loss :  9.361043)\n",
      "step : 79, loss :  26.452318)\n",
      "step : 80, loss :  48.321426)\n",
      "step : 81, loss :  69.119507)\n",
      "step : 82, loss :  88.463730)\n",
      "step : 83, loss :  106.256828)\n",
      "step : 84, loss :  122.476074)\n",
      "step : 85, loss :  137.101456)\n",
      "step : 86, loss :  150.123245)\n",
      "step : 87, loss :  161.555084)\n",
      "step : 88, loss :  171.370697)\n",
      "step : 89, loss :  179.583633)\n",
      "step : 90, loss :  186.199539)\n",
      "step : 91, loss :  191.189499)\n",
      "step : 92, loss :  194.576035)\n",
      "step : 93, loss :  196.363113)\n",
      "step : 94, loss :  196.525757)\n",
      "step : 95, loss :  195.094940)\n",
      "step : 96, loss :  192.040161)\n",
      "step : 97, loss :  187.389725)\n",
      "step : 98, loss :  181.125473)\n",
      "step : 99, loss :  173.253769)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAB30lEQVR4Ae3TsQ1BARRG4UdeSChsIAqr2ExjA5XOCLawgA00opAQnTG+wrkLnJtz8k+e193isBo6YeBzmY3v/eZ12wp6zGF9XE3TYA0UwPofClAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjWwAOMC7Pj/tljr/4V/x4+v4AGdsOgyBfwywAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init_tensor.grad  \n",
    "conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n",
    "lr = 1e-5\n",
    "num_step = 100\n",
    "\n",
    "for step in range(num_step):\n",
    "    y = conv(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step : {step}, loss : {loss.item(): 3f})')\n",
    "        for param in conv.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr * param.grad\n",
    "    # conv.zero_grad()\n",
    "\n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAB30lEQVR4Ae3TsQ1BARRG4UdeSChsIAqr2ExjA5XOCLawgA00opAQnTG+wrkLnJtz8k+e193isBo6YeBzmY3v/eZ12wp6zGF9XE3TYA0UwPofClAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjWwAOMC7Pj/tljr/4V/x4+v4AGdsOgyBfwywAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(conv(init_tensor).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDWooor80PyAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABr0lEQVR4Ae3TwREAMAiEQJP+ezbpYj/YADPgndnpoIEL2aG/gQLgNyhAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4FFAAbwPgWUABsAONbQAGwAYxvAQXABjC+BRQAG8D4FlAAbADjW0ABsAGMbwEFwAYwvgUUABvA+BZQAGwA41tAAbABjG8BBcAGML4F4AAPzuIB/0LykZIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int=1, out_features: int=1) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "linear = Linear(in_features=3, out_features=3)\n",
    "x = torch.randn(1, 3)\n",
    "target_tensor = torch.randn(1, 3)\n",
    "y = linear(x)\n",
    "loss = ((y - target_tensor)**2).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2100,  0.0452, -0.6306],\n",
       "        [ 0.2792, -0.0601,  0.8381],\n",
       "        [-0.2378,  0.0512, -0.7139]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resblock(nn.Module):\n",
    "    def __init__(self, in_channels : int = 1, out_channels : int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.conv2(y)\n",
    "        # y = self.activation(y)\n",
    "        return y + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/laure/vton/laure/autoencoder_laure.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_step):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     y \u001b[39m=\u001b[39m block(init_tensor)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m (y \u001b[39m-\u001b[39;49m target_tensor)\u001b[39m.\u001b[39mnorm()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/laure/vton/laure/autoencoder_laure.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "block = Resblock(in_channels = 3, out_channels =3)\n",
    "\n",
    "lr = 1e-5\n",
    "num_step = 10000\n",
    "for step in range(num_step):\n",
    "    y = block(init_tensor)\n",
    "    loss = (y - target_tensor).norm()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f'step : {step}, loss : {loss.item(): 3f})')\n",
    "        for param in block.parameters():\n",
    "            assert param.grad is not None\n",
    "            param -= lr * param.grad\n",
    "    # conv.zero_grad()\n",
    "            \n",
    "result = conv(init_tensor)\n",
    "tensor_to_image(result.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.maxpool(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.conv4(y)\n",
    "        y = self.maxpool(y)\n",
    "        # y = self.silu(y)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, output_channels, 3, padding=1)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv1(x)\n",
    "        y = self.upsample(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.upsample(y)\n",
    "        y = self.silu(y)\n",
    "        y = self.conv4(y)\n",
    "        # y = self.silu(y)\n",
    "        y = self.upsample(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() # type: ignore\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self) -> None:\n",
    "        self.data = list(range(100))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'Dataset(len={len(self)})'\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "    \n",
    "    def __getitem__(self, key : str|int) -> int:\n",
    "        match key:\n",
    "            case key if isinstance(key, str):\n",
    "                raise ValueError('Dataset does not take string as index.')\n",
    "            case _:\n",
    "                return self.data[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset[2]\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 124.7420883178711\n",
      "step 1 : loss 124.46776580810547\n",
      "step 2 : loss 124.17717742919922\n",
      "step 3 : loss 123.85760498046875\n",
      "step 4 : loss 123.49578094482422\n",
      "step 5 : loss 123.06446838378906\n",
      "step 6 : loss 122.53224182128906\n",
      "step 7 : loss 121.86459350585938\n",
      "step 8 : loss 121.01408386230469\n",
      "step 9 : loss 119.90970611572266\n",
      "step 10 : loss 118.47626495361328\n",
      "step 11 : loss 116.60877990722656\n",
      "step 12 : loss 114.1825942993164\n",
      "step 13 : loss 111.03224182128906\n",
      "step 14 : loss 106.95996856689453\n",
      "step 15 : loss 101.7313461303711\n",
      "step 16 : loss 95.07386016845703\n",
      "step 17 : loss 86.69741821289062\n",
      "step 18 : loss 76.39803314208984\n",
      "step 19 : loss 64.34163665771484\n",
      "step 20 : loss 52.09644317626953\n",
      "step 21 : loss 45.83066177368164\n",
      "step 22 : loss 52.42335891723633\n",
      "step 23 : loss 57.16010284423828\n",
      "step 24 : loss 54.325843811035156\n",
      "step 25 : loss 47.93583679199219\n",
      "step 26 : loss 42.39128494262695\n",
      "step 27 : loss 40.17490005493164\n",
      "step 28 : loss 40.76582336425781\n",
      "step 29 : loss 42.22091293334961\n",
      "step 30 : loss 43.23994445800781\n",
      "step 31 : loss 43.36878967285156\n",
      "step 32 : loss 42.584896087646484\n",
      "step 33 : loss 41.04551696777344\n",
      "step 34 : loss 39.00263977050781\n",
      "step 35 : loss 36.78874588012695\n",
      "step 36 : loss 34.79612350463867\n",
      "step 37 : loss 33.39086151123047\n",
      "step 38 : loss 32.71719741821289\n",
      "step 39 : loss 32.54681396484375\n",
      "step 40 : loss 32.39885711669922\n",
      "step 41 : loss 31.87609100341797\n",
      "step 42 : loss 30.88205337524414\n",
      "step 43 : loss 29.59592628479004\n",
      "step 44 : loss 28.325220108032227\n",
      "step 45 : loss 27.332578659057617\n",
      "step 46 : loss 26.714054107666016\n",
      "step 47 : loss 26.389036178588867\n",
      "step 48 : loss 26.176668167114258\n",
      "step 49 : loss 25.88829803466797\n",
      "step 50 : loss 25.391101837158203\n",
      "step 51 : loss 24.652894973754883\n",
      "step 52 : loss 23.749469757080078\n",
      "step 53 : loss 22.84598731994629\n",
      "step 54 : loss 22.131851196289062\n",
      "step 55 : loss 21.699966430664062\n",
      "step 56 : loss 21.441234588623047\n",
      "step 57 : loss 21.114601135253906\n",
      "step 58 : loss 20.563724517822266\n",
      "step 59 : loss 19.866436004638672\n",
      "step 60 : loss 19.273889541625977\n",
      "step 61 : loss 18.962894439697266\n",
      "step 62 : loss 18.81789779663086\n",
      "step 63 : loss 18.547780990600586\n",
      "step 64 : loss 18.028837203979492\n",
      "step 65 : loss 17.455495834350586\n",
      "step 66 : loss 17.114709854125977\n",
      "step 67 : loss 16.97042465209961\n",
      "step 68 : loss 16.706951141357422\n",
      "step 69 : loss 16.255329132080078\n",
      "step 70 : loss 15.862043380737305\n",
      "step 71 : loss 15.659345626831055\n",
      "step 72 : loss 15.482170104980469\n",
      "step 73 : loss 15.163753509521484\n",
      "step 74 : loss 14.792868614196777\n",
      "step 75 : loss 14.574877738952637\n",
      "step 76 : loss 14.438593864440918\n",
      "step 77 : loss 14.152137756347656\n",
      "step 78 : loss 13.845890045166016\n",
      "step 79 : loss 13.706380844116211\n",
      "step 80 : loss 13.534822463989258\n",
      "step 81 : loss 13.24682903289795\n",
      "step 82 : loss 13.051287651062012\n",
      "step 83 : loss 12.917190551757812\n",
      "step 84 : loss 12.674283981323242\n",
      "step 85 : loss 12.456520080566406\n",
      "step 86 : loss 12.319266319274902\n",
      "step 87 : loss 12.09184741973877\n",
      "step 88 : loss 11.887382507324219\n",
      "step 89 : loss 11.751428604125977\n",
      "step 90 : loss 11.513233184814453\n",
      "step 91 : loss 11.358182907104492\n",
      "step 92 : loss 11.180697441101074\n",
      "step 93 : loss 10.98099422454834\n",
      "step 94 : loss 10.842533111572266\n",
      "step 95 : loss 10.631911277770996\n",
      "step 96 : loss 10.490951538085938\n",
      "step 97 : loss 10.293822288513184\n",
      "step 98 : loss 10.142075538635254\n",
      "step 99 : loss 9.95730209350586\n",
      "step 100 : loss 9.802809715270996\n",
      "step 101 : loss 9.627227783203125\n",
      "step 102 : loss 9.481529235839844\n",
      "step 103 : loss 9.310887336730957\n",
      "step 104 : loss 9.176655769348145\n",
      "step 105 : loss 9.010056495666504\n",
      "step 106 : loss 8.882487297058105\n",
      "step 107 : loss 8.721991539001465\n",
      "step 108 : loss 8.590359687805176\n",
      "step 109 : loss 8.447903633117676\n",
      "step 110 : loss 8.306612014770508\n",
      "step 111 : loss 8.194684028625488\n",
      "step 112 : loss 8.095148086547852\n",
      "step 113 : loss 8.098938941955566\n",
      "step 114 : loss 8.197709083557129\n",
      "step 115 : loss 7.912267208099365\n",
      "step 116 : loss 7.597428321838379\n",
      "step 117 : loss 7.650055408477783\n",
      "step 118 : loss 7.619634628295898\n",
      "step 119 : loss 7.332236289978027\n",
      "step 120 : loss 7.268025875091553\n",
      "step 121 : loss 7.291615009307861\n",
      "step 122 : loss 7.085897922515869\n",
      "step 123 : loss 6.969330310821533\n",
      "step 124 : loss 6.97951602935791\n",
      "step 125 : loss 6.8471293449401855\n",
      "step 126 : loss 6.706279754638672\n",
      "step 127 : loss 6.678657531738281\n",
      "step 128 : loss 6.61463737487793\n",
      "step 129 : loss 6.483481407165527\n",
      "step 130 : loss 6.3928704261779785\n",
      "step 131 : loss 6.353971481323242\n",
      "step 132 : loss 6.288694381713867\n",
      "step 133 : loss 6.16945219039917\n",
      "step 134 : loss 6.083752632141113\n",
      "step 135 : loss 6.058629035949707\n",
      "step 136 : loss 5.992129802703857\n",
      "step 137 : loss 5.878505706787109\n",
      "step 138 : loss 5.813688278198242\n",
      "step 139 : loss 5.776245594024658\n",
      "step 140 : loss 5.709991931915283\n",
      "step 141 : loss 5.636614799499512\n",
      "step 142 : loss 5.569769859313965\n",
      "step 143 : loss 5.501336097717285\n",
      "step 144 : loss 5.44733190536499\n",
      "step 145 : loss 5.410060405731201\n",
      "step 146 : loss 5.3609161376953125\n",
      "step 147 : loss 5.294526100158691\n",
      "step 148 : loss 5.238359451293945\n",
      "step 149 : loss 5.211272239685059\n",
      "step 150 : loss 5.219822883605957\n",
      "step 151 : loss 5.3059892654418945\n",
      "step 152 : loss 5.400583267211914\n",
      "step 153 : loss 5.3408708572387695\n",
      "step 154 : loss 5.026799201965332\n",
      "step 155 : loss 4.896524906158447\n",
      "step 156 : loss 4.988734722137451\n",
      "step 157 : loss 5.065941333770752\n",
      "step 158 : loss 4.959782123565674\n",
      "step 159 : loss 4.770920753479004\n",
      "step 160 : loss 4.730282306671143\n",
      "step 161 : loss 4.8060784339904785\n",
      "step 162 : loss 4.806081771850586\n",
      "step 163 : loss 4.682000637054443\n",
      "step 164 : loss 4.567831993103027\n",
      "step 165 : loss 4.572911262512207\n",
      "step 166 : loss 4.621994495391846\n",
      "step 167 : loss 4.577053546905518\n",
      "step 168 : loss 4.478930473327637\n",
      "step 169 : loss 4.419254302978516\n",
      "step 170 : loss 4.414213180541992\n",
      "step 171 : loss 4.419983863830566\n",
      "step 172 : loss 4.39603328704834\n",
      "step 173 : loss 4.34735631942749\n",
      "step 174 : loss 4.287298202514648\n",
      "step 175 : loss 4.248508453369141\n",
      "step 176 : loss 4.240312099456787\n",
      "step 177 : loss 4.24079704284668\n",
      "step 178 : loss 4.224051475524902\n",
      "step 179 : loss 4.1808037757873535\n",
      "step 180 : loss 4.136662483215332\n",
      "step 181 : loss 4.101368427276611\n",
      "step 182 : loss 4.075906276702881\n",
      "step 183 : loss 4.0595622062683105\n",
      "step 184 : loss 4.053538799285889\n",
      "step 185 : loss 4.053393363952637\n",
      "step 186 : loss 4.043428897857666\n",
      "step 187 : loss 4.015964508056641\n",
      "step 188 : loss 3.975687265396118\n",
      "step 189 : loss 3.9430861473083496\n",
      "step 190 : loss 3.9344356060028076\n",
      "step 191 : loss 3.9575438499450684\n",
      "step 192 : loss 4.020942687988281\n",
      "step 193 : loss 4.064566612243652\n",
      "step 194 : loss 4.053803443908691\n",
      "step 195 : loss 3.9278945922851562\n",
      "step 196 : loss 3.820314407348633\n",
      "step 197 : loss 3.76680064201355\n",
      "step 198 : loss 3.7610867023468018\n",
      "step 199 : loss 3.779017925262451\n",
      "step 200 : loss 3.7940714359283447\n",
      "step 201 : loss 3.8029093742370605\n",
      "step 202 : loss 3.781891345977783\n",
      "step 203 : loss 3.7527594566345215\n",
      "step 204 : loss 3.7101118564605713\n",
      "step 205 : loss 3.6714656352996826\n",
      "step 206 : loss 3.635220527648926\n",
      "step 207 : loss 3.6031055450439453\n",
      "step 208 : loss 3.5798394680023193\n",
      "step 209 : loss 3.5703043937683105\n",
      "step 210 : loss 3.580291986465454\n",
      "step 211 : loss 3.6002962589263916\n",
      "step 212 : loss 3.6179771423339844\n",
      "step 213 : loss 3.5989649295806885\n",
      "step 214 : loss 3.556774854660034\n",
      "step 215 : loss 3.5019643306732178\n",
      "step 216 : loss 3.4613773822784424\n",
      "step 217 : loss 3.4368765354156494\n",
      "step 218 : loss 3.4267640113830566\n",
      "step 219 : loss 3.4243669509887695\n",
      "step 220 : loss 3.424048900604248\n",
      "step 221 : loss 3.4160118103027344\n",
      "step 222 : loss 3.397462844848633\n",
      "step 223 : loss 3.3689019680023193\n",
      "step 224 : loss 3.339576005935669\n",
      "step 225 : loss 3.3147637844085693\n",
      "step 226 : loss 3.296766519546509\n",
      "step 227 : loss 3.285126209259033\n",
      "step 228 : loss 3.2810511589050293\n",
      "step 229 : loss 3.28974986076355\n",
      "step 230 : loss 3.3260250091552734\n",
      "step 231 : loss 3.402541160583496\n",
      "step 232 : loss 3.5249762535095215\n",
      "step 233 : loss 3.5399041175842285\n",
      "step 234 : loss 3.464991807937622\n",
      "step 235 : loss 3.324345588684082\n",
      "step 236 : loss 3.2574315071105957\n",
      "step 237 : loss 3.2379887104034424\n",
      "step 238 : loss 3.2397093772888184\n",
      "step 239 : loss 3.244556427001953\n",
      "step 240 : loss 3.2481155395507812\n",
      "step 241 : loss 3.2704074382781982\n",
      "step 242 : loss 3.2760515213012695\n",
      "step 243 : loss 3.2626614570617676\n",
      "step 244 : loss 3.1863584518432617\n",
      "step 245 : loss 3.1086504459381104\n",
      "step 246 : loss 3.059558391571045\n",
      "step 247 : loss 3.0536975860595703\n",
      "step 248 : loss 3.084487199783325\n",
      "step 249 : loss 3.126924991607666\n",
      "step 250 : loss 3.1544392108917236\n",
      "step 251 : loss 3.1277530193328857\n",
      "step 252 : loss 3.090803384780884\n",
      "step 253 : loss 3.0575380325317383\n",
      "step 254 : loss 3.044386863708496\n",
      "step 255 : loss 3.0314395427703857\n",
      "step 256 : loss 3.010720729827881\n",
      "step 257 : loss 2.9795780181884766\n",
      "step 258 : loss 2.9480409622192383\n",
      "step 259 : loss 2.9240059852600098\n",
      "step 260 : loss 2.9103641510009766\n",
      "step 261 : loss 2.905458450317383\n",
      "step 262 : loss 2.9056379795074463\n",
      "step 263 : loss 2.9061834812164307\n",
      "step 264 : loss 2.9038448333740234\n",
      "step 265 : loss 2.895895481109619\n",
      "step 266 : loss 2.883704662322998\n",
      "step 267 : loss 2.8683664798736572\n",
      "step 268 : loss 2.8527441024780273\n",
      "step 269 : loss 2.838834285736084\n",
      "step 270 : loss 2.83004093170166\n",
      "step 271 : loss 2.8341939449310303\n",
      "step 272 : loss 2.869131326675415\n",
      "step 273 : loss 2.9770445823669434\n",
      "step 274 : loss 3.1356616020202637\n",
      "step 275 : loss 3.227943181991577\n",
      "step 276 : loss 3.0028133392333984\n",
      "step 277 : loss 2.8281495571136475\n",
      "step 278 : loss 2.751692771911621\n",
      "step 279 : loss 2.743196725845337\n",
      "step 280 : loss 2.786186456680298\n",
      "step 281 : loss 2.8826723098754883\n",
      "step 282 : loss 3.0088188648223877\n",
      "step 283 : loss 2.981735944747925\n",
      "step 284 : loss 2.8724260330200195\n",
      "step 285 : loss 2.7540717124938965\n",
      "step 286 : loss 2.7006778717041016\n",
      "step 287 : loss 2.69266414642334\n",
      "step 288 : loss 2.716046094894409\n",
      "step 289 : loss 2.759347677230835\n",
      "step 290 : loss 2.792771816253662\n",
      "step 291 : loss 2.7956511974334717\n",
      "step 292 : loss 2.749516725540161\n",
      "step 293 : loss 2.708773612976074\n",
      "step 294 : loss 2.688270092010498\n",
      "step 295 : loss 2.705587387084961\n",
      "step 296 : loss 2.7359185218811035\n",
      "step 297 : loss 2.7581851482391357\n",
      "step 298 : loss 2.7167398929595947\n",
      "step 299 : loss 2.653010129928589\n",
      "step 300 : loss 2.596038341522217\n",
      "step 301 : loss 2.5683131217956543\n",
      "step 302 : loss 2.5655810832977295\n",
      "step 303 : loss 2.582334041595459\n",
      "step 304 : loss 2.6103880405426025\n",
      "step 305 : loss 2.6383559703826904\n",
      "step 306 : loss 2.644911050796509\n",
      "step 307 : loss 2.6361677646636963\n",
      "step 308 : loss 2.622654676437378\n",
      "step 309 : loss 2.6433887481689453\n",
      "step 310 : loss 2.6795899868011475\n",
      "step 311 : loss 2.719484329223633\n",
      "step 312 : loss 2.666372060775757\n",
      "step 313 : loss 2.5903213024139404\n",
      "step 314 : loss 2.5176897048950195\n",
      "step 315 : loss 2.4796149730682373\n",
      "step 316 : loss 2.4657859802246094\n",
      "step 317 : loss 2.4684016704559326\n",
      "step 318 : loss 2.480923652648926\n",
      "step 319 : loss 2.4997572898864746\n",
      "step 320 : loss 2.5174248218536377\n",
      "step 321 : loss 2.533886432647705\n",
      "step 322 : loss 2.543691396713257\n",
      "step 323 : loss 2.5699822902679443\n",
      "step 324 : loss 2.5934810638427734\n",
      "step 325 : loss 2.626189947128296\n",
      "step 326 : loss 2.5847692489624023\n",
      "step 327 : loss 2.525177240371704\n",
      "step 328 : loss 2.4508447647094727\n",
      "step 329 : loss 2.4019625186920166\n",
      "step 330 : loss 2.3718667030334473\n",
      "step 331 : loss 2.355560302734375\n",
      "step 332 : loss 2.3468551635742188\n",
      "step 333 : loss 2.3426899909973145\n",
      "step 334 : loss 2.342430353164673\n",
      "step 335 : loss 2.3485891819000244\n",
      "step 336 : loss 2.3670146465301514\n",
      "step 337 : loss 2.4099090099334717\n",
      "step 338 : loss 2.478245258331299\n",
      "step 339 : loss 2.5616672039031982\n",
      "step 340 : loss 2.5590109825134277\n",
      "step 341 : loss 2.526062488555908\n",
      "step 342 : loss 2.462829828262329\n",
      "step 343 : loss 2.443964719772339\n",
      "step 344 : loss 2.4184482097625732\n",
      "step 345 : loss 2.390634536743164\n",
      "step 346 : loss 2.3439149856567383\n",
      "step 347 : loss 2.3041203022003174\n",
      "step 348 : loss 2.274639129638672\n",
      "step 349 : loss 2.2589755058288574\n",
      "step 350 : loss 2.2570199966430664\n",
      "step 351 : loss 2.279017210006714\n",
      "step 352 : loss 2.345592975616455\n",
      "step 353 : loss 2.4829251766204834\n",
      "step 354 : loss 2.55033016204834\n",
      "step 355 : loss 2.4954638481140137\n",
      "step 356 : loss 2.3360815048217773\n",
      "step 357 : loss 2.2500202655792236\n",
      "step 358 : loss 2.215101718902588\n",
      "step 359 : loss 2.2079570293426514\n",
      "step 360 : loss 2.2160542011260986\n",
      "step 361 : loss 2.2380661964416504\n",
      "step 362 : loss 2.2698733806610107\n",
      "step 363 : loss 2.2994425296783447\n",
      "step 364 : loss 2.3048622608184814\n",
      "step 365 : loss 2.280806303024292\n",
      "step 366 : loss 2.258942127227783\n",
      "step 367 : loss 2.268721580505371\n",
      "step 368 : loss 2.3506791591644287\n",
      "step 369 : loss 2.432752847671509\n",
      "step 370 : loss 2.4468538761138916\n",
      "step 371 : loss 2.2966959476470947\n",
      "step 372 : loss 2.1913864612579346\n",
      "step 373 : loss 2.1377017498016357\n",
      "step 374 : loss 2.117248773574829\n",
      "step 375 : loss 2.112138032913208\n",
      "step 376 : loss 2.1178581714630127\n",
      "step 377 : loss 2.1376240253448486\n",
      "step 378 : loss 2.184481382369995\n",
      "step 379 : loss 2.2628908157348633\n",
      "step 380 : loss 2.359591007232666\n",
      "step 381 : loss 2.3494997024536133\n",
      "step 382 : loss 2.3075151443481445\n",
      "step 383 : loss 2.242692470550537\n",
      "step 384 : loss 2.224867343902588\n",
      "step 385 : loss 2.1954782009124756\n",
      "step 386 : loss 2.1612253189086914\n",
      "step 387 : loss 2.1158039569854736\n",
      "step 388 : loss 2.0813188552856445\n",
      "step 389 : loss 2.058159112930298\n",
      "step 390 : loss 2.0452327728271484\n",
      "step 391 : loss 2.039963722229004\n",
      "step 392 : loss 2.047562837600708\n",
      "step 393 : loss 2.0859997272491455\n",
      "step 394 : loss 2.1991002559661865\n",
      "step 395 : loss 2.34511399269104\n",
      "step 396 : loss 2.390577793121338\n",
      "step 397 : loss 2.1847307682037354\n",
      "step 398 : loss 2.059190273284912\n",
      "step 399 : loss 2.007840871810913\n",
      "step 400 : loss 1.9915075302124023\n",
      "step 401 : loss 1.9911983013153076\n",
      "step 402 : loss 2.0076231956481934\n",
      "step 403 : loss 2.0575625896453857\n",
      "step 404 : loss 2.1589059829711914\n",
      "step 405 : loss 2.3023948669433594\n",
      "step 406 : loss 2.2684288024902344\n",
      "step 407 : loss 2.187947988510132\n",
      "step 408 : loss 2.1162753105163574\n",
      "step 409 : loss 2.1164724826812744\n",
      "step 410 : loss 2.114131212234497\n",
      "step 411 : loss 2.081826686859131\n",
      "step 412 : loss 2.0257084369659424\n",
      "step 413 : loss 1.9781053066253662\n",
      "step 414 : loss 1.9497039318084717\n",
      "step 415 : loss 1.9352539777755737\n",
      "step 416 : loss 1.929860234260559\n",
      "step 417 : loss 1.9349843263626099\n",
      "step 418 : loss 1.9687532186508179\n",
      "step 419 : loss 2.0681889057159424\n",
      "step 420 : loss 2.256274461746216\n",
      "step 421 : loss 2.248155117034912\n",
      "step 422 : loss 2.1174726486206055\n",
      "step 423 : loss 1.9840103387832642\n",
      "step 424 : loss 1.931505560874939\n",
      "step 425 : loss 1.9177143573760986\n",
      "step 426 : loss 1.926999807357788\n",
      "step 427 : loss 1.9558649063110352\n",
      "step 428 : loss 1.9994176626205444\n",
      "step 429 : loss 2.0341603755950928\n",
      "step 430 : loss 2.031766653060913\n",
      "step 431 : loss 1.99805748462677\n",
      "step 432 : loss 1.9899290800094604\n",
      "step 433 : loss 2.03865122795105\n",
      "step 434 : loss 2.163914680480957\n",
      "step 435 : loss 2.1532697677612305\n",
      "step 436 : loss 2.0583271980285645\n",
      "step 437 : loss 1.935869574546814\n",
      "step 438 : loss 1.8761730194091797\n",
      "step 439 : loss 1.8506311178207397\n",
      "step 440 : loss 1.8465282917022705\n",
      "step 441 : loss 1.8652665615081787\n",
      "step 442 : loss 1.9374750852584839\n",
      "step 443 : loss 2.085587978363037\n",
      "step 444 : loss 2.2351150512695312\n",
      "step 445 : loss 2.05721378326416\n",
      "step 446 : loss 1.9081392288208008\n",
      "step 447 : loss 1.8375887870788574\n",
      "step 448 : loss 1.8124585151672363\n",
      "step 449 : loss 1.801201581954956\n",
      "step 450 : loss 1.7964125871658325\n",
      "step 451 : loss 1.8006677627563477\n",
      "step 452 : loss 1.8270373344421387\n",
      "step 453 : loss 1.9062968492507935\n",
      "step 454 : loss 2.080129861831665\n",
      "step 455 : loss 2.1451947689056396\n",
      "step 456 : loss 2.0530083179473877\n",
      "step 457 : loss 1.8897758722305298\n",
      "step 458 : loss 1.8299297094345093\n",
      "step 459 : loss 1.8274420499801636\n",
      "step 460 : loss 1.8638118505477905\n",
      "step 461 : loss 1.9122016429901123\n",
      "step 462 : loss 1.9404094219207764\n",
      "step 463 : loss 1.902071237564087\n",
      "step 464 : loss 1.8462815284729004\n",
      "step 465 : loss 1.8017010688781738\n",
      "step 466 : loss 1.7837610244750977\n",
      "step 467 : loss 1.793137550354004\n",
      "step 468 : loss 1.8563798666000366\n",
      "step 469 : loss 1.9776188135147095\n",
      "step 470 : loss 2.096630334854126\n",
      "step 471 : loss 1.962978720664978\n",
      "step 472 : loss 1.850612759590149\n",
      "step 473 : loss 1.797640323638916\n",
      "step 474 : loss 1.7942101955413818\n",
      "step 475 : loss 1.8108505010604858\n",
      "step 476 : loss 1.833648681640625\n",
      "step 477 : loss 1.842747449874878\n",
      "step 478 : loss 1.838782548904419\n",
      "step 479 : loss 1.8335514068603516\n",
      "step 480 : loss 1.8724619150161743\n",
      "step 481 : loss 1.942064881324768\n",
      "step 482 : loss 2.016371488571167\n",
      "step 483 : loss 1.9100762605667114\n",
      "step 484 : loss 1.8121041059494019\n",
      "step 485 : loss 1.7461894750595093\n",
      "step 486 : loss 1.717808723449707\n",
      "step 487 : loss 1.706406831741333\n",
      "step 488 : loss 1.710052728652954\n",
      "step 489 : loss 1.7300655841827393\n",
      "step 490 : loss 1.7815883159637451\n",
      "step 491 : loss 1.8598428964614868\n",
      "step 492 : loss 1.9560068845748901\n",
      "step 493 : loss 1.9216384887695312\n",
      "step 494 : loss 1.8822046518325806\n",
      "step 495 : loss 1.8261737823486328\n",
      "step 496 : loss 1.7982633113861084\n",
      "step 497 : loss 1.759548306465149\n",
      "step 498 : loss 1.722900152206421\n",
      "step 499 : loss 1.6938847303390503\n",
      "step 500 : loss 1.6797996759414673\n",
      "step 501 : loss 1.6824908256530762\n",
      "step 502 : loss 1.7144651412963867\n",
      "step 503 : loss 1.7950692176818848\n",
      "step 504 : loss 1.9348801374435425\n",
      "step 505 : loss 1.927825927734375\n",
      "step 506 : loss 1.8458259105682373\n",
      "step 507 : loss 1.74216890335083\n",
      "step 508 : loss 1.7027796506881714\n",
      "step 509 : loss 1.6869525909423828\n",
      "step 510 : loss 1.684944987297058\n",
      "step 511 : loss 1.6844197511672974\n",
      "step 512 : loss 1.6904692649841309\n",
      "step 513 : loss 1.6979721784591675\n",
      "step 514 : loss 1.7221328020095825\n",
      "step 515 : loss 1.7619333267211914\n",
      "step 516 : loss 1.848618745803833\n",
      "step 517 : loss 1.8728617429733276\n",
      "step 518 : loss 1.8536053895950317\n",
      "step 519 : loss 1.7488007545471191\n",
      "step 520 : loss 1.6833988428115845\n",
      "step 521 : loss 1.6448837518692017\n",
      "step 522 : loss 1.6261177062988281\n",
      "step 523 : loss 1.6190087795257568\n",
      "step 524 : loss 1.6225825548171997\n",
      "step 525 : loss 1.6366103887557983\n",
      "step 526 : loss 1.6638938188552856\n",
      "step 527 : loss 1.7053492069244385\n",
      "step 528 : loss 1.7709007263183594\n",
      "step 529 : loss 1.8137847185134888\n",
      "step 530 : loss 1.8465172052383423\n",
      "step 531 : loss 1.7722588777542114\n",
      "step 532 : loss 1.7152159214019775\n",
      "step 533 : loss 1.655995488166809\n",
      "step 534 : loss 1.6202512979507446\n",
      "step 535 : loss 1.5964186191558838\n",
      "step 536 : loss 1.5894043445587158\n",
      "step 537 : loss 1.5989974737167358\n",
      "step 538 : loss 1.6432615518569946\n",
      "step 539 : loss 1.734748125076294\n",
      "step 540 : loss 1.8693856000900269\n",
      "step 541 : loss 1.812119483947754\n",
      "step 542 : loss 1.7260236740112305\n",
      "step 543 : loss 1.6520367860794067\n",
      "step 544 : loss 1.6240875720977783\n",
      "step 545 : loss 1.6059504747390747\n",
      "step 546 : loss 1.5932574272155762\n",
      "step 547 : loss 1.5861369371414185\n",
      "step 548 : loss 1.5952814817428589\n",
      "step 549 : loss 1.631123423576355\n",
      "step 550 : loss 1.7218433618545532\n",
      "step 551 : loss 1.8018178939819336\n",
      "step 552 : loss 1.8214149475097656\n",
      "step 553 : loss 1.684476375579834\n",
      "step 554 : loss 1.6077115535736084\n",
      "step 555 : loss 1.571054220199585\n",
      "step 556 : loss 1.5588432550430298\n",
      "step 557 : loss 1.5523320436477661\n",
      "step 558 : loss 1.5543140172958374\n",
      "step 559 : loss 1.5637019872665405\n",
      "step 560 : loss 1.5907783508300781\n",
      "step 561 : loss 1.634250283241272\n",
      "step 562 : loss 1.7172106504440308\n",
      "step 563 : loss 1.753588318824768\n",
      "step 564 : loss 1.7591959238052368\n",
      "step 565 : loss 1.661780834197998\n",
      "step 566 : loss 1.598819613456726\n",
      "step 567 : loss 1.5568161010742188\n",
      "step 568 : loss 1.5334886312484741\n",
      "step 569 : loss 1.5198938846588135\n",
      "step 570 : loss 1.5181092023849487\n",
      "step 571 : loss 1.5310628414154053\n",
      "step 572 : loss 1.570675253868103\n",
      "step 573 : loss 1.6429768800735474\n",
      "step 574 : loss 1.7466239929199219\n",
      "step 575 : loss 1.7271616458892822\n",
      "step 576 : loss 1.676567792892456\n",
      "step 577 : loss 1.59610915184021\n",
      "step 578 : loss 1.5565332174301147\n",
      "step 579 : loss 1.5259687900543213\n",
      "step 580 : loss 1.508244276046753\n",
      "step 581 : loss 1.4986517429351807\n",
      "step 582 : loss 1.5074591636657715\n",
      "step 583 : loss 1.5412155389785767\n",
      "step 584 : loss 1.6280008554458618\n",
      "step 585 : loss 1.7118394374847412\n",
      "step 586 : loss 1.7459189891815186\n",
      "step 587 : loss 1.6202874183654785\n",
      "step 588 : loss 1.550533652305603\n",
      "step 589 : loss 1.5204391479492188\n",
      "step 590 : loss 1.5115065574645996\n",
      "step 591 : loss 1.5032539367675781\n",
      "step 592 : loss 1.4977521896362305\n",
      "step 593 : loss 1.4996099472045898\n",
      "step 594 : loss 1.5231815576553345\n",
      "step 595 : loss 1.5773239135742188\n",
      "step 596 : loss 1.6775544881820679\n",
      "step 597 : loss 1.6878615617752075\n",
      "step 598 : loss 1.6456212997436523\n",
      "step 599 : loss 1.5468107461929321\n",
      "step 600 : loss 1.49742591381073\n",
      "step 601 : loss 1.470018982887268\n",
      "step 602 : loss 1.4603099822998047\n",
      "step 603 : loss 1.4610470533370972\n",
      "step 604 : loss 1.484508752822876\n",
      "step 605 : loss 1.5353984832763672\n",
      "step 606 : loss 1.6299183368682861\n",
      "step 607 : loss 1.6568055152893066\n",
      "step 608 : loss 1.639755368232727\n",
      "step 609 : loss 1.551527500152588\n",
      "step 610 : loss 1.507979154586792\n",
      "step 611 : loss 1.4829392433166504\n",
      "step 612 : loss 1.4727783203125\n",
      "step 613 : loss 1.4661414623260498\n",
      "step 614 : loss 1.474584698677063\n",
      "step 615 : loss 1.5023747682571411\n",
      "step 616 : loss 1.5692694187164307\n",
      "step 617 : loss 1.6126888990402222\n",
      "step 618 : loss 1.624904990196228\n",
      "step 619 : loss 1.5392112731933594\n",
      "step 620 : loss 1.487363576889038\n",
      "step 621 : loss 1.4559905529022217\n",
      "step 622 : loss 1.450886845588684\n",
      "step 623 : loss 1.4544520378112793\n",
      "step 624 : loss 1.479225516319275\n",
      "step 625 : loss 1.5106580257415771\n",
      "step 626 : loss 1.560874104499817\n",
      "step 627 : loss 1.5568413734436035\n",
      "step 628 : loss 1.5445120334625244\n",
      "step 629 : loss 1.4979599714279175\n",
      "step 630 : loss 1.475873351097107\n",
      "step 631 : loss 1.4593956470489502\n",
      "step 632 : loss 1.4622266292572021\n",
      "step 633 : loss 1.4668524265289307\n",
      "step 634 : loss 1.4871083498001099\n",
      "step 635 : loss 1.4954676628112793\n",
      "step 636 : loss 1.5117247104644775\n",
      "step 637 : loss 1.4959797859191895\n",
      "step 638 : loss 1.4899213314056396\n",
      "step 639 : loss 1.4672695398330688\n",
      "step 640 : loss 1.4623961448669434\n",
      "step 641 : loss 1.451990008354187\n",
      "step 642 : loss 1.4579463005065918\n",
      "step 643 : loss 1.4562656879425049\n",
      "step 644 : loss 1.4681562185287476\n",
      "step 645 : loss 1.464463233947754\n",
      "step 646 : loss 1.471261739730835\n",
      "step 647 : loss 1.4592983722686768\n",
      "step 648 : loss 1.4589684009552002\n",
      "step 649 : loss 1.446445107460022\n",
      "step 650 : loss 1.4467774629592896\n",
      "step 651 : loss 1.4387528896331787\n",
      "step 652 : loss 1.4422650337219238\n",
      "step 653 : loss 1.4366481304168701\n",
      "step 654 : loss 1.4415513277053833\n",
      "step 655 : loss 1.4355978965759277\n",
      "step 656 : loss 1.440794825553894\n",
      "step 657 : loss 1.4339908361434937\n",
      "step 658 : loss 1.4393320083618164\n",
      "step 659 : loss 1.431570291519165\n",
      "step 660 : loss 1.4360243082046509\n",
      "step 661 : loss 1.4270761013031006\n",
      "step 662 : loss 1.4301844835281372\n",
      "step 663 : loss 1.4211887121200562\n",
      "step 664 : loss 1.4240976572036743\n",
      "step 665 : loss 1.416085958480835\n",
      "step 666 : loss 1.419159173965454\n",
      "step 667 : loss 1.4115300178527832\n",
      "step 668 : loss 1.4140818119049072\n",
      "step 669 : loss 1.4062532186508179\n",
      "step 670 : loss 1.4085623025894165\n",
      "step 671 : loss 1.4012707471847534\n",
      "step 672 : loss 1.404604434967041\n",
      "step 673 : loss 1.3985779285430908\n",
      "step 674 : loss 1.4038234949111938\n",
      "step 675 : loss 1.3990031480789185\n",
      "step 676 : loss 1.406519889831543\n",
      "step 677 : loss 1.4029242992401123\n",
      "step 678 : loss 1.4130017757415771\n",
      "step 679 : loss 1.410418152809143\n",
      "step 680 : loss 1.420767068862915\n",
      "step 681 : loss 1.4165011644363403\n",
      "step 682 : loss 1.4206281900405884\n",
      "step 683 : loss 1.412230134010315\n",
      "step 684 : loss 1.407721757888794\n",
      "step 685 : loss 1.3979219198226929\n",
      "step 686 : loss 1.3903933763504028\n",
      "step 687 : loss 1.383458137512207\n",
      "step 688 : loss 1.3778233528137207\n",
      "step 689 : loss 1.3743972778320312\n",
      "step 690 : loss 1.370527982711792\n",
      "step 691 : loss 1.368043065071106\n",
      "step 692 : loss 1.3640575408935547\n",
      "step 693 : loss 1.3610894680023193\n",
      "step 694 : loss 1.358025074005127\n",
      "step 695 : loss 1.3567109107971191\n",
      "step 696 : loss 1.358599066734314\n",
      "step 697 : loss 1.3629794120788574\n",
      "step 698 : loss 1.3766388893127441\n",
      "step 699 : loss 1.3915890455245972\n",
      "step 700 : loss 1.4231003522872925\n",
      "step 701 : loss 1.4304111003875732\n",
      "step 702 : loss 1.4431602954864502\n",
      "step 703 : loss 1.4070125818252563\n",
      "step 704 : loss 1.385886549949646\n",
      "step 705 : loss 1.356918454170227\n",
      "step 706 : loss 1.3481836318969727\n",
      "step 707 : loss 1.3422356843948364\n",
      "step 708 : loss 1.3558639287948608\n",
      "step 709 : loss 1.3706748485565186\n",
      "step 710 : loss 1.40421724319458\n",
      "step 711 : loss 1.4075913429260254\n",
      "step 712 : loss 1.4145220518112183\n",
      "step 713 : loss 1.3829267024993896\n",
      "step 714 : loss 1.367699384689331\n",
      "step 715 : loss 1.3465927839279175\n",
      "step 716 : loss 1.3404273986816406\n",
      "step 717 : loss 1.3330787420272827\n",
      "step 718 : loss 1.3381812572479248\n",
      "step 719 : loss 1.343291163444519\n",
      "step 720 : loss 1.3635032176971436\n",
      "step 721 : loss 1.3714085817337036\n",
      "step 722 : loss 1.3871634006500244\n",
      "step 723 : loss 1.3683574199676514\n",
      "step 724 : loss 1.358959674835205\n",
      "step 725 : loss 1.3341808319091797\n",
      "step 726 : loss 1.326164960861206\n",
      "step 727 : loss 1.315002202987671\n",
      "step 728 : loss 1.3185182809829712\n",
      "step 729 : loss 1.319228172302246\n",
      "step 730 : loss 1.3335002660751343\n",
      "step 731 : loss 1.3370522260665894\n",
      "step 732 : loss 1.3508310317993164\n",
      "step 733 : loss 1.3428318500518799\n",
      "step 734 : loss 1.3455042839050293\n",
      "step 735 : loss 1.3306158781051636\n",
      "step 736 : loss 1.328412652015686\n",
      "step 737 : loss 1.3161026239395142\n",
      "step 738 : loss 1.3141082525253296\n",
      "step 739 : loss 1.306362509727478\n",
      "step 740 : loss 1.3076668977737427\n",
      "step 741 : loss 1.3050777912139893\n",
      "step 742 : loss 1.3108794689178467\n",
      "step 743 : loss 1.3101248741149902\n",
      "step 744 : loss 1.3175320625305176\n",
      "step 745 : loss 1.313888430595398\n",
      "step 746 : loss 1.320000410079956\n",
      "step 747 : loss 1.3119441270828247\n",
      "step 748 : loss 1.3154687881469727\n",
      "step 749 : loss 1.3054512739181519\n",
      "step 750 : loss 1.3081247806549072\n",
      "step 751 : loss 1.299921989440918\n",
      "step 752 : loss 1.3036253452301025\n",
      "step 753 : loss 1.2967628240585327\n",
      "step 754 : loss 1.299562931060791\n",
      "step 755 : loss 1.2916826009750366\n",
      "step 756 : loss 1.293249487876892\n",
      "step 757 : loss 1.285954475402832\n",
      "step 758 : loss 1.2893716096878052\n",
      "step 759 : loss 1.2842222452163696\n",
      "step 760 : loss 1.2902946472167969\n",
      "step 761 : loss 1.2860108613967896\n",
      "step 762 : loss 1.2937883138656616\n",
      "step 763 : loss 1.2895911931991577\n",
      "step 764 : loss 1.2976090908050537\n",
      "step 765 : loss 1.2917572259902954\n",
      "step 766 : loss 1.2958141565322876\n",
      "step 767 : loss 1.2869136333465576\n",
      "step 768 : loss 1.2862718105316162\n",
      "step 769 : loss 1.2777832746505737\n",
      "step 770 : loss 1.2767726182937622\n",
      "step 771 : loss 1.2712366580963135\n",
      "step 772 : loss 1.2713706493377686\n",
      "step 773 : loss 1.2672817707061768\n",
      "step 774 : loss 1.2676112651824951\n",
      "step 775 : loss 1.2637032270431519\n",
      "step 776 : loss 1.2649919986724854\n",
      "step 777 : loss 1.262200117111206\n",
      "step 778 : loss 1.2665538787841797\n",
      "step 779 : loss 1.265239953994751\n",
      "step 780 : loss 1.2732925415039062\n",
      "step 781 : loss 1.2718969583511353\n",
      "step 782 : loss 1.28249990940094\n",
      "step 783 : loss 1.2784806489944458\n",
      "step 784 : loss 1.2875019311904907\n",
      "step 785 : loss 1.278181552886963\n",
      "step 786 : loss 1.2806228399276733\n",
      "step 787 : loss 1.267802119255066\n",
      "step 788 : loss 1.2658792734146118\n",
      "step 789 : loss 1.2551430463790894\n",
      "step 790 : loss 1.2544270753860474\n",
      "step 791 : loss 1.247873067855835\n",
      "step 792 : loss 1.2500816583633423\n",
      "step 793 : loss 1.2459591627120972\n",
      "step 794 : loss 1.2502491474151611\n",
      "step 795 : loss 1.2467575073242188\n",
      "step 796 : loss 1.2536342144012451\n",
      "step 797 : loss 1.2510383129119873\n",
      "step 798 : loss 1.2611662149429321\n",
      "step 799 : loss 1.2573106288909912\n",
      "step 800 : loss 1.2653950452804565\n",
      "step 801 : loss 1.255739688873291\n",
      "step 802 : loss 1.25663161277771\n",
      "step 803 : loss 1.2449743747711182\n",
      "step 804 : loss 1.2434442043304443\n",
      "step 805 : loss 1.2352941036224365\n",
      "step 806 : loss 1.2355968952178955\n",
      "step 807 : loss 1.2308100461959839\n",
      "step 808 : loss 1.233185887336731\n",
      "step 809 : loss 1.2298164367675781\n",
      "step 810 : loss 1.2342274188995361\n",
      "step 811 : loss 1.2312837839126587\n",
      "step 812 : loss 1.2379581928253174\n",
      "step 813 : loss 1.2341331243515015\n",
      "step 814 : loss 1.241377830505371\n",
      "step 815 : loss 1.2347291707992554\n",
      "step 816 : loss 1.2398735284805298\n",
      "step 817 : loss 1.2306973934173584\n",
      "step 818 : loss 1.2333545684814453\n",
      "step 819 : loss 1.2239903211593628\n",
      "step 820 : loss 1.225592851638794\n",
      "step 821 : loss 1.2173724174499512\n",
      "step 822 : loss 1.218953013420105\n",
      "step 823 : loss 1.2120158672332764\n",
      "step 824 : loss 1.2141802310943604\n",
      "step 825 : loss 1.2083182334899902\n",
      "step 826 : loss 1.2115274667739868\n",
      "step 827 : loss 1.2064152956008911\n",
      "step 828 : loss 1.2109659910202026\n",
      "step 829 : loss 1.206435203552246\n",
      "step 830 : loss 1.2127736806869507\n",
      "step 831 : loss 1.208838701248169\n",
      "step 832 : loss 1.2167011499404907\n",
      "step 833 : loss 1.2124214172363281\n",
      "step 834 : loss 1.2194080352783203\n",
      "step 835 : loss 1.2133909463882446\n",
      "step 836 : loss 1.2167456150054932\n",
      "step 837 : loss 1.2096223831176758\n",
      "step 838 : loss 1.2095016241073608\n",
      "step 839 : loss 1.2028110027313232\n",
      "step 840 : loss 1.2009351253509521\n",
      "step 841 : loss 1.1956058740615845\n",
      "step 842 : loss 1.1938376426696777\n",
      "step 843 : loss 1.1901881694793701\n",
      "step 844 : loss 1.1894747018814087\n",
      "step 845 : loss 1.1869317293167114\n",
      "step 846 : loss 1.1874815225601196\n",
      "step 847 : loss 1.1856063604354858\n",
      "step 848 : loss 1.1884658336639404\n",
      "step 849 : loss 1.187961459159851\n",
      "step 850 : loss 1.1953215599060059\n",
      "step 851 : loss 1.196368932723999\n",
      "step 852 : loss 1.20823335647583\n",
      "step 853 : loss 1.2061784267425537\n",
      "step 854 : loss 1.2152491807937622\n",
      "step 855 : loss 1.204409122467041\n",
      "step 856 : loss 1.205016851425171\n",
      "step 857 : loss 1.1912317276000977\n",
      "step 858 : loss 1.189467191696167\n",
      "step 859 : loss 1.1797916889190674\n",
      "step 860 : loss 1.1807904243469238\n",
      "step 861 : loss 1.1755049228668213\n",
      "step 862 : loss 1.1795786619186401\n",
      "step 863 : loss 1.1761854887008667\n",
      "step 864 : loss 1.1823025941848755\n",
      "step 865 : loss 1.178896427154541\n",
      "step 866 : loss 1.186390995979309\n",
      "step 867 : loss 1.181525707244873\n",
      "step 868 : loss 1.1882129907608032\n",
      "step 869 : loss 1.180506706237793\n",
      "step 870 : loss 1.183901309967041\n",
      "step 871 : loss 1.174903392791748\n",
      "step 872 : loss 1.1758345365524292\n",
      "step 873 : loss 1.1679840087890625\n",
      "step 874 : loss 1.1682924032211304\n",
      "step 875 : loss 1.1622140407562256\n",
      "step 876 : loss 1.1632338762283325\n",
      "step 877 : loss 1.1587563753128052\n",
      "step 878 : loss 1.1613088846206665\n",
      "step 879 : loss 1.1577028036117554\n",
      "step 880 : loss 1.1617920398712158\n",
      "step 881 : loss 1.1580866575241089\n",
      "step 882 : loss 1.1636604070663452\n",
      "step 883 : loss 1.1596369743347168\n",
      "step 884 : loss 1.1663810014724731\n",
      "step 885 : loss 1.1610630750656128\n",
      "step 886 : loss 1.1665831804275513\n",
      "step 887 : loss 1.1587938070297241\n",
      "step 888 : loss 1.1615418195724487\n",
      "step 889 : loss 1.1530436277389526\n",
      "step 890 : loss 1.154749870300293\n",
      "step 891 : loss 1.1476588249206543\n",
      "step 892 : loss 1.1495362520217896\n",
      "step 893 : loss 1.14350163936615\n",
      "step 894 : loss 1.1450873613357544\n",
      "step 895 : loss 1.1393468379974365\n",
      "step 896 : loss 1.1407665014266968\n",
      "step 897 : loss 1.1355204582214355\n",
      "step 898 : loss 1.1374891996383667\n",
      "step 899 : loss 1.1327906847000122\n",
      "step 900 : loss 1.1355799436569214\n",
      "step 901 : loss 1.1313163042068481\n",
      "step 902 : loss 1.1356251239776611\n",
      "step 903 : loss 1.1325303316116333\n",
      "step 904 : loss 1.1398470401763916\n",
      "step 905 : loss 1.1383012533187866\n",
      "step 906 : loss 1.1482914686203003\n",
      "step 907 : loss 1.1459014415740967\n",
      "step 908 : loss 1.153915524482727\n",
      "step 909 : loss 1.1478794813156128\n",
      "step 910 : loss 1.1497763395309448\n",
      "step 911 : loss 1.1416453123092651\n",
      "step 912 : loss 1.1389625072479248\n",
      "step 913 : loss 1.1316852569580078\n",
      "step 914 : loss 1.128378987312317\n",
      "step 915 : loss 1.1239672899246216\n",
      "step 916 : loss 1.1230101585388184\n",
      "step 917 : loss 1.1215004920959473\n",
      "step 918 : loss 1.123038649559021\n",
      "step 919 : loss 1.1224113702774048\n",
      "step 920 : loss 1.1258715391159058\n",
      "step 921 : loss 1.1254124641418457\n",
      "step 922 : loss 1.1323072910308838\n",
      "step 923 : loss 1.131706714630127\n",
      "step 924 : loss 1.1405147314071655\n",
      "step 925 : loss 1.1348835229873657\n",
      "step 926 : loss 1.1390798091888428\n",
      "step 927 : loss 1.1276684999465942\n",
      "step 928 : loss 1.1278085708618164\n",
      "step 929 : loss 1.1174598932266235\n",
      "step 930 : loss 1.1187636852264404\n",
      "step 931 : loss 1.1119611263275146\n",
      "step 932 : loss 1.115293025970459\n",
      "step 933 : loss 1.1101329326629639\n",
      "step 934 : loss 1.1143344640731812\n",
      "step 935 : loss 1.1093769073486328\n",
      "step 936 : loss 1.1143381595611572\n",
      "step 937 : loss 1.1094670295715332\n",
      "step 938 : loss 1.1150728464126587\n",
      "step 939 : loss 1.109730839729309\n",
      "step 940 : loss 1.1148818731307983\n",
      "step 941 : loss 1.108994722366333\n",
      "step 942 : loss 1.1130503416061401\n",
      "step 943 : loss 1.1073077917099\n",
      "step 944 : loss 1.1097394227981567\n",
      "step 945 : loss 1.1040072441101074\n",
      "step 946 : loss 1.104371190071106\n",
      "step 947 : loss 1.098893404006958\n",
      "step 948 : loss 1.0984179973602295\n",
      "step 949 : loss 1.0941952466964722\n",
      "step 950 : loss 1.094228982925415\n",
      "step 951 : loss 1.0910576581954956\n",
      "step 952 : loss 1.0914688110351562\n",
      "step 953 : loss 1.0885449647903442\n",
      "step 954 : loss 1.0895612239837646\n",
      "step 955 : loss 1.087124228477478\n",
      "step 956 : loss 1.0898029804229736\n",
      "step 957 : loss 1.0880630016326904\n",
      "step 958 : loss 1.0929663181304932\n",
      "step 959 : loss 1.0914500951766968\n",
      "step 960 : loss 1.0988438129425049\n",
      "step 961 : loss 1.096864938735962\n",
      "step 962 : loss 1.1057558059692383\n",
      "step 963 : loss 1.101304531097412\n",
      "step 964 : loss 1.1074228286743164\n",
      "step 965 : loss 1.0988811254501343\n",
      "step 966 : loss 1.0999749898910522\n",
      "step 967 : loss 1.0904531478881836\n",
      "step 968 : loss 1.0898200273513794\n",
      "step 969 : loss 1.0825077295303345\n",
      "step 970 : loss 1.0827746391296387\n",
      "step 971 : loss 1.0776766538619995\n",
      "step 972 : loss 1.079161286354065\n",
      "step 973 : loss 1.0751901865005493\n",
      "step 974 : loss 1.0779163837432861\n",
      "step 975 : loss 1.0745923519134521\n",
      "step 976 : loss 1.0792258977890015\n",
      "step 977 : loss 1.0766102075576782\n",
      "step 978 : loss 1.0835716724395752\n",
      "step 979 : loss 1.0806903839111328\n",
      "step 980 : loss 1.0880794525146484\n",
      "step 981 : loss 1.0824146270751953\n",
      "step 982 : loss 1.0864572525024414\n",
      "step 983 : loss 1.0779982805252075\n",
      "step 984 : loss 1.0787608623504639\n",
      "step 985 : loss 1.0710490942001343\n",
      "step 986 : loss 1.0713984966278076\n",
      "step 987 : loss 1.0658788681030273\n",
      "step 988 : loss 1.0668683052062988\n",
      "step 989 : loss 1.0627937316894531\n",
      "step 990 : loss 1.0645233392715454\n",
      "step 991 : loss 1.061131238937378\n",
      "step 992 : loss 1.0639232397079468\n",
      "step 993 : loss 1.0608363151550293\n",
      "step 994 : loss 1.0649502277374268\n",
      "step 995 : loss 1.061606526374817\n",
      "step 996 : loss 1.0667344331741333\n",
      "step 997 : loss 1.0625447034835815\n",
      "step 998 : loss 1.0679347515106201\n",
      "step 999 : loss 1.0625381469726562\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Autoencoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(init_tensor)\n",
    "    loss = (y-init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "result = autoencoder(init_tensor)\n",
    "tensor_to_image(result.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK+ZP3EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABrUlEQVR4Ae3TwQ0AMAyDQLf779yOweeyABLE582VBm4Jx94EiL9AAAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4CxAgNhDjLUCA2ECMtwABYgMx3gIEiA3EeAsQIDYQ4y1AgNhAjLcAAWIDMd4CBIgNxHgLECA2EOMtQIDYQIy3AAFiAzHeAgSIDcR4C4gDfM/hAf+qY6fJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_image(init_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "\n",
    "def generate_image(size): # , num_images):\n",
    "    # for i in range(num_images):\n",
    "    # Create a new image with a random background color\n",
    "    img = Image.new(\"RGB\", size, color=(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "\n",
    "    # Get a drawing context\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Choose a random shape (circle, square, or triangle)\n",
    "    # shape = random.choice([\"circle\", \"square\", \"triangle\"])\n",
    "    shape = \"rectangle\"\n",
    "\n",
    "    # Choose a random position\n",
    "    position = (random.randint(20, size[0]-20), random.randint(20, size[1]-20))\n",
    "\n",
    "    # Choose a random color for the shape\n",
    "    shape_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "    # Draw the shape on the image\n",
    "    if shape == \"circle\":\n",
    "        draw.ellipse([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "    elif shape == \"square\":\n",
    "        draw.rectangle([position[0]-20, position[1]-20, position[0]+20, position[1]+20], fill=shape_color)\n",
    "    elif shape == \"triangle\":\n",
    "        draw.polygon([(position[0], position[1]-20), (position[0]-20, position[1]+20), (position[0]+20, position[1]+20)], fill=shape_color)\n",
    "\n",
    "    # Show the image\n",
    "    # img.show()\n",
    "    # print(type(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDi6KKK4z6IKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAABsUlEQVR4Ae3TUQ0AIAzEUMDTBOKcoKI/bwaatLt9Z5brDJwOjfwNCBD/gQACxAZivAUIEBuI8RYgQGwgxluAALGBGG8BAsQGYrwFCBAbiPEWIEBsIMZbgACxgRhvAQLEBmK8BQgQG4jxFiBAbCDGW4AAsYEYbwECxAZivAUIEBuI8RYgQGwgxluAALGBGG8BAsQGYrwFCBAbiPEWIEBsIMZbgACxgRhvAQLEBmK8BQgQG4jxFiBAbCDGW4AAsYEYbwECxAZivAUIEBuI8RYgQGwgxluAALGBGG8BAsQGYrwFCBAbiPEWIEBsIMZbgACxgRhvAQLEBmK8BQgQG4jxFiBAbCDGW4AAsYEYbwECxAZivAUIEBuI8RYgQGwgxluAALGBGG8BAsQGYrwFCBAbiPEWIEBsIMZbgACxgRhvAQLEBmK8BQgQG4jxFiBAbCDGW4AAsYEYbwECxAZivAUIEBuI8RYgQGwgxluAALGBGG8BAsQGYrwFCBAbiPEWIEBsIMZbgACxgRhvAQLEBmK8BQgQG4jxFiBAbCDGW4AAsYEYbwECxAZivAUIEBuI8RYQB3jEdQHram6POgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size = (128, 128)\n",
    "num_images = 5  # Change this to the number of images you want to generate\n",
    "\n",
    "new_image = generate_image(image_size)\n",
    "new_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 75.06912231445312\n",
      "step 1 : loss 74.73161315917969\n",
      "step 2 : loss 74.38299560546875\n",
      "step 3 : loss 74.01148986816406\n",
      "step 4 : loss 73.60192108154297\n",
      "step 5 : loss 73.14067840576172\n",
      "step 6 : loss 72.60647583007812\n",
      "step 7 : loss 71.9726791381836\n",
      "step 8 : loss 71.20694732666016\n",
      "step 9 : loss 70.26868438720703\n",
      "step 10 : loss 69.10427856445312\n",
      "step 11 : loss 67.65171813964844\n",
      "step 12 : loss 65.8350830078125\n",
      "step 13 : loss 63.562599182128906\n",
      "step 14 : loss 60.7329216003418\n",
      "step 15 : loss 57.24049377441406\n",
      "step 16 : loss 53.002864837646484\n",
      "step 17 : loss 48.03232192993164\n",
      "step 18 : loss 42.597412109375\n",
      "step 19 : loss 37.608821868896484\n",
      "step 20 : loss 34.8899040222168\n",
      "step 21 : loss 35.33303451538086\n",
      "step 22 : loss 35.95085144042969\n",
      "step 23 : loss 34.187339782714844\n",
      "step 24 : loss 30.58880043029785\n",
      "step 25 : loss 26.71663475036621\n",
      "step 26 : loss 24.087448120117188\n",
      "step 27 : loss 23.340715408325195\n",
      "step 28 : loss 23.83203125\n",
      "step 29 : loss 24.56380271911621\n",
      "step 30 : loss 25.00974464416504\n",
      "step 31 : loss 25.09130859375\n",
      "step 32 : loss 24.94657325744629\n",
      "step 33 : loss 24.755563735961914\n",
      "step 34 : loss 24.601499557495117\n",
      "step 35 : loss 24.41219711303711\n",
      "step 36 : loss 24.04458999633789\n",
      "step 37 : loss 23.430530548095703\n",
      "step 38 : loss 22.63826560974121\n",
      "step 39 : loss 21.82573127746582\n",
      "step 40 : loss 21.145946502685547\n",
      "step 41 : loss 20.673450469970703\n",
      "step 42 : loss 20.391401290893555\n",
      "step 43 : loss 20.233089447021484\n",
      "step 44 : loss 20.131282806396484\n",
      "step 45 : loss 20.04387664794922\n",
      "step 46 : loss 19.952341079711914\n",
      "step 47 : loss 19.84662437438965\n",
      "step 48 : loss 19.714126586914062\n",
      "step 49 : loss 19.536970138549805\n",
      "step 50 : loss 19.29952049255371\n",
      "step 51 : loss 18.99696159362793\n",
      "step 52 : loss 18.640771865844727\n",
      "step 53 : loss 18.2551212310791\n",
      "step 54 : loss 17.869047164916992\n",
      "step 55 : loss 17.5062313079834\n",
      "step 56 : loss 17.179845809936523\n",
      "step 57 : loss 16.889911651611328\n",
      "step 58 : loss 16.626380920410156\n",
      "step 59 : loss 16.373428344726562\n",
      "step 60 : loss 16.112232208251953\n",
      "step 61 : loss 15.825016975402832\n",
      "step 62 : loss 15.496920585632324\n",
      "step 63 : loss 15.118167877197266\n",
      "step 64 : loss 14.687455177307129\n",
      "step 65 : loss 14.212129592895508\n",
      "step 66 : loss 13.707856178283691\n",
      "step 67 : loss 13.195323944091797\n",
      "step 68 : loss 12.694299697875977\n",
      "step 69 : loss 12.217691421508789\n",
      "step 70 : loss 11.766962051391602\n",
      "step 71 : loss 11.332601547241211\n",
      "step 72 : loss 10.900906562805176\n",
      "step 73 : loss 10.46410083770752\n",
      "step 74 : loss 10.030299186706543\n",
      "step 75 : loss 9.629316329956055\n",
      "step 76 : loss 9.303969383239746\n",
      "step 77 : loss 9.073159217834473\n",
      "step 78 : loss 8.892824172973633\n",
      "step 79 : loss 8.70383071899414\n",
      "step 80 : loss 8.530057907104492\n",
      "step 81 : loss 8.434952735900879\n",
      "step 82 : loss 8.3778076171875\n",
      "step 83 : loss 8.272355079650879\n",
      "step 84 : loss 8.135587692260742\n",
      "step 85 : loss 8.021337509155273\n",
      "step 86 : loss 7.917430400848389\n",
      "step 87 : loss 7.780491828918457\n",
      "step 88 : loss 7.609315872192383\n",
      "step 89 : loss 7.452097415924072\n",
      "step 90 : loss 7.310870170593262\n",
      "step 91 : loss 7.129373073577881\n",
      "step 92 : loss 6.94293737411499\n",
      "step 93 : loss 6.799178123474121\n",
      "step 94 : loss 6.639660358428955\n",
      "step 95 : loss 6.477027416229248\n",
      "step 96 : loss 6.358759880065918\n",
      "step 97 : loss 6.231093406677246\n",
      "step 98 : loss 6.104986190795898\n",
      "step 99 : loss 6.0120849609375\n",
      "step 100 : loss 5.907444953918457\n",
      "step 101 : loss 5.808477878570557\n",
      "step 102 : loss 5.720487117767334\n",
      "step 103 : loss 5.613104343414307\n",
      "step 104 : loss 5.516872406005859\n",
      "step 105 : loss 5.407701015472412\n",
      "step 106 : loss 5.292792320251465\n",
      "step 107 : loss 5.181789398193359\n",
      "step 108 : loss 5.059952259063721\n",
      "step 109 : loss 4.953781604766846\n",
      "step 110 : loss 4.846101760864258\n",
      "step 111 : loss 4.802396297454834\n",
      "step 112 : loss 4.868926048278809\n",
      "step 113 : loss 4.812868595123291\n",
      "step 114 : loss 4.502578258514404\n",
      "step 115 : loss 4.487176895141602\n",
      "step 116 : loss 4.541251182556152\n",
      "step 117 : loss 4.308822154998779\n",
      "step 118 : loss 4.227529525756836\n",
      "step 119 : loss 4.290992259979248\n",
      "step 120 : loss 4.106887340545654\n",
      "step 121 : loss 4.000716209411621\n",
      "step 122 : loss 4.04744815826416\n",
      "step 123 : loss 3.9348888397216797\n",
      "step 124 : loss 3.793008327484131\n",
      "step 125 : loss 3.8147764205932617\n",
      "step 126 : loss 3.7811620235443115\n",
      "step 127 : loss 3.634725570678711\n",
      "step 128 : loss 3.579962730407715\n",
      "step 129 : loss 3.594393491744995\n",
      "step 130 : loss 3.524456024169922\n",
      "step 131 : loss 3.4018709659576416\n",
      "step 132 : loss 3.3506529331207275\n",
      "step 133 : loss 3.348693370819092\n",
      "step 134 : loss 3.3178107738494873\n",
      "step 135 : loss 3.2256100177764893\n",
      "step 136 : loss 3.1355221271514893\n",
      "step 137 : loss 3.090019941329956\n",
      "step 138 : loss 3.0765902996063232\n",
      "step 139 : loss 3.073065996170044\n",
      "step 140 : loss 3.05161714553833\n",
      "step 141 : loss 2.9942545890808105\n",
      "step 142 : loss 2.913856267929077\n",
      "step 143 : loss 2.837538719177246\n",
      "step 144 : loss 2.775467872619629\n",
      "step 145 : loss 2.7281858921051025\n",
      "step 146 : loss 2.687772274017334\n",
      "step 147 : loss 2.6538822650909424\n",
      "step 148 : loss 2.639256715774536\n",
      "step 149 : loss 2.689246416091919\n",
      "step 150 : loss 2.899244785308838\n",
      "step 151 : loss 3.0469422340393066\n",
      "step 152 : loss 2.728524684906006\n",
      "step 153 : loss 2.4405293464660645\n",
      "step 154 : loss 2.4656710624694824\n",
      "step 155 : loss 2.6963958740234375\n",
      "step 156 : loss 2.6636545658111572\n",
      "step 157 : loss 2.379944324493408\n",
      "step 158 : loss 2.2883830070495605\n",
      "step 159 : loss 2.413987636566162\n",
      "step 160 : loss 2.5193700790405273\n",
      "step 161 : loss 2.3326704502105713\n",
      "step 162 : loss 2.1769559383392334\n",
      "step 163 : loss 2.1937711238861084\n",
      "step 164 : loss 2.3102779388427734\n",
      "step 165 : loss 2.3260271549224854\n",
      "step 166 : loss 2.16776967048645\n",
      "step 167 : loss 2.0582127571105957\n",
      "step 168 : loss 2.0514540672302246\n",
      "step 169 : loss 2.120229482650757\n",
      "step 170 : loss 2.197674036026001\n",
      "step 171 : loss 2.1498355865478516\n",
      "step 172 : loss 2.0299293994903564\n",
      "step 173 : loss 1.9419753551483154\n",
      "step 174 : loss 1.9067397117614746\n",
      "step 175 : loss 1.911070704460144\n",
      "step 176 : loss 1.9587304592132568\n",
      "step 177 : loss 2.053374767303467\n",
      "step 178 : loss 2.0998966693878174\n",
      "step 179 : loss 2.018958568572998\n",
      "step 180 : loss 1.8900460004806519\n",
      "step 181 : loss 1.8137152194976807\n",
      "step 182 : loss 1.7773759365081787\n",
      "step 183 : loss 1.7595773935317993\n",
      "step 184 : loss 1.7534735202789307\n",
      "step 185 : loss 1.7687718868255615\n",
      "step 186 : loss 1.8410919904708862\n",
      "step 187 : loss 1.99466872215271\n",
      "step 188 : loss 2.0708513259887695\n",
      "step 189 : loss 1.880419373512268\n",
      "step 190 : loss 1.7298457622528076\n",
      "step 191 : loss 1.6746394634246826\n",
      "step 192 : loss 1.668744444847107\n",
      "step 193 : loss 1.693804383277893\n",
      "step 194 : loss 1.7476646900177002\n",
      "step 195 : loss 1.8307901620864868\n",
      "step 196 : loss 1.8777326345443726\n",
      "step 197 : loss 1.8359824419021606\n",
      "step 198 : loss 1.7136921882629395\n",
      "step 199 : loss 1.614489197731018\n",
      "step 200 : loss 1.5629971027374268\n",
      "step 201 : loss 1.5518877506256104\n",
      "step 202 : loss 1.5780435800552368\n",
      "step 203 : loss 1.6516305208206177\n",
      "step 204 : loss 1.7644191980361938\n",
      "step 205 : loss 1.7988760471343994\n",
      "step 206 : loss 1.725327491760254\n",
      "step 207 : loss 1.62078058719635\n",
      "step 208 : loss 1.548500418663025\n",
      "step 209 : loss 1.498252511024475\n",
      "step 210 : loss 1.47133469581604\n",
      "step 211 : loss 1.4653370380401611\n",
      "step 212 : loss 1.4774751663208008\n",
      "step 213 : loss 1.5126078128814697\n",
      "step 214 : loss 1.5874688625335693\n",
      "step 215 : loss 1.7135335206985474\n",
      "step 216 : loss 1.7499892711639404\n",
      "step 217 : loss 1.6391949653625488\n",
      "step 218 : loss 1.5054936408996582\n",
      "step 219 : loss 1.4330912828445435\n",
      "step 220 : loss 1.401356816291809\n",
      "step 221 : loss 1.390485405921936\n",
      "step 222 : loss 1.3902461528778076\n",
      "step 223 : loss 1.39544677734375\n",
      "step 224 : loss 1.4031153917312622\n",
      "step 225 : loss 1.418226957321167\n",
      "step 226 : loss 1.465409755706787\n",
      "step 227 : loss 1.5979169607162476\n",
      "step 228 : loss 1.750730037689209\n",
      "step 229 : loss 1.6060047149658203\n",
      "step 230 : loss 1.4356191158294678\n",
      "step 231 : loss 1.3572120666503906\n",
      "step 232 : loss 1.327829360961914\n",
      "step 233 : loss 1.3154065608978271\n",
      "step 234 : loss 1.3098362684249878\n",
      "step 235 : loss 1.3090301752090454\n",
      "step 236 : loss 1.3158713579177856\n",
      "step 237 : loss 1.3444082736968994\n",
      "step 238 : loss 1.4345226287841797\n",
      "step 239 : loss 1.6211527585983276\n",
      "step 240 : loss 1.6531087160110474\n",
      "step 241 : loss 1.4965425729751587\n",
      "step 242 : loss 1.3778154850006104\n",
      "step 243 : loss 1.3244801759719849\n",
      "step 244 : loss 1.2926210165023804\n",
      "step 245 : loss 1.2685688734054565\n",
      "step 246 : loss 1.2537052631378174\n",
      "step 247 : loss 1.2503842115402222\n",
      "step 248 : loss 1.2719154357910156\n",
      "step 249 : loss 1.36738121509552\n",
      "step 250 : loss 1.5686993598937988\n",
      "step 251 : loss 1.5985277891159058\n",
      "step 252 : loss 1.374965786933899\n",
      "step 253 : loss 1.2739866971969604\n",
      "step 254 : loss 1.2538256645202637\n",
      "step 255 : loss 1.2536596059799194\n",
      "step 256 : loss 1.2520897388458252\n",
      "step 257 : loss 1.2417576313018799\n",
      "step 258 : loss 1.235500693321228\n",
      "step 259 : loss 1.2616175413131714\n",
      "step 260 : loss 1.3780909776687622\n",
      "step 261 : loss 1.5476516485214233\n",
      "step 262 : loss 1.431302547454834\n",
      "step 263 : loss 1.2699559926986694\n",
      "step 264 : loss 1.2031184434890747\n",
      "step 265 : loss 1.1872930526733398\n",
      "step 266 : loss 1.1979482173919678\n",
      "step 267 : loss 1.2515051364898682\n",
      "step 268 : loss 1.369775652885437\n",
      "step 269 : loss 1.4487948417663574\n",
      "step 270 : loss 1.3325461149215698\n",
      "step 271 : loss 1.2307780981063843\n",
      "step 272 : loss 1.1923675537109375\n",
      "step 273 : loss 1.188117504119873\n",
      "step 274 : loss 1.1998851299285889\n",
      "step 275 : loss 1.2322124242782593\n",
      "step 276 : loss 1.285416841506958\n",
      "step 277 : loss 1.3394436836242676\n",
      "step 278 : loss 1.3170790672302246\n",
      "step 279 : loss 1.2529089450836182\n",
      "step 280 : loss 1.1963471174240112\n",
      "step 281 : loss 1.1654462814331055\n",
      "step 282 : loss 1.1552578210830688\n",
      "step 283 : loss 1.1697003841400146\n",
      "step 284 : loss 1.2193117141723633\n",
      "step 285 : loss 1.2916046380996704\n",
      "step 286 : loss 1.2888197898864746\n",
      "step 287 : loss 1.2193927764892578\n",
      "step 288 : loss 1.1574739217758179\n",
      "step 289 : loss 1.1319500207901\n",
      "step 290 : loss 1.1323959827423096\n",
      "step 291 : loss 1.1609858274459839\n",
      "step 292 : loss 1.2156918048858643\n",
      "step 293 : loss 1.259634256362915\n",
      "step 294 : loss 1.2241822481155396\n",
      "step 295 : loss 1.1641943454742432\n",
      "step 296 : loss 1.1249958276748657\n",
      "step 297 : loss 1.11441171169281\n",
      "step 298 : loss 1.1265456676483154\n",
      "step 299 : loss 1.1635568141937256\n",
      "step 300 : loss 1.2041257619857788\n",
      "step 301 : loss 1.2096394300460815\n",
      "step 302 : loss 1.1642411947250366\n",
      "step 303 : loss 1.1227630376815796\n",
      "step 304 : loss 1.1019501686096191\n",
      "step 305 : loss 1.104124903678894\n",
      "step 306 : loss 1.1253336668014526\n",
      "step 307 : loss 1.1606305837631226\n",
      "step 308 : loss 1.1754180192947388\n",
      "step 309 : loss 1.156349539756775\n",
      "step 310 : loss 1.1178956031799316\n",
      "step 311 : loss 1.0936440229415894\n",
      "step 312 : loss 1.086889624595642\n",
      "step 313 : loss 1.0992426872253418\n",
      "step 314 : loss 1.1228493452072144\n",
      "step 315 : loss 1.1443918943405151\n",
      "step 316 : loss 1.1359754800796509\n",
      "step 317 : loss 1.1102731227874756\n",
      "step 318 : loss 1.0847314596176147\n",
      "step 319 : loss 1.0752207040786743\n",
      "step 320 : loss 1.0808912515640259\n",
      "step 321 : loss 1.1008087396621704\n",
      "step 322 : loss 1.1202497482299805\n",
      "step 323 : loss 1.121869683265686\n",
      "step 324 : loss 1.103285551071167\n",
      "step 325 : loss 1.0809972286224365\n",
      "step 326 : loss 1.0738468170166016\n",
      "step 327 : loss 1.0760332345962524\n",
      "step 328 : loss 1.0873174667358398\n",
      "step 329 : loss 1.0868470668792725\n",
      "step 330 : loss 1.0900883674621582\n",
      "step 331 : loss 1.101593017578125\n",
      "step 332 : loss 1.1224236488342285\n",
      "step 333 : loss 1.1098188161849976\n",
      "step 334 : loss 1.0682401657104492\n",
      "step 335 : loss 1.026033639907837\n",
      "step 336 : loss 1.0005465745925903\n",
      "step 337 : loss 0.9904561638832092\n",
      "step 338 : loss 0.9954367280006409\n",
      "step 339 : loss 1.02254319190979\n",
      "step 340 : loss 1.0797785520553589\n",
      "step 341 : loss 1.1259853839874268\n",
      "step 342 : loss 1.105785608291626\n",
      "step 343 : loss 1.0449113845825195\n",
      "step 344 : loss 1.010444164276123\n",
      "step 345 : loss 1.0016664266586304\n",
      "step 346 : loss 1.0114408731460571\n",
      "step 347 : loss 1.0275002717971802\n",
      "step 348 : loss 1.0443966388702393\n",
      "step 349 : loss 1.0493661165237427\n",
      "step 350 : loss 1.0541839599609375\n",
      "step 351 : loss 1.0565526485443115\n",
      "step 352 : loss 1.054932713508606\n",
      "step 353 : loss 1.0351041555404663\n",
      "step 354 : loss 1.0093282461166382\n",
      "step 355 : loss 0.990081787109375\n",
      "step 356 : loss 0.9869145154953003\n",
      "step 357 : loss 1.0028197765350342\n",
      "step 358 : loss 1.036502480506897\n",
      "step 359 : loss 1.0560258626937866\n",
      "step 360 : loss 1.0404036045074463\n",
      "step 361 : loss 1.0013974905014038\n",
      "step 362 : loss 0.9758238196372986\n",
      "step 363 : loss 0.967686653137207\n",
      "step 364 : loss 0.9782204031944275\n",
      "step 365 : loss 1.0012108087539673\n",
      "step 366 : loss 1.025748610496521\n",
      "step 367 : loss 1.0234466791152954\n",
      "step 368 : loss 1.002811312675476\n",
      "step 369 : loss 0.9804477095603943\n",
      "step 370 : loss 0.972722053527832\n",
      "step 371 : loss 0.976495623588562\n",
      "step 372 : loss 0.9868745803833008\n",
      "step 373 : loss 0.9914423823356628\n",
      "step 374 : loss 0.9866045713424683\n",
      "step 375 : loss 0.974928617477417\n",
      "step 376 : loss 0.9687724709510803\n",
      "step 377 : loss 0.9700155854225159\n",
      "step 378 : loss 0.9797651767730713\n",
      "step 379 : loss 0.985173225402832\n",
      "step 380 : loss 0.9826086163520813\n",
      "step 381 : loss 0.9679617881774902\n",
      "step 382 : loss 0.9553444981575012\n",
      "step 383 : loss 0.9471269845962524\n",
      "step 384 : loss 0.9484875202178955\n",
      "step 385 : loss 0.953636109828949\n",
      "step 386 : loss 0.9615253806114197\n",
      "step 387 : loss 0.9625890254974365\n",
      "step 388 : loss 0.96087247133255\n",
      "step 389 : loss 0.9561485648155212\n",
      "step 390 : loss 0.9570922255516052\n",
      "step 391 : loss 0.9596633315086365\n",
      "step 392 : loss 0.9623873233795166\n",
      "step 393 : loss 0.9576094150543213\n",
      "step 394 : loss 0.9469649195671082\n",
      "step 395 : loss 0.9356713891029358\n",
      "step 396 : loss 0.927184522151947\n",
      "step 397 : loss 0.9246279001235962\n",
      "step 398 : loss 0.925746500492096\n",
      "step 399 : loss 0.9299774169921875\n",
      "step 400 : loss 0.934209406375885\n",
      "step 401 : loss 0.9370939135551453\n",
      "step 402 : loss 0.9404769539833069\n",
      "step 403 : loss 0.941983699798584\n",
      "step 404 : loss 0.9443346858024597\n",
      "step 405 : loss 0.9401280283927917\n",
      "step 406 : loss 0.9335094094276428\n",
      "step 407 : loss 0.9212389588356018\n",
      "step 408 : loss 0.9119020104408264\n",
      "step 409 : loss 0.9035552740097046\n",
      "step 410 : loss 0.9021064639091492\n",
      "step 411 : loss 0.904525637626648\n",
      "step 412 : loss 0.9144765734672546\n",
      "step 413 : loss 0.9251790046691895\n",
      "step 414 : loss 0.9349222183227539\n",
      "step 415 : loss 0.9329150319099426\n",
      "step 416 : loss 0.924957275390625\n",
      "step 417 : loss 0.9128149747848511\n",
      "step 418 : loss 0.903307318687439\n",
      "step 419 : loss 0.8969435691833496\n",
      "step 420 : loss 0.8936210870742798\n",
      "step 421 : loss 0.8937708735466003\n",
      "step 422 : loss 0.8974454998970032\n",
      "step 423 : loss 0.9034453630447388\n",
      "step 424 : loss 0.9105260372161865\n",
      "step 425 : loss 0.9113929867744446\n",
      "step 426 : loss 0.9079498052597046\n",
      "step 427 : loss 0.8982175588607788\n",
      "step 428 : loss 0.8907358050346375\n",
      "step 429 : loss 0.8839006423950195\n",
      "step 430 : loss 0.8818627595901489\n",
      "step 431 : loss 0.8802720904350281\n",
      "step 432 : loss 0.8823343515396118\n",
      "step 433 : loss 0.8840467929840088\n",
      "step 434 : loss 0.8887253403663635\n",
      "step 435 : loss 0.891132652759552\n",
      "step 436 : loss 0.8929251432418823\n",
      "step 437 : loss 0.8889006972312927\n",
      "step 438 : loss 0.8831557631492615\n",
      "step 439 : loss 0.8758865594863892\n",
      "step 440 : loss 0.8706330060958862\n",
      "step 441 : loss 0.867685854434967\n",
      "step 442 : loss 0.8669623136520386\n",
      "step 443 : loss 0.8675046563148499\n",
      "step 444 : loss 0.868802547454834\n",
      "step 445 : loss 0.8694652318954468\n",
      "step 446 : loss 0.8709152936935425\n",
      "step 447 : loss 0.8707067370414734\n",
      "step 448 : loss 0.8713786005973816\n",
      "step 449 : loss 0.8689500689506531\n",
      "step 450 : loss 0.8669191598892212\n",
      "step 451 : loss 0.861801266670227\n",
      "step 452 : loss 0.8582895994186401\n",
      "step 453 : loss 0.8530518412590027\n",
      "step 454 : loss 0.8506312966346741\n",
      "step 455 : loss 0.847364604473114\n",
      "step 456 : loss 0.8475292921066284\n",
      "step 457 : loss 0.8476288318634033\n",
      "step 458 : loss 0.8514521718025208\n",
      "step 459 : loss 0.8549159169197083\n",
      "step 460 : loss 0.8601319789886475\n",
      "step 461 : loss 0.8613114356994629\n",
      "step 462 : loss 0.8605847358703613\n",
      "step 463 : loss 0.8554558753967285\n",
      "step 464 : loss 0.8494747281074524\n",
      "step 465 : loss 0.8437756299972534\n",
      "step 466 : loss 0.8383029699325562\n",
      "step 467 : loss 0.8349261283874512\n",
      "step 468 : loss 0.8317911028862\n",
      "step 469 : loss 0.8314167857170105\n",
      "step 470 : loss 0.8326276540756226\n",
      "step 471 : loss 0.8362783789634705\n",
      "step 472 : loss 0.8411613702774048\n",
      "step 473 : loss 0.8437195420265198\n",
      "step 474 : loss 0.844294011592865\n",
      "step 475 : loss 0.8396735191345215\n",
      "step 476 : loss 0.8351761102676392\n",
      "step 477 : loss 0.8289085030555725\n",
      "step 478 : loss 0.8251942992210388\n",
      "step 479 : loss 0.8202683925628662\n",
      "step 480 : loss 0.8180983066558838\n",
      "step 481 : loss 0.8157151341438293\n",
      "step 482 : loss 0.8176481127738953\n",
      "step 483 : loss 0.8208527565002441\n",
      "step 484 : loss 0.827481210231781\n",
      "step 485 : loss 0.8311798572540283\n",
      "step 486 : loss 0.8323421478271484\n",
      "step 487 : loss 0.8274056315422058\n",
      "step 488 : loss 0.8214583396911621\n",
      "step 489 : loss 0.8156105875968933\n",
      "step 490 : loss 0.8114784955978394\n",
      "step 491 : loss 0.8090793490409851\n",
      "step 492 : loss 0.807220995426178\n",
      "step 493 : loss 0.8067026138305664\n",
      "step 494 : loss 0.8072471618652344\n",
      "step 495 : loss 0.8090596795082092\n",
      "step 496 : loss 0.8120831251144409\n",
      "step 497 : loss 0.8132325410842896\n",
      "step 498 : loss 0.8133699297904968\n",
      "step 499 : loss 0.8096431493759155\n",
      "step 500 : loss 0.8064112067222595\n",
      "step 501 : loss 0.8018189072608948\n",
      "step 502 : loss 0.7995700836181641\n",
      "step 503 : loss 0.7960317134857178\n",
      "step 504 : loss 0.7944508194923401\n",
      "step 505 : loss 0.7916843295097351\n",
      "step 506 : loss 0.7919028401374817\n",
      "step 507 : loss 0.7924972176551819\n",
      "step 508 : loss 0.7963922023773193\n",
      "step 509 : loss 0.7992581129074097\n",
      "step 510 : loss 0.8021323084831238\n",
      "step 511 : loss 0.8008857369422913\n",
      "step 512 : loss 0.7985007762908936\n",
      "step 513 : loss 0.7944681644439697\n",
      "step 514 : loss 0.7908735871315002\n",
      "step 515 : loss 0.7877943515777588\n",
      "step 516 : loss 0.7843131422996521\n",
      "step 517 : loss 0.7817954421043396\n",
      "step 518 : loss 0.7791876196861267\n",
      "step 519 : loss 0.7786245346069336\n",
      "step 520 : loss 0.7791427969932556\n",
      "step 521 : loss 0.7810150384902954\n",
      "step 522 : loss 0.7832733988761902\n",
      "step 523 : loss 0.7840757966041565\n",
      "step 524 : loss 0.7845479249954224\n",
      "step 525 : loss 0.7827345132827759\n",
      "step 526 : loss 0.7816644906997681\n",
      "step 527 : loss 0.7786046266555786\n",
      "step 528 : loss 0.7764124274253845\n",
      "step 529 : loss 0.771731972694397\n",
      "step 530 : loss 0.7686236500740051\n",
      "step 531 : loss 0.7643322944641113\n",
      "step 532 : loss 0.763329803943634\n",
      "step 533 : loss 0.762570858001709\n",
      "step 534 : loss 0.7653517723083496\n",
      "step 535 : loss 0.767937421798706\n",
      "step 536 : loss 0.7725046873092651\n",
      "step 537 : loss 0.7750255465507507\n",
      "step 538 : loss 0.7770975828170776\n",
      "step 539 : loss 0.7754098773002625\n",
      "step 540 : loss 0.771914541721344\n",
      "step 541 : loss 0.7667222619056702\n",
      "step 542 : loss 0.7610824704170227\n",
      "step 543 : loss 0.7572731971740723\n",
      "step 544 : loss 0.7541207075119019\n",
      "step 545 : loss 0.7537530660629272\n",
      "step 546 : loss 0.7539982795715332\n",
      "step 547 : loss 0.7559959888458252\n",
      "step 548 : loss 0.7581713795661926\n",
      "step 549 : loss 0.7598074674606323\n",
      "step 550 : loss 0.7609059810638428\n",
      "step 551 : loss 0.7593533396720886\n",
      "step 552 : loss 0.7573925852775574\n",
      "step 553 : loss 0.7532275319099426\n",
      "step 554 : loss 0.7501850128173828\n",
      "step 555 : loss 0.7460649013519287\n",
      "step 556 : loss 0.7440612316131592\n",
      "step 557 : loss 0.7415841221809387\n",
      "step 558 : loss 0.7417926788330078\n",
      "step 559 : loss 0.7420019507408142\n",
      "step 560 : loss 0.7447139620780945\n",
      "step 561 : loss 0.746696412563324\n",
      "step 562 : loss 0.7493528723716736\n",
      "step 563 : loss 0.749409556388855\n",
      "step 564 : loss 0.7485753893852234\n",
      "step 565 : loss 0.745644211769104\n",
      "step 566 : loss 0.7424366474151611\n",
      "step 567 : loss 0.739345371723175\n",
      "step 568 : loss 0.7362784743309021\n",
      "step 569 : loss 0.7343348264694214\n",
      "step 570 : loss 0.7323551774024963\n",
      "step 571 : loss 0.7318219542503357\n",
      "step 572 : loss 0.7315561175346375\n",
      "step 573 : loss 0.7322478294372559\n",
      "step 574 : loss 0.7331663966178894\n",
      "step 575 : loss 0.7336406111717224\n",
      "step 576 : loss 0.7341699004173279\n",
      "step 577 : loss 0.7333400845527649\n",
      "step 578 : loss 0.7329174280166626\n",
      "step 579 : loss 0.7309255003929138\n",
      "step 580 : loss 0.7295997738838196\n",
      "step 581 : loss 0.7264349460601807\n",
      "step 582 : loss 0.7243534922599792\n",
      "step 583 : loss 0.7207504510879517\n",
      "step 584 : loss 0.7192363739013672\n",
      "step 585 : loss 0.7169936895370483\n",
      "step 586 : loss 0.7173539400100708\n",
      "step 587 : loss 0.7173565626144409\n",
      "step 588 : loss 0.7197686433792114\n",
      "step 589 : loss 0.7218598127365112\n",
      "step 590 : loss 0.7255191206932068\n",
      "step 591 : loss 0.7276102304458618\n",
      "step 592 : loss 0.7288039326667786\n",
      "step 593 : loss 0.7269275188446045\n",
      "step 594 : loss 0.7232035994529724\n",
      "step 595 : loss 0.7190814018249512\n",
      "step 596 : loss 0.7143386006355286\n",
      "step 597 : loss 0.7116655111312866\n",
      "step 598 : loss 0.7085261344909668\n",
      "step 599 : loss 0.7078977227210999\n",
      "step 600 : loss 0.7070562243461609\n",
      "step 601 : loss 0.7085044384002686\n",
      "step 602 : loss 0.7101007103919983\n",
      "step 603 : loss 0.7122520804405212\n",
      "step 604 : loss 0.7136951088905334\n",
      "step 605 : loss 0.713085412979126\n",
      "step 606 : loss 0.7116274833679199\n",
      "step 607 : loss 0.7083501815795898\n",
      "step 608 : loss 0.7057791948318481\n",
      "step 609 : loss 0.7022069096565247\n",
      "step 610 : loss 0.7000617980957031\n",
      "step 611 : loss 0.6972064971923828\n",
      "step 612 : loss 0.696578860282898\n",
      "step 613 : loss 0.6961065530776978\n",
      "step 614 : loss 0.6981582641601562\n",
      "step 615 : loss 0.7000238299369812\n",
      "step 616 : loss 0.7027989029884338\n",
      "step 617 : loss 0.7037407755851746\n",
      "step 618 : loss 0.703984797000885\n",
      "step 619 : loss 0.7022966146469116\n",
      "step 620 : loss 0.6999245882034302\n",
      "step 621 : loss 0.697136640548706\n",
      "step 622 : loss 0.6939821243286133\n",
      "step 623 : loss 0.6918460130691528\n",
      "step 624 : loss 0.6896355152130127\n",
      "step 625 : loss 0.6890344619750977\n",
      "step 626 : loss 0.6883830428123474\n",
      "step 627 : loss 0.6888276934623718\n",
      "step 628 : loss 0.6891030073165894\n",
      "step 629 : loss 0.6895378828048706\n",
      "step 630 : loss 0.6899187564849854\n",
      "step 631 : loss 0.689641535282135\n",
      "step 632 : loss 0.6894546151161194\n",
      "step 633 : loss 0.688117504119873\n",
      "step 634 : loss 0.687076985836029\n",
      "step 635 : loss 0.684737503528595\n",
      "step 636 : loss 0.6831681728363037\n",
      "step 637 : loss 0.6803856492042542\n",
      "step 638 : loss 0.6790015697479248\n",
      "step 639 : loss 0.6767175197601318\n",
      "step 640 : loss 0.6763594150543213\n",
      "step 641 : loss 0.6755165457725525\n",
      "step 642 : loss 0.6766778230667114\n",
      "step 643 : loss 0.67760169506073\n",
      "step 644 : loss 0.6801089644432068\n",
      "step 645 : loss 0.6820600628852844\n",
      "step 646 : loss 0.6842086315155029\n",
      "step 647 : loss 0.6845531463623047\n",
      "step 648 : loss 0.6834729909896851\n",
      "step 649 : loss 0.681057333946228\n",
      "step 650 : loss 0.6773452758789062\n",
      "step 651 : loss 0.6746034026145935\n",
      "step 652 : loss 0.6708783507347107\n",
      "step 653 : loss 0.6692723035812378\n",
      "step 654 : loss 0.6667158007621765\n",
      "step 655 : loss 0.6665515899658203\n",
      "step 656 : loss 0.6658547520637512\n",
      "step 657 : loss 0.6671742796897888\n",
      "step 658 : loss 0.6681532263755798\n",
      "step 659 : loss 0.669609010219574\n",
      "step 660 : loss 0.67039954662323\n",
      "step 661 : loss 0.6701160669326782\n",
      "step 662 : loss 0.6692854166030884\n",
      "step 663 : loss 0.6671134233474731\n",
      "step 664 : loss 0.6650973558425903\n",
      "step 665 : loss 0.6619630455970764\n",
      "step 666 : loss 0.6598379015922546\n",
      "step 667 : loss 0.6570675373077393\n",
      "step 668 : loss 0.6562469005584717\n",
      "step 669 : loss 0.6554006338119507\n",
      "step 670 : loss 0.6566992998123169\n",
      "step 671 : loss 0.6579129099845886\n",
      "step 672 : loss 0.6604062914848328\n",
      "step 673 : loss 0.6621226668357849\n",
      "step 674 : loss 0.6636267900466919\n",
      "step 675 : loss 0.6634450554847717\n",
      "step 676 : loss 0.6619356274604797\n",
      "step 677 : loss 0.6594266295433044\n",
      "step 678 : loss 0.656091570854187\n",
      "step 679 : loss 0.6537854671478271\n",
      "step 680 : loss 0.6512080430984497\n",
      "step 681 : loss 0.6504010558128357\n",
      "step 682 : loss 0.6491222977638245\n",
      "step 683 : loss 0.6493462324142456\n",
      "step 684 : loss 0.6491562128067017\n",
      "step 685 : loss 0.64988112449646\n",
      "step 686 : loss 0.6502765417098999\n",
      "step 687 : loss 0.6505241990089417\n",
      "step 688 : loss 0.6503482460975647\n",
      "step 689 : loss 0.6494195461273193\n",
      "step 690 : loss 0.6484773755073547\n",
      "step 691 : loss 0.6467208862304688\n",
      "step 692 : loss 0.645334005355835\n",
      "step 693 : loss 0.6429939866065979\n",
      "step 694 : loss 0.6415501832962036\n",
      "step 695 : loss 0.6394506692886353\n",
      "step 696 : loss 0.638919472694397\n",
      "step 697 : loss 0.6380704045295715\n",
      "step 698 : loss 0.6387934684753418\n",
      "step 699 : loss 0.6392651200294495\n",
      "step 700 : loss 0.6409332156181335\n",
      "step 701 : loss 0.6423429846763611\n",
      "step 702 : loss 0.6440950036048889\n",
      "step 703 : loss 0.6448848843574524\n",
      "step 704 : loss 0.644573450088501\n",
      "step 705 : loss 0.643186092376709\n",
      "step 706 : loss 0.6405055522918701\n",
      "step 707 : loss 0.6384215354919434\n",
      "step 708 : loss 0.6352875232696533\n",
      "step 709 : loss 0.6338348388671875\n",
      "step 710 : loss 0.631175696849823\n",
      "step 711 : loss 0.630547285079956\n",
      "step 712 : loss 0.6289564371109009\n",
      "step 713 : loss 0.6293983459472656\n",
      "step 714 : loss 0.6291696429252625\n",
      "step 715 : loss 0.630179762840271\n",
      "step 716 : loss 0.6306073665618896\n",
      "step 717 : loss 0.631266713142395\n",
      "step 718 : loss 0.6314812302589417\n",
      "step 719 : loss 0.631145179271698\n",
      "step 720 : loss 0.6304383873939514\n",
      "step 721 : loss 0.6287189722061157\n",
      "step 722 : loss 0.627014696598053\n",
      "step 723 : loss 0.62429279088974\n",
      "step 724 : loss 0.6225422024726868\n",
      "step 725 : loss 0.6201870441436768\n",
      "step 726 : loss 0.6195753216743469\n",
      "step 727 : loss 0.6186795234680176\n",
      "step 728 : loss 0.6195989847183228\n",
      "step 729 : loss 0.6204783916473389\n",
      "step 730 : loss 0.6227635145187378\n",
      "step 731 : loss 0.6249243021011353\n",
      "step 732 : loss 0.6270990967750549\n",
      "step 733 : loss 0.6279502511024475\n",
      "step 734 : loss 0.6269761919975281\n",
      "step 735 : loss 0.6248416900634766\n",
      "step 736 : loss 0.6213194727897644\n",
      "step 737 : loss 0.6189403533935547\n",
      "step 738 : loss 0.6158730983734131\n",
      "step 739 : loss 0.6148821115493774\n",
      "step 740 : loss 0.6130892634391785\n",
      "step 741 : loss 0.613332211971283\n",
      "step 742 : loss 0.6129472851753235\n",
      "step 743 : loss 0.6140952110290527\n",
      "step 744 : loss 0.6145393252372742\n",
      "step 745 : loss 0.615210235118866\n",
      "step 746 : loss 0.6149548888206482\n",
      "step 747 : loss 0.6142488121986389\n",
      "step 748 : loss 0.613143801689148\n",
      "step 749 : loss 0.6115500330924988\n",
      "step 750 : loss 0.609994649887085\n",
      "step 751 : loss 0.6078392863273621\n",
      "step 752 : loss 0.6063612103462219\n",
      "step 753 : loss 0.6046693921089172\n",
      "step 754 : loss 0.6043239235877991\n",
      "step 755 : loss 0.60397869348526\n",
      "step 756 : loss 0.6048010587692261\n",
      "step 757 : loss 0.6055057644844055\n",
      "step 758 : loss 0.6068316102027893\n",
      "step 759 : loss 0.6079896688461304\n",
      "step 760 : loss 0.6088694334030151\n",
      "step 761 : loss 0.6090812087059021\n",
      "step 762 : loss 0.6080510020256042\n",
      "step 763 : loss 0.6067618727684021\n",
      "step 764 : loss 0.6043835282325745\n",
      "step 765 : loss 0.6029727458953857\n",
      "step 766 : loss 0.6005357503890991\n",
      "step 767 : loss 0.5995787382125854\n",
      "step 768 : loss 0.5975180864334106\n",
      "step 769 : loss 0.5971927046775818\n",
      "step 770 : loss 0.5959975123405457\n",
      "step 771 : loss 0.5963512063026428\n",
      "step 772 : loss 0.5958800315856934\n",
      "step 773 : loss 0.5963429808616638\n",
      "step 774 : loss 0.5961843132972717\n",
      "step 775 : loss 0.5965532064437866\n",
      "step 776 : loss 0.5966126322746277\n",
      "step 777 : loss 0.5966912508010864\n",
      "step 778 : loss 0.5963705778121948\n",
      "step 779 : loss 0.5954059362411499\n",
      "step 780 : loss 0.5942819714546204\n",
      "step 781 : loss 0.5922388434410095\n",
      "step 782 : loss 0.5907455682754517\n",
      "step 783 : loss 0.5884050130844116\n",
      "step 784 : loss 0.5874068737030029\n",
      "step 785 : loss 0.5858486890792847\n",
      "step 786 : loss 0.5859286785125732\n",
      "step 787 : loss 0.5856953859329224\n",
      "step 788 : loss 0.5870217084884644\n",
      "step 789 : loss 0.5885878801345825\n",
      "step 790 : loss 0.5912753939628601\n",
      "step 791 : loss 0.5940486192703247\n",
      "step 792 : loss 0.5958654284477234\n",
      "step 793 : loss 0.5962768793106079\n",
      "step 794 : loss 0.5940134525299072\n",
      "step 795 : loss 0.5914846062660217\n",
      "step 796 : loss 0.587261974811554\n",
      "step 797 : loss 0.5850688815116882\n",
      "step 798 : loss 0.5817245244979858\n",
      "step 799 : loss 0.5811045169830322\n",
      "step 800 : loss 0.5796096324920654\n",
      "step 801 : loss 0.5807734131813049\n",
      "step 802 : loss 0.5810945630073547\n",
      "step 803 : loss 0.582883894443512\n",
      "step 804 : loss 0.5832916498184204\n",
      "step 805 : loss 0.5837064385414124\n",
      "step 806 : loss 0.5827345848083496\n",
      "step 807 : loss 0.5814124345779419\n",
      "step 808 : loss 0.5794650316238403\n",
      "step 809 : loss 0.5774279236793518\n",
      "step 810 : loss 0.5756909251213074\n",
      "step 811 : loss 0.5740743279457092\n",
      "step 812 : loss 0.5733122229576111\n",
      "step 813 : loss 0.572790801525116\n",
      "step 814 : loss 0.5732253193855286\n",
      "step 815 : loss 0.5739173889160156\n",
      "step 816 : loss 0.5750765204429626\n",
      "step 817 : loss 0.5762265920639038\n",
      "step 818 : loss 0.5769235491752625\n",
      "step 819 : loss 0.5772106051445007\n",
      "step 820 : loss 0.5761632323265076\n",
      "step 821 : loss 0.5750812292098999\n",
      "step 822 : loss 0.5728360414505005\n",
      "step 823 : loss 0.5717279314994812\n",
      "step 824 : loss 0.569728672504425\n",
      "step 825 : loss 0.5692775249481201\n",
      "step 826 : loss 0.567758321762085\n",
      "step 827 : loss 0.567732572555542\n",
      "step 828 : loss 0.5667466521263123\n",
      "step 829 : loss 0.5670506954193115\n",
      "step 830 : loss 0.5665158033370972\n",
      "step 831 : loss 0.5667598247528076\n",
      "step 832 : loss 0.5663592219352722\n",
      "step 833 : loss 0.5664737224578857\n",
      "step 834 : loss 0.5662272572517395\n",
      "step 835 : loss 0.5661411881446838\n",
      "step 836 : loss 0.5656578540802002\n",
      "step 837 : loss 0.5648513436317444\n",
      "step 838 : loss 0.5638865232467651\n",
      "step 839 : loss 0.5624435544013977\n",
      "step 840 : loss 0.5614249110221863\n",
      "step 841 : loss 0.5598036050796509\n",
      "step 842 : loss 0.5590181946754456\n",
      "step 843 : loss 0.5576919317245483\n",
      "step 844 : loss 0.557510256767273\n",
      "step 845 : loss 0.5571333765983582\n",
      "step 846 : loss 0.5579065084457397\n",
      "step 847 : loss 0.5589519143104553\n",
      "step 848 : loss 0.5607901215553284\n",
      "step 849 : loss 0.563096284866333\n",
      "step 850 : loss 0.5649536848068237\n",
      "step 851 : loss 0.5664228200912476\n",
      "step 852 : loss 0.5655585527420044\n",
      "step 853 : loss 0.5643309354782104\n",
      "step 854 : loss 0.5606549382209778\n",
      "step 855 : loss 0.5585756301879883\n",
      "step 856 : loss 0.55467689037323\n",
      "step 857 : loss 0.5535041689872742\n",
      "step 858 : loss 0.5508169531822205\n",
      "step 859 : loss 0.551074206829071\n",
      "step 860 : loss 0.5500211119651794\n",
      "step 861 : loss 0.5515022873878479\n",
      "step 862 : loss 0.5517062544822693\n",
      "step 863 : loss 0.5534026622772217\n",
      "step 864 : loss 0.5536337494850159\n",
      "step 865 : loss 0.5541262626647949\n",
      "step 866 : loss 0.5531122088432312\n",
      "step 867 : loss 0.5518789291381836\n",
      "step 868 : loss 0.5498703718185425\n",
      "step 869 : loss 0.5479243993759155\n",
      "step 870 : loss 0.5462337732315063\n",
      "step 871 : loss 0.5448185801506042\n",
      "step 872 : loss 0.5442230105400085\n",
      "step 873 : loss 0.5439903736114502\n",
      "step 874 : loss 0.5445709824562073\n",
      "step 875 : loss 0.5455055832862854\n",
      "step 876 : loss 0.5467115640640259\n",
      "step 877 : loss 0.5480227470397949\n",
      "step 878 : loss 0.5486624240875244\n",
      "step 879 : loss 0.549067497253418\n",
      "step 880 : loss 0.5479663610458374\n",
      "step 881 : loss 0.547113835811615\n",
      "step 882 : loss 0.5448296666145325\n",
      "step 883 : loss 0.5438538193702698\n",
      "step 884 : loss 0.541700541973114\n",
      "step 885 : loss 0.5413382649421692\n",
      "step 886 : loss 0.5398122668266296\n",
      "step 887 : loss 0.5399858951568604\n",
      "step 888 : loss 0.5390446186065674\n",
      "step 889 : loss 0.5395842790603638\n",
      "step 890 : loss 0.5391148924827576\n",
      "step 891 : loss 0.5397213101387024\n",
      "step 892 : loss 0.539394736289978\n",
      "step 893 : loss 0.5397120118141174\n",
      "step 894 : loss 0.5392258167266846\n",
      "step 895 : loss 0.538853645324707\n",
      "step 896 : loss 0.5378667116165161\n",
      "step 897 : loss 0.536656379699707\n",
      "step 898 : loss 0.5353508591651917\n",
      "step 899 : loss 0.5338944792747498\n",
      "step 900 : loss 0.5329625010490417\n",
      "step 901 : loss 0.5320658087730408\n",
      "step 902 : loss 0.5319576263427734\n",
      "step 903 : loss 0.5320094227790833\n",
      "step 904 : loss 0.5327603220939636\n",
      "step 905 : loss 0.5337823033332825\n",
      "step 906 : loss 0.534977376461029\n",
      "step 907 : loss 0.5364102721214294\n",
      "step 908 : loss 0.5368925929069519\n",
      "step 909 : loss 0.5373534560203552\n",
      "step 910 : loss 0.5360406637191772\n",
      "step 911 : loss 0.5353806018829346\n",
      "step 912 : loss 0.5333285927772522\n",
      "step 913 : loss 0.5330514311790466\n",
      "step 914 : loss 0.5313563346862793\n",
      "step 915 : loss 0.5316064357757568\n",
      "step 916 : loss 0.5296831727027893\n",
      "step 917 : loss 0.5294809341430664\n",
      "step 918 : loss 0.5271039009094238\n",
      "step 919 : loss 0.5264669060707092\n",
      "step 920 : loss 0.524282693862915\n",
      "step 921 : loss 0.5235487222671509\n",
      "step 922 : loss 0.5219878554344177\n",
      "step 923 : loss 0.5218698978424072\n",
      "step 924 : loss 0.5218327641487122\n",
      "step 925 : loss 0.5232345461845398\n",
      "step 926 : loss 0.5246360898017883\n",
      "step 927 : loss 0.5265706181526184\n",
      "step 928 : loss 0.5271648168563843\n",
      "step 929 : loss 0.5271735787391663\n",
      "step 930 : loss 0.5254443287849426\n",
      "step 931 : loss 0.5232524871826172\n",
      "step 932 : loss 0.5208137631416321\n",
      "step 933 : loss 0.5185672044754028\n",
      "step 934 : loss 0.5176101326942444\n",
      "step 935 : loss 0.5171100497245789\n",
      "step 936 : loss 0.5179540514945984\n",
      "step 937 : loss 0.5184088349342346\n",
      "step 938 : loss 0.5191002488136292\n",
      "step 939 : loss 0.5191752314567566\n",
      "step 940 : loss 0.5193484425544739\n",
      "step 941 : loss 0.5197385549545288\n",
      "step 942 : loss 0.5201611518859863\n",
      "step 943 : loss 0.5210138559341431\n",
      "step 944 : loss 0.5214717388153076\n",
      "step 945 : loss 0.5225265026092529\n",
      "step 946 : loss 0.5219321250915527\n",
      "step 947 : loss 0.5216648578643799\n",
      "step 948 : loss 0.5186302661895752\n",
      "step 949 : loss 0.5169248580932617\n",
      "step 950 : loss 0.5137274265289307\n",
      "step 951 : loss 0.5129373073577881\n",
      "step 952 : loss 0.5112651586532593\n",
      "step 953 : loss 0.5118098855018616\n",
      "step 954 : loss 0.5114160180091858\n",
      "step 955 : loss 0.5130152702331543\n",
      "step 956 : loss 0.5133141875267029\n",
      "step 957 : loss 0.5145629048347473\n",
      "step 958 : loss 0.5138579607009888\n",
      "step 959 : loss 0.5134463310241699\n",
      "step 960 : loss 0.5120761394500732\n",
      "step 961 : loss 0.5113173127174377\n",
      "step 962 : loss 0.5103548765182495\n",
      "step 963 : loss 0.5096973180770874\n",
      "step 964 : loss 0.5089817047119141\n",
      "step 965 : loss 0.508287787437439\n",
      "step 966 : loss 0.5076804161071777\n",
      "step 967 : loss 0.506876528263092\n",
      "step 968 : loss 0.5063119530677795\n",
      "step 969 : loss 0.5058807730674744\n",
      "step 970 : loss 0.5059767961502075\n",
      "step 971 : loss 0.5067660212516785\n",
      "step 972 : loss 0.5076442360877991\n",
      "step 973 : loss 0.5092430114746094\n",
      "step 974 : loss 0.5098726153373718\n",
      "step 975 : loss 0.5109431743621826\n",
      "step 976 : loss 0.510039746761322\n",
      "step 977 : loss 0.5094844102859497\n",
      "step 978 : loss 0.5069131851196289\n",
      "step 979 : loss 0.5057432651519775\n",
      "step 980 : loss 0.5032270550727844\n",
      "step 981 : loss 0.5028233528137207\n",
      "step 982 : loss 0.5009732246398926\n",
      "step 983 : loss 0.501160204410553\n",
      "step 984 : loss 0.4998251795768738\n",
      "step 985 : loss 0.5004494190216064\n",
      "step 986 : loss 0.4996023178100586\n",
      "step 987 : loss 0.5003248453140259\n",
      "step 988 : loss 0.49961933493614197\n",
      "step 989 : loss 0.5002554059028625\n",
      "step 990 : loss 0.49983400106430054\n",
      "step 991 : loss 0.5005194544792175\n",
      "step 992 : loss 0.5002012848854065\n",
      "step 993 : loss 0.5003208518028259\n",
      "step 994 : loss 0.4995131492614746\n",
      "step 995 : loss 0.49855664372444153\n",
      "step 996 : loss 0.4972260296344757\n",
      "step 997 : loss 0.4956507682800293\n",
      "step 998 : loss 0.4944615364074707\n",
      "step 999 : loss 0.493253231048584\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDigAMcUUGiuQ+iCiiikAUUUUAFFFFABRRRQAUUUUAFFFFMAoIDdRRRmgAooooAKKKKQBRRRQAUUUUAFFFFABRRRQAUUUUwCiiigAooopAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRTAKKKKQBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRTAKKKKQBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFMAooopAFFFFABRRRQAUUUUAFFFFABRRRTAKKKKACigMrD5TRQAUUUUAFFFFIAooooAKKKKACiiigAooooAKMGijPNMD/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_init_tensor = image_to_tensor(new_image)\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    y = autoencoder(new_init_tensor)\n",
    "    loss = (y-new_init_tensor).norm()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "result = autoencoder(new_init_tensor)\n",
    "tensor_to_image(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_train = 20\n",
    "num_images_test = 5\n",
    "dataset_train = [None] * num_images_train\n",
    "dataset_test = [None] * num_images_test\n",
    "\n",
    "for i in range(num_images_train):\n",
    "    dataset_train[i] = image_to_tensor(generate_image(image_size))\n",
    "\n",
    "for i in range(num_images_test):\n",
    "    dataset_test[i] = image_to_tensor(generate_image(image_size))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 116.97880554199219\n",
      "step 1 : loss 116.64715576171875\n",
      "step 2 : loss 116.29903411865234\n",
      "step 3 : loss 115.91722106933594\n",
      "step 4 : loss 115.4833984375\n",
      "step 5 : loss 114.97599029541016\n",
      "step 6 : loss 114.3666000366211\n",
      "step 7 : loss 113.6183090209961\n",
      "step 8 : loss 112.6842269897461\n",
      "step 9 : loss 111.5044174194336\n",
      "step 10 : loss 110.00482177734375\n",
      "step 11 : loss 108.0951156616211\n",
      "step 12 : loss 105.66751861572266\n",
      "step 13 : loss 102.5966796875\n",
      "step 14 : loss 98.74269104003906\n",
      "step 15 : loss 93.96373748779297\n",
      "step 16 : loss 88.15357971191406\n",
      "step 17 : loss 81.33827209472656\n",
      "step 18 : loss 73.92469787597656\n",
      "step 19 : loss 67.32197570800781\n",
      "step 20 : loss 64.80978393554688\n",
      "step 21 : loss 68.824462890625\n",
      "step 22 : loss 72.8460464477539\n",
      "step 23 : loss 72.23407745361328\n",
      "step 24 : loss 68.79522705078125\n",
      "step 25 : loss 65.153564453125\n",
      "step 26 : loss 62.92864227294922\n",
      "step 27 : loss 62.42051315307617\n",
      "step 28 : loss 63.03845977783203\n",
      "step 29 : loss 64.01029205322266\n",
      "step 30 : loss 64.80449676513672\n",
      "step 31 : loss 65.18022155761719\n",
      "step 32 : loss 65.09380340576172\n",
      "step 33 : loss 64.61366271972656\n",
      "step 34 : loss 63.87248992919922\n",
      "step 35 : loss 63.039215087890625\n",
      "step 36 : loss 62.29072189331055\n",
      "step 37 : loss 61.77143478393555\n",
      "step 38 : loss 61.545814514160156\n",
      "step 39 : loss 61.56987380981445\n",
      "step 40 : loss 61.71001052856445\n",
      "step 41 : loss 61.80983352661133\n",
      "step 42 : loss 61.765968322753906\n",
      "step 43 : loss 61.5658073425293\n",
      "step 44 : loss 61.27167892456055\n",
      "step 45 : loss 60.97315216064453\n",
      "step 46 : loss 60.74090576171875\n",
      "step 47 : loss 60.6031608581543\n",
      "step 48 : loss 60.5470085144043\n",
      "step 49 : loss 60.53440475463867\n",
      "step 50 : loss 60.52174758911133\n",
      "step 51 : loss 60.47435760498047\n",
      "step 52 : loss 60.37451934814453\n",
      "step 53 : loss 60.22290802001953\n",
      "step 54 : loss 60.03568649291992\n",
      "step 55 : loss 59.838661193847656\n",
      "step 56 : loss 59.6591796875\n",
      "step 57 : loss 59.51755905151367\n",
      "step 58 : loss 59.41986083984375\n",
      "step 59 : loss 59.35493850708008\n",
      "step 60 : loss 59.29848098754883\n",
      "step 61 : loss 59.2232780456543\n",
      "step 62 : loss 59.11170196533203\n",
      "step 63 : loss 58.962982177734375\n",
      "step 64 : loss 58.79180908203125\n",
      "step 65 : loss 58.61922073364258\n",
      "step 66 : loss 58.4618034362793\n",
      "step 67 : loss 58.32523727416992\n",
      "step 68 : loss 58.20349884033203\n",
      "step 69 : loss 58.083778381347656\n",
      "step 70 : loss 57.95299530029297\n",
      "step 71 : loss 57.80364990234375\n",
      "step 72 : loss 57.6366081237793\n",
      "step 73 : loss 57.46014404296875\n",
      "step 74 : loss 57.28602981567383\n",
      "step 75 : loss 57.123321533203125\n",
      "step 76 : loss 56.97331619262695\n",
      "step 77 : loss 56.828643798828125\n",
      "step 78 : loss 56.67830276489258\n",
      "step 79 : loss 56.51637649536133\n",
      "step 80 : loss 56.3472785949707\n",
      "step 81 : loss 56.18274688720703\n",
      "step 82 : loss 56.032325744628906\n",
      "step 83 : loss 55.89643478393555\n",
      "step 84 : loss 55.76723098754883\n",
      "step 85 : loss 55.636009216308594\n",
      "step 86 : loss 55.5012092590332\n",
      "step 87 : loss 55.36836624145508\n",
      "step 88 : loss 55.24312210083008\n",
      "step 89 : loss 55.122772216796875\n",
      "step 90 : loss 54.99762725830078\n",
      "step 91 : loss 54.86181640625\n",
      "step 92 : loss 54.71966552734375\n",
      "step 93 : loss 54.57880783081055\n",
      "step 94 : loss 54.43879318237305\n",
      "step 95 : loss 54.29180145263672\n",
      "step 96 : loss 54.13340377807617\n",
      "step 97 : loss 53.96870803833008\n",
      "step 98 : loss 53.8044319152832\n",
      "step 99 : loss 53.63835906982422\n",
      "step 100 : loss 53.463623046875\n",
      "step 101 : loss 53.28065872192383\n",
      "step 102 : loss 53.094520568847656\n",
      "step 103 : loss 52.90241622924805\n",
      "step 104 : loss 52.696922302246094\n",
      "step 105 : loss 52.48063278198242\n",
      "step 106 : loss 52.25938034057617\n",
      "step 107 : loss 52.028221130371094\n",
      "step 108 : loss 51.785614013671875\n",
      "step 109 : loss 51.53955078125\n",
      "step 110 : loss 51.28602981567383\n",
      "step 111 : loss 51.022735595703125\n",
      "step 112 : loss 50.75886535644531\n",
      "step 113 : loss 50.48728561401367\n",
      "step 114 : loss 50.21177673339844\n",
      "step 115 : loss 49.92797088623047\n",
      "step 116 : loss 49.63246536254883\n",
      "step 117 : loss 49.32428741455078\n",
      "step 118 : loss 49.000160217285156\n",
      "step 119 : loss 48.65283966064453\n",
      "step 120 : loss 48.286216735839844\n",
      "step 121 : loss 47.8953857421875\n",
      "step 122 : loss 47.483551025390625\n",
      "step 123 : loss 47.05796432495117\n",
      "step 124 : loss 46.625328063964844\n",
      "step 125 : loss 46.20160675048828\n",
      "step 126 : loss 45.855377197265625\n",
      "step 127 : loss 45.82472229003906\n",
      "step 128 : loss 45.570404052734375\n",
      "step 129 : loss 44.718116760253906\n",
      "step 130 : loss 44.731971740722656\n",
      "step 131 : loss 44.451690673828125\n",
      "step 132 : loss 43.78691864013672\n",
      "step 133 : loss 43.879371643066406\n",
      "step 134 : loss 43.39427947998047\n",
      "step 135 : loss 42.993873596191406\n",
      "step 136 : loss 42.95143127441406\n",
      "step 137 : loss 42.504554748535156\n",
      "step 138 : loss 42.17298126220703\n",
      "step 139 : loss 42.053749084472656\n",
      "step 140 : loss 41.708553314208984\n",
      "step 141 : loss 41.32097625732422\n",
      "step 142 : loss 41.22605895996094\n",
      "step 143 : loss 40.889198303222656\n",
      "step 144 : loss 40.51199722290039\n",
      "step 145 : loss 40.27241516113281\n",
      "step 146 : loss 40.06219482421875\n",
      "step 147 : loss 39.65876007080078\n",
      "step 148 : loss 39.2821159362793\n",
      "step 149 : loss 38.94938278198242\n",
      "step 150 : loss 38.68548583984375\n",
      "step 151 : loss 38.401512145996094\n",
      "step 152 : loss 38.08538055419922\n",
      "step 153 : loss 37.68468475341797\n",
      "step 154 : loss 37.252479553222656\n",
      "step 155 : loss 36.86870574951172\n",
      "step 156 : loss 36.505043029785156\n",
      "step 157 : loss 36.194053649902344\n",
      "step 158 : loss 35.91541290283203\n",
      "step 159 : loss 35.826133728027344\n",
      "step 160 : loss 36.364498138427734\n",
      "step 161 : loss 37.26513671875\n",
      "step 162 : loss 35.261993408203125\n",
      "step 163 : loss 34.989540100097656\n",
      "step 164 : loss 35.946937561035156\n",
      "step 165 : loss 34.40169143676758\n",
      "step 166 : loss 34.74213790893555\n",
      "step 167 : loss 34.80573654174805\n",
      "step 168 : loss 33.77617645263672\n",
      "step 169 : loss 34.53834915161133\n",
      "step 170 : loss 33.637386322021484\n",
      "step 171 : loss 33.653099060058594\n",
      "step 172 : loss 33.65057373046875\n",
      "step 173 : loss 33.025543212890625\n",
      "step 174 : loss 33.33038330078125\n",
      "step 175 : loss 32.77790069580078\n",
      "step 176 : loss 32.809791564941406\n",
      "step 177 : loss 32.69577407836914\n",
      "step 178 : loss 32.33383560180664\n",
      "step 179 : loss 32.52302551269531\n",
      "step 180 : loss 32.064762115478516\n",
      "step 181 : loss 32.14141082763672\n",
      "step 182 : loss 31.946269989013672\n",
      "step 183 : loss 31.75351333618164\n",
      "step 184 : loss 31.796600341796875\n",
      "step 185 : loss 31.513797760009766\n",
      "step 186 : loss 31.523128509521484\n",
      "step 187 : loss 31.395782470703125\n",
      "step 188 : loss 31.221996307373047\n",
      "step 189 : loss 31.23980712890625\n",
      "step 190 : loss 31.033111572265625\n",
      "step 191 : loss 30.978946685791016\n",
      "step 192 : loss 30.907550811767578\n",
      "step 193 : loss 30.737131118774414\n",
      "step 194 : loss 30.71720314025879\n",
      "step 195 : loss 30.578266143798828\n",
      "step 196 : loss 30.47857666015625\n",
      "step 197 : loss 30.422494888305664\n",
      "step 198 : loss 30.282581329345703\n",
      "step 199 : loss 30.211822509765625\n",
      "step 200 : loss 30.125381469726562\n",
      "step 201 : loss 29.9992618560791\n",
      "step 202 : loss 29.936756134033203\n",
      "step 203 : loss 29.828210830688477\n",
      "step 204 : loss 29.721294403076172\n",
      "step 205 : loss 29.65060043334961\n",
      "step 206 : loss 29.534008026123047\n",
      "step 207 : loss 29.440526962280273\n",
      "step 208 : loss 29.355676651000977\n",
      "step 209 : loss 29.24338722229004\n",
      "step 210 : loss 29.149734497070312\n",
      "step 211 : loss 29.05813980102539\n",
      "step 212 : loss 28.94869613647461\n",
      "step 213 : loss 28.849590301513672\n",
      "step 214 : loss 28.7556095123291\n",
      "step 215 : loss 28.645248413085938\n",
      "step 216 : loss 28.539758682250977\n",
      "step 217 : loss 28.441959381103516\n",
      "step 218 : loss 28.331064224243164\n",
      "step 219 : loss 28.218124389648438\n",
      "step 220 : loss 28.112728118896484\n",
      "step 221 : loss 28.003000259399414\n",
      "step 222 : loss 27.885522842407227\n",
      "step 223 : loss 27.7687931060791\n",
      "step 224 : loss 27.655420303344727\n",
      "step 225 : loss 27.540502548217773\n",
      "step 226 : loss 27.420917510986328\n",
      "step 227 : loss 27.297924041748047\n",
      "step 228 : loss 27.1765079498291\n",
      "step 229 : loss 27.0562801361084\n",
      "step 230 : loss 26.937368392944336\n",
      "step 231 : loss 26.821481704711914\n",
      "step 232 : loss 26.709564208984375\n",
      "step 233 : loss 26.611902236938477\n",
      "step 234 : loss 26.554943084716797\n",
      "step 235 : loss 26.591487884521484\n",
      "step 236 : loss 26.813207626342773\n",
      "step 237 : loss 27.05167579650879\n",
      "step 238 : loss 26.76384925842285\n",
      "step 239 : loss 26.12765121459961\n",
      "step 240 : loss 25.888097763061523\n",
      "step 241 : loss 26.136959075927734\n",
      "step 242 : loss 26.26357650756836\n",
      "step 243 : loss 25.873077392578125\n",
      "step 244 : loss 25.54183006286621\n",
      "step 245 : loss 25.633670806884766\n",
      "step 246 : loss 25.775671005249023\n",
      "step 247 : loss 25.572643280029297\n",
      "step 248 : loss 25.268291473388672\n",
      "step 249 : loss 25.244699478149414\n",
      "step 250 : loss 25.36688804626465\n",
      "step 251 : loss 25.28590965270996\n",
      "step 252 : loss 25.04564666748047\n",
      "step 253 : loss 24.92514991760254\n",
      "step 254 : loss 24.974231719970703\n",
      "step 255 : loss 24.997920989990234\n",
      "step 256 : loss 24.867691040039062\n",
      "step 257 : loss 24.698993682861328\n",
      "step 258 : loss 24.628467559814453\n",
      "step 259 : loss 24.644556045532227\n",
      "step 260 : loss 24.646970748901367\n",
      "step 261 : loss 24.5658016204834\n",
      "step 262 : loss 24.438474655151367\n",
      "step 263 : loss 24.338088989257812\n",
      "step 264 : loss 24.295400619506836\n",
      "step 265 : loss 24.290376663208008\n",
      "step 266 : loss 24.283588409423828\n",
      "step 267 : loss 24.24909019470215\n",
      "step 268 : loss 24.17947769165039\n",
      "step 269 : loss 24.092220306396484\n",
      "step 270 : loss 24.006155014038086\n",
      "step 271 : loss 23.93465232849121\n",
      "step 272 : loss 23.87868309020996\n",
      "step 273 : loss 23.835430145263672\n",
      "step 274 : loss 23.80268669128418\n",
      "step 275 : loss 23.78278160095215\n",
      "step 276 : loss 23.78559684753418\n",
      "step 277 : loss 23.830154418945312\n",
      "step 278 : loss 23.942432403564453\n",
      "step 279 : loss 24.105823516845703\n",
      "step 280 : loss 24.191865921020508\n",
      "step 281 : loss 24.003978729248047\n",
      "step 282 : loss 23.658836364746094\n",
      "step 283 : loss 23.419124603271484\n",
      "step 284 : loss 23.394258499145508\n",
      "step 285 : loss 23.516828536987305\n",
      "step 286 : loss 23.634166717529297\n",
      "step 287 : loss 23.591196060180664\n",
      "step 288 : loss 23.395456314086914\n",
      "step 289 : loss 23.223928451538086\n",
      "step 290 : loss 23.168277740478516\n",
      "step 291 : loss 23.202470779418945\n",
      "step 292 : loss 23.239212036132812\n",
      "step 293 : loss 23.221683502197266\n",
      "step 294 : loss 23.15290069580078\n",
      "step 295 : loss 23.07392120361328\n",
      "step 296 : loss 23.020368576049805\n",
      "step 297 : loss 22.969125747680664\n",
      "step 298 : loss 22.913562774658203\n",
      "step 299 : loss 22.854618072509766\n",
      "step 300 : loss 22.826183319091797\n",
      "step 301 : loss 22.838109970092773\n",
      "step 302 : loss 22.866743087768555\n",
      "step 303 : loss 22.88473129272461\n",
      "step 304 : loss 22.84615135192871\n",
      "step 305 : loss 22.789058685302734\n",
      "step 306 : loss 22.737918853759766\n",
      "step 307 : loss 22.719249725341797\n",
      "step 308 : loss 22.711978912353516\n",
      "step 309 : loss 22.67403793334961\n",
      "step 310 : loss 22.60736083984375\n",
      "step 311 : loss 22.532323837280273\n",
      "step 312 : loss 22.485368728637695\n",
      "step 313 : loss 22.471538543701172\n",
      "step 314 : loss 22.46605110168457\n",
      "step 315 : loss 22.45536994934082\n",
      "step 316 : loss 22.42287826538086\n",
      "step 317 : loss 22.39908790588379\n",
      "step 318 : loss 22.399049758911133\n",
      "step 319 : loss 22.438365936279297\n",
      "step 320 : loss 22.49636459350586\n",
      "step 321 : loss 22.5194034576416\n",
      "step 322 : loss 22.469419479370117\n",
      "step 323 : loss 22.349140167236328\n",
      "step 324 : loss 22.221574783325195\n",
      "step 325 : loss 22.134862899780273\n",
      "step 326 : loss 22.083126068115234\n",
      "step 327 : loss 22.04941177368164\n",
      "step 328 : loss 22.01056671142578\n",
      "step 329 : loss 21.97299575805664\n",
      "step 330 : loss 21.947486877441406\n",
      "step 331 : loss 21.94991111755371\n",
      "step 332 : loss 21.989032745361328\n",
      "step 333 : loss 22.059001922607422\n",
      "step 334 : loss 22.14754295349121\n",
      "step 335 : loss 22.179460525512695\n",
      "step 336 : loss 22.14018440246582\n",
      "step 337 : loss 22.02326202392578\n",
      "step 338 : loss 21.907176971435547\n",
      "step 339 : loss 21.827571868896484\n",
      "step 340 : loss 21.767959594726562\n",
      "step 341 : loss 21.718496322631836\n",
      "step 342 : loss 21.66583824157715\n",
      "step 343 : loss 21.6296443939209\n",
      "step 344 : loss 21.6231689453125\n",
      "step 345 : loss 21.655630111694336\n",
      "step 346 : loss 21.725154876708984\n",
      "step 347 : loss 21.801488876342773\n",
      "step 348 : loss 21.85087013244629\n",
      "step 349 : loss 21.807422637939453\n",
      "step 350 : loss 21.700397491455078\n",
      "step 351 : loss 21.58144760131836\n",
      "step 352 : loss 21.49664878845215\n",
      "step 353 : loss 21.450889587402344\n",
      "step 354 : loss 21.4173583984375\n",
      "step 355 : loss 21.3863582611084\n",
      "step 356 : loss 21.350603103637695\n",
      "step 357 : loss 21.327533721923828\n",
      "step 358 : loss 21.329137802124023\n",
      "step 359 : loss 21.365375518798828\n",
      "step 360 : loss 21.43335723876953\n",
      "step 361 : loss 21.498382568359375\n",
      "step 362 : loss 21.522979736328125\n",
      "step 363 : loss 21.456436157226562\n",
      "step 364 : loss 21.348159790039062\n",
      "step 365 : loss 21.24928855895996\n",
      "step 366 : loss 21.187969207763672\n",
      "step 367 : loss 21.16220474243164\n",
      "step 368 : loss 21.14109230041504\n",
      "step 369 : loss 21.115083694458008\n",
      "step 370 : loss 21.067026138305664\n",
      "step 371 : loss 21.014360427856445\n",
      "step 372 : loss 20.965551376342773\n",
      "step 373 : loss 20.931486129760742\n",
      "step 374 : loss 20.912614822387695\n",
      "step 375 : loss 20.905498504638672\n",
      "step 376 : loss 20.907756805419922\n",
      "step 377 : loss 20.918405532836914\n",
      "step 378 : loss 20.95104217529297\n",
      "step 379 : loss 21.017677307128906\n",
      "step 380 : loss 21.175012588500977\n",
      "step 381 : loss 21.389522552490234\n",
      "step 382 : loss 21.55649757385254\n",
      "step 383 : loss 21.410263061523438\n",
      "step 384 : loss 21.06266212463379\n",
      "step 385 : loss 20.803749084472656\n",
      "step 386 : loss 20.71290397644043\n",
      "step 387 : loss 20.755725860595703\n",
      "step 388 : loss 20.87467384338379\n",
      "step 389 : loss 20.99582290649414\n",
      "step 390 : loss 20.975696563720703\n",
      "step 391 : loss 20.808574676513672\n",
      "step 392 : loss 20.63296127319336\n",
      "step 393 : loss 20.53768539428711\n",
      "step 394 : loss 20.53486442565918\n",
      "step 395 : loss 20.59372329711914\n",
      "step 396 : loss 20.669414520263672\n",
      "step 397 : loss 20.686931610107422\n",
      "step 398 : loss 20.611522674560547\n",
      "step 399 : loss 20.489002227783203\n",
      "step 400 : loss 20.38580322265625\n",
      "step 401 : loss 20.328590393066406\n",
      "step 402 : loss 20.315418243408203\n",
      "step 403 : loss 20.33603858947754\n",
      "step 404 : loss 20.378936767578125\n",
      "step 405 : loss 20.42812156677246\n",
      "step 406 : loss 20.450162887573242\n",
      "step 407 : loss 20.43375015258789\n",
      "step 408 : loss 20.372102737426758\n",
      "step 409 : loss 20.308578491210938\n",
      "step 410 : loss 20.25475311279297\n",
      "step 411 : loss 20.233394622802734\n",
      "step 412 : loss 20.225038528442383\n",
      "step 413 : loss 20.233871459960938\n",
      "step 414 : loss 20.211421966552734\n",
      "step 415 : loss 20.16695785522461\n",
      "step 416 : loss 20.078710556030273\n",
      "step 417 : loss 19.994741439819336\n",
      "step 418 : loss 19.93225860595703\n",
      "step 419 : loss 19.90384292602539\n",
      "step 420 : loss 19.905139923095703\n",
      "step 421 : loss 19.9257755279541\n",
      "step 422 : loss 19.958805084228516\n",
      "step 423 : loss 19.990970611572266\n",
      "step 424 : loss 20.03280258178711\n",
      "step 425 : loss 20.091068267822266\n",
      "step 426 : loss 20.183448791503906\n",
      "step 427 : loss 20.29524040222168\n",
      "step 428 : loss 20.26806640625\n",
      "step 429 : loss 20.11646842956543\n",
      "step 430 : loss 19.83602523803711\n",
      "step 431 : loss 19.63551902770996\n",
      "step 432 : loss 19.55228614807129\n",
      "step 433 : loss 19.571359634399414\n",
      "step 434 : loss 19.650484085083008\n",
      "step 435 : loss 19.72191047668457\n",
      "step 436 : loss 19.766189575195312\n",
      "step 437 : loss 19.74240493774414\n",
      "step 438 : loss 19.704471588134766\n",
      "step 439 : loss 19.642475128173828\n",
      "step 440 : loss 19.54077911376953\n",
      "step 441 : loss 19.416654586791992\n",
      "step 442 : loss 19.303829193115234\n",
      "step 443 : loss 19.23403549194336\n",
      "step 444 : loss 19.208663940429688\n",
      "step 445 : loss 19.214462280273438\n",
      "step 446 : loss 19.2364444732666\n",
      "step 447 : loss 19.261615753173828\n",
      "step 448 : loss 19.29301643371582\n",
      "step 449 : loss 19.33592987060547\n",
      "step 450 : loss 19.38979148864746\n",
      "step 451 : loss 19.428401947021484\n",
      "step 452 : loss 19.371191024780273\n",
      "step 453 : loss 19.239200592041016\n",
      "step 454 : loss 19.053890228271484\n",
      "step 455 : loss 18.90697479248047\n",
      "step 456 : loss 18.812875747680664\n",
      "step 457 : loss 18.763774871826172\n",
      "step 458 : loss 18.740880966186523\n",
      "step 459 : loss 18.729061126708984\n",
      "step 460 : loss 18.72119140625\n",
      "step 461 : loss 18.70956802368164\n",
      "step 462 : loss 18.691421508789062\n",
      "step 463 : loss 18.664220809936523\n",
      "step 464 : loss 18.646419525146484\n",
      "step 465 : loss 18.6752872467041\n",
      "step 466 : loss 18.792985916137695\n",
      "step 467 : loss 18.960786819458008\n",
      "step 468 : loss 18.983806610107422\n",
      "step 469 : loss 18.77425765991211\n",
      "step 470 : loss 18.53986358642578\n",
      "step 471 : loss 18.39530372619629\n",
      "step 472 : loss 18.345552444458008\n",
      "step 473 : loss 18.282146453857422\n",
      "step 474 : loss 18.208797454833984\n",
      "step 475 : loss 18.1260986328125\n",
      "step 476 : loss 18.103473663330078\n",
      "step 477 : loss 18.1644344329834\n",
      "step 478 : loss 18.27401351928711\n",
      "step 479 : loss 18.325946807861328\n",
      "step 480 : loss 18.185321807861328\n",
      "step 481 : loss 17.967411041259766\n",
      "step 482 : loss 17.786666870117188\n",
      "step 483 : loss 17.72153663635254\n",
      "step 484 : loss 17.764137268066406\n",
      "step 485 : loss 17.886898040771484\n",
      "step 486 : loss 17.905054092407227\n",
      "step 487 : loss 17.855300903320312\n",
      "step 488 : loss 17.741008758544922\n",
      "step 489 : loss 17.71504783630371\n",
      "step 490 : loss 17.544010162353516\n",
      "step 491 : loss 17.29729652404785\n",
      "step 492 : loss 17.114459991455078\n",
      "step 493 : loss 17.115074157714844\n",
      "step 494 : loss 17.285818099975586\n",
      "step 495 : loss 17.427494049072266\n",
      "step 496 : loss 17.488767623901367\n",
      "step 497 : loss 17.274288177490234\n",
      "step 498 : loss 17.114585876464844\n",
      "step 499 : loss 16.922706604003906\n",
      "step 500 : loss 16.79110336303711\n",
      "step 501 : loss 16.759586334228516\n",
      "step 502 : loss 16.787952423095703\n",
      "step 503 : loss 16.755210876464844\n",
      "step 504 : loss 16.53169822692871\n",
      "step 505 : loss 16.318721771240234\n",
      "step 506 : loss 16.274585723876953\n",
      "step 507 : loss 16.521259307861328\n",
      "step 508 : loss 17.014202117919922\n",
      "step 509 : loss 17.45125389099121\n",
      "step 510 : loss 16.791425704956055\n",
      "step 511 : loss 16.11736488342285\n",
      "step 512 : loss 15.805623054504395\n",
      "step 513 : loss 15.828622817993164\n",
      "step 514 : loss 15.994325637817383\n",
      "step 515 : loss 16.088375091552734\n",
      "step 516 : loss 15.992599487304688\n",
      "step 517 : loss 15.946113586425781\n",
      "step 518 : loss 16.26392364501953\n",
      "step 519 : loss 16.222991943359375\n",
      "step 520 : loss 15.80560302734375\n",
      "step 521 : loss 15.193931579589844\n",
      "step 522 : loss 14.956329345703125\n",
      "step 523 : loss 15.030972480773926\n",
      "step 524 : loss 15.239811897277832\n",
      "step 525 : loss 15.424896240234375\n",
      "step 526 : loss 15.377128601074219\n",
      "step 527 : loss 15.429003715515137\n",
      "step 528 : loss 15.409566879272461\n",
      "step 529 : loss 15.341484069824219\n",
      "step 530 : loss 14.974119186401367\n",
      "step 531 : loss 14.62939167022705\n",
      "step 532 : loss 14.47618579864502\n",
      "step 533 : loss 14.500265121459961\n",
      "step 534 : loss 14.623438835144043\n",
      "step 535 : loss 14.742422103881836\n",
      "step 536 : loss 14.838831901550293\n",
      "step 537 : loss 14.903460502624512\n",
      "step 538 : loss 15.026771545410156\n",
      "step 539 : loss 14.872491836547852\n",
      "step 540 : loss 14.614921569824219\n",
      "step 541 : loss 14.315503120422363\n",
      "step 542 : loss 14.168713569641113\n",
      "step 543 : loss 14.163511276245117\n",
      "step 544 : loss 14.260526657104492\n",
      "step 545 : loss 14.401087760925293\n",
      "step 546 : loss 14.432805061340332\n",
      "step 547 : loss 14.4091796875\n",
      "step 548 : loss 14.308245658874512\n",
      "step 549 : loss 14.263079643249512\n",
      "step 550 : loss 14.2191743850708\n",
      "step 551 : loss 14.141067504882812\n",
      "step 552 : loss 14.035807609558105\n",
      "step 553 : loss 13.940533638000488\n",
      "step 554 : loss 13.88410758972168\n",
      "step 555 : loss 13.867884635925293\n",
      "step 556 : loss 13.885313034057617\n",
      "step 557 : loss 13.931100845336914\n",
      "step 558 : loss 13.999992370605469\n",
      "step 559 : loss 14.085844039916992\n",
      "step 560 : loss 14.184310913085938\n",
      "step 561 : loss 14.242919921875\n",
      "step 562 : loss 14.276843070983887\n",
      "step 563 : loss 14.166404724121094\n",
      "step 564 : loss 14.031100273132324\n",
      "step 565 : loss 13.854701042175293\n",
      "step 566 : loss 13.734048843383789\n",
      "step 567 : loss 13.659464836120605\n",
      "step 568 : loss 13.624180793762207\n",
      "step 569 : loss 13.61555004119873\n",
      "step 570 : loss 13.626848220825195\n",
      "step 571 : loss 13.659541130065918\n",
      "step 572 : loss 13.71226978302002\n",
      "step 573 : loss 13.799481391906738\n",
      "step 574 : loss 13.883230209350586\n",
      "step 575 : loss 13.974004745483398\n",
      "step 576 : loss 13.974247932434082\n",
      "step 577 : loss 13.939645767211914\n",
      "step 578 : loss 13.857501029968262\n",
      "step 579 : loss 13.768068313598633\n",
      "step 580 : loss 13.687826156616211\n",
      "step 581 : loss 13.603022575378418\n",
      "step 582 : loss 13.533857345581055\n",
      "step 583 : loss 13.470744132995605\n",
      "step 584 : loss 13.423673629760742\n",
      "step 585 : loss 13.387784004211426\n",
      "step 586 : loss 13.362113952636719\n",
      "step 587 : loss 13.342809677124023\n",
      "step 588 : loss 13.326995849609375\n",
      "step 589 : loss 13.312812805175781\n",
      "step 590 : loss 13.299577713012695\n",
      "step 591 : loss 13.287745475769043\n",
      "step 592 : loss 13.278663635253906\n",
      "step 593 : loss 13.275337219238281\n",
      "step 594 : loss 13.28327465057373\n",
      "step 595 : loss 13.314981460571289\n",
      "step 596 : loss 13.386088371276855\n",
      "step 597 : loss 13.516942977905273\n",
      "step 598 : loss 13.652944564819336\n",
      "step 599 : loss 13.717964172363281\n",
      "step 600 : loss 13.58265209197998\n",
      "step 601 : loss 13.396398544311523\n",
      "step 602 : loss 13.241887092590332\n",
      "step 603 : loss 13.176997184753418\n",
      "step 604 : loss 13.21142292022705\n",
      "step 605 : loss 13.40998649597168\n",
      "step 606 : loss 13.85154914855957\n",
      "step 607 : loss 14.218612670898438\n",
      "step 608 : loss 14.070222854614258\n",
      "step 609 : loss 13.598817825317383\n",
      "step 610 : loss 13.336633682250977\n",
      "step 611 : loss 13.26646900177002\n",
      "step 612 : loss 13.237016677856445\n",
      "step 613 : loss 13.207789421081543\n",
      "step 614 : loss 13.17186450958252\n",
      "step 615 : loss 13.217257499694824\n",
      "step 616 : loss 13.359174728393555\n",
      "step 617 : loss 13.530423164367676\n",
      "step 618 : loss 13.548787117004395\n",
      "step 619 : loss 13.3171968460083\n",
      "step 620 : loss 13.098653793334961\n",
      "step 621 : loss 12.982340812683105\n",
      "step 622 : loss 12.960797309875488\n",
      "step 623 : loss 12.988197326660156\n",
      "step 624 : loss 13.020177841186523\n",
      "step 625 : loss 13.033703804016113\n",
      "step 626 : loss 13.008100509643555\n",
      "step 627 : loss 12.983787536621094\n",
      "step 628 : loss 12.987167358398438\n",
      "step 629 : loss 13.052459716796875\n",
      "step 630 : loss 13.177988052368164\n",
      "step 631 : loss 13.287923812866211\n",
      "step 632 : loss 13.271588325500488\n",
      "step 633 : loss 13.123098373413086\n",
      "step 634 : loss 12.965907096862793\n",
      "step 635 : loss 12.870292663574219\n",
      "step 636 : loss 12.830263137817383\n",
      "step 637 : loss 12.82537841796875\n",
      "step 638 : loss 12.830967903137207\n",
      "step 639 : loss 12.842781066894531\n",
      "step 640 : loss 12.848358154296875\n",
      "step 641 : loss 12.857511520385742\n",
      "step 642 : loss 12.867874145507812\n",
      "step 643 : loss 12.892341613769531\n",
      "step 644 : loss 12.933046340942383\n",
      "step 645 : loss 12.99482250213623\n",
      "step 646 : loss 13.059137344360352\n",
      "step 647 : loss 13.087498664855957\n",
      "step 648 : loss 13.041574478149414\n",
      "step 649 : loss 12.948968887329102\n",
      "step 650 : loss 12.840174674987793\n",
      "step 651 : loss 12.75629997253418\n",
      "step 652 : loss 12.69399356842041\n",
      "step 653 : loss 12.653740882873535\n",
      "step 654 : loss 12.626540184020996\n",
      "step 655 : loss 12.609430313110352\n",
      "step 656 : loss 12.600214004516602\n",
      "step 657 : loss 12.597047805786133\n",
      "step 658 : loss 12.601936340332031\n",
      "step 659 : loss 12.614866256713867\n",
      "step 660 : loss 12.643611907958984\n",
      "step 661 : loss 12.68795394897461\n",
      "step 662 : loss 12.753173828125\n",
      "step 663 : loss 12.824350357055664\n",
      "step 664 : loss 12.876167297363281\n",
      "step 665 : loss 12.900815963745117\n",
      "step 666 : loss 12.88048267364502\n",
      "step 667 : loss 12.864587783813477\n",
      "step 668 : loss 12.829229354858398\n",
      "step 669 : loss 12.80394458770752\n",
      "step 670 : loss 12.748666763305664\n",
      "step 671 : loss 12.679646492004395\n",
      "step 672 : loss 12.598997116088867\n",
      "step 673 : loss 12.521649360656738\n",
      "step 674 : loss 12.461341857910156\n",
      "step 675 : loss 12.419853210449219\n",
      "step 676 : loss 12.39631175994873\n",
      "step 677 : loss 12.388444900512695\n",
      "step 678 : loss 12.394552230834961\n",
      "step 679 : loss 12.419293403625488\n",
      "step 680 : loss 12.469313621520996\n",
      "step 681 : loss 12.564193725585938\n",
      "step 682 : loss 12.68435001373291\n",
      "step 683 : loss 12.800771713256836\n",
      "step 684 : loss 12.79582691192627\n",
      "step 685 : loss 12.700826644897461\n",
      "step 686 : loss 12.571623802185059\n",
      "step 687 : loss 12.471145629882812\n",
      "step 688 : loss 12.408183097839355\n",
      "step 689 : loss 12.368867874145508\n",
      "step 690 : loss 12.348180770874023\n",
      "step 691 : loss 12.343652725219727\n",
      "step 692 : loss 12.361427307128906\n",
      "step 693 : loss 12.410451889038086\n",
      "step 694 : loss 12.479330062866211\n",
      "step 695 : loss 12.55443000793457\n",
      "step 696 : loss 12.549487113952637\n",
      "step 697 : loss 12.49632740020752\n",
      "step 698 : loss 12.395251274108887\n",
      "step 699 : loss 12.322386741638184\n",
      "step 700 : loss 12.281205177307129\n",
      "step 701 : loss 12.28004264831543\n",
      "step 702 : loss 12.320634841918945\n",
      "step 703 : loss 12.410821914672852\n",
      "step 704 : loss 12.524556159973145\n",
      "step 705 : loss 12.592494010925293\n",
      "step 706 : loss 12.536904335021973\n",
      "step 707 : loss 12.418387413024902\n",
      "step 708 : loss 12.298690795898438\n",
      "step 709 : loss 12.229302406311035\n",
      "step 710 : loss 12.191243171691895\n",
      "step 711 : loss 12.179023742675781\n",
      "step 712 : loss 12.177844047546387\n",
      "step 713 : loss 12.191499710083008\n",
      "step 714 : loss 12.210162162780762\n",
      "step 715 : loss 12.247770309448242\n",
      "step 716 : loss 12.283748626708984\n",
      "step 717 : loss 12.33502197265625\n",
      "step 718 : loss 12.36181640625\n",
      "step 719 : loss 12.378206253051758\n",
      "step 720 : loss 12.356680870056152\n",
      "step 721 : loss 12.316797256469727\n",
      "step 722 : loss 12.266039848327637\n",
      "step 723 : loss 12.221171379089355\n",
      "step 724 : loss 12.19556999206543\n",
      "step 725 : loss 12.195775032043457\n",
      "step 726 : loss 12.221261024475098\n",
      "step 727 : loss 12.268555641174316\n",
      "step 728 : loss 12.292281150817871\n",
      "step 729 : loss 12.290456771850586\n",
      "step 730 : loss 12.219012260437012\n",
      "step 731 : loss 12.139744758605957\n",
      "step 732 : loss 12.057413101196289\n",
      "step 733 : loss 12.002711296081543\n",
      "step 734 : loss 11.971088409423828\n",
      "step 735 : loss 11.96240234375\n",
      "step 736 : loss 11.97494125366211\n",
      "step 737 : loss 12.014364242553711\n",
      "step 738 : loss 12.08574390411377\n",
      "step 739 : loss 12.193272590637207\n",
      "step 740 : loss 12.289079666137695\n",
      "step 741 : loss 12.331116676330566\n",
      "step 742 : loss 12.256916999816895\n",
      "step 743 : loss 12.15717887878418\n",
      "step 744 : loss 12.065786361694336\n",
      "step 745 : loss 12.027273178100586\n",
      "step 746 : loss 12.027355194091797\n",
      "step 747 : loss 12.062204360961914\n",
      "step 748 : loss 12.097786903381348\n",
      "step 749 : loss 12.118106842041016\n",
      "step 750 : loss 12.083142280578613\n",
      "step 751 : loss 12.020414352416992\n",
      "step 752 : loss 11.949870109558105\n",
      "step 753 : loss 11.898992538452148\n",
      "step 754 : loss 11.872106552124023\n",
      "step 755 : loss 11.870051383972168\n",
      "step 756 : loss 11.893470764160156\n",
      "step 757 : loss 11.949586868286133\n",
      "step 758 : loss 12.038823127746582\n",
      "step 759 : loss 12.148397445678711\n",
      "step 760 : loss 12.201324462890625\n",
      "step 761 : loss 12.177848815917969\n",
      "step 762 : loss 12.062865257263184\n",
      "step 763 : loss 11.960768699645996\n",
      "step 764 : loss 11.88648509979248\n",
      "step 765 : loss 11.856197357177734\n",
      "step 766 : loss 11.85168743133545\n",
      "step 767 : loss 11.872238159179688\n",
      "step 768 : loss 11.900545120239258\n",
      "step 769 : loss 11.940035820007324\n",
      "step 770 : loss 11.953920364379883\n",
      "step 771 : loss 11.955053329467773\n",
      "step 772 : loss 11.91653060913086\n",
      "step 773 : loss 11.880033493041992\n",
      "step 774 : loss 11.847428321838379\n",
      "step 775 : loss 11.843975067138672\n",
      "step 776 : loss 11.865501403808594\n",
      "step 777 : loss 11.90869140625\n",
      "step 778 : loss 11.952028274536133\n",
      "step 779 : loss 11.971013069152832\n",
      "step 780 : loss 11.952463150024414\n",
      "step 781 : loss 11.90978717803955\n",
      "step 782 : loss 11.863001823425293\n",
      "step 783 : loss 11.840121269226074\n",
      "step 784 : loss 11.834067344665527\n",
      "step 785 : loss 11.856548309326172\n",
      "step 786 : loss 11.86170482635498\n",
      "step 787 : loss 11.862876892089844\n",
      "step 788 : loss 11.817529678344727\n",
      "step 789 : loss 11.773855209350586\n",
      "step 790 : loss 11.72397518157959\n",
      "step 791 : loss 11.694158554077148\n",
      "step 792 : loss 11.675241470336914\n",
      "step 793 : loss 11.673847198486328\n",
      "step 794 : loss 11.683954238891602\n",
      "step 795 : loss 11.712858200073242\n",
      "step 796 : loss 11.751895904541016\n",
      "step 797 : loss 11.8023099899292\n",
      "step 798 : loss 11.831380844116211\n",
      "step 799 : loss 11.836576461791992\n",
      "step 800 : loss 11.799250602722168\n",
      "step 801 : loss 11.753087997436523\n",
      "step 802 : loss 11.709753036499023\n",
      "step 803 : loss 11.693314552307129\n",
      "step 804 : loss 11.708131790161133\n",
      "step 805 : loss 11.76652717590332\n",
      "step 806 : loss 11.844706535339355\n",
      "step 807 : loss 11.908550262451172\n",
      "step 808 : loss 11.88011360168457\n",
      "step 809 : loss 11.80205249786377\n",
      "step 810 : loss 11.70341682434082\n",
      "step 811 : loss 11.641134262084961\n",
      "step 812 : loss 11.60517406463623\n",
      "step 813 : loss 11.597105979919434\n",
      "step 814 : loss 11.603638648986816\n",
      "step 815 : loss 11.627815246582031\n",
      "step 816 : loss 11.651817321777344\n",
      "step 817 : loss 11.677528381347656\n",
      "step 818 : loss 11.672144889831543\n",
      "step 819 : loss 11.657965660095215\n",
      "step 820 : loss 11.624659538269043\n",
      "step 821 : loss 11.610849380493164\n",
      "step 822 : loss 11.61320686340332\n",
      "step 823 : loss 11.651249885559082\n",
      "step 824 : loss 11.706048011779785\n",
      "step 825 : loss 11.763466835021973\n",
      "step 826 : loss 11.772518157958984\n",
      "step 827 : loss 11.726850509643555\n",
      "step 828 : loss 11.645576477050781\n",
      "step 829 : loss 11.572065353393555\n",
      "step 830 : loss 11.521145820617676\n",
      "step 831 : loss 11.497089385986328\n",
      "step 832 : loss 11.494197845458984\n",
      "step 833 : loss 11.515554428100586\n",
      "step 834 : loss 11.556039810180664\n",
      "step 835 : loss 11.62110424041748\n",
      "step 836 : loss 11.669201850891113\n",
      "step 837 : loss 11.695425987243652\n",
      "step 838 : loss 11.64643669128418\n",
      "step 839 : loss 11.580617904663086\n",
      "step 840 : loss 11.504859924316406\n",
      "step 841 : loss 11.45320987701416\n",
      "step 842 : loss 11.419702529907227\n",
      "step 843 : loss 11.403603553771973\n",
      "step 844 : loss 11.400620460510254\n",
      "step 845 : loss 11.410387992858887\n",
      "step 846 : loss 11.436196327209473\n",
      "step 847 : loss 11.473264694213867\n",
      "step 848 : loss 11.518173217773438\n",
      "step 849 : loss 11.534721374511719\n",
      "step 850 : loss 11.525650024414062\n",
      "step 851 : loss 11.481365203857422\n",
      "step 852 : loss 11.442148208618164\n",
      "step 853 : loss 11.419622421264648\n",
      "step 854 : loss 11.430893898010254\n",
      "step 855 : loss 11.494218826293945\n",
      "step 856 : loss 11.61903190612793\n",
      "step 857 : loss 11.789965629577637\n",
      "step 858 : loss 11.833440780639648\n",
      "step 859 : loss 11.746749877929688\n",
      "step 860 : loss 11.554361343383789\n",
      "step 861 : loss 11.424528121948242\n",
      "step 862 : loss 11.352645874023438\n",
      "step 863 : loss 11.32154655456543\n",
      "step 864 : loss 11.311861038208008\n",
      "step 865 : loss 11.315417289733887\n",
      "step 866 : loss 11.332468032836914\n",
      "step 867 : loss 11.363576889038086\n",
      "step 868 : loss 11.415754318237305\n",
      "step 869 : loss 11.470852851867676\n",
      "step 870 : loss 11.520622253417969\n",
      "step 871 : loss 11.50815486907959\n",
      "step 872 : loss 11.459972381591797\n",
      "step 873 : loss 11.381656646728516\n",
      "step 874 : loss 11.332347869873047\n",
      "step 875 : loss 11.327157020568848\n",
      "step 876 : loss 11.388404846191406\n",
      "step 877 : loss 11.503946304321289\n",
      "step 878 : loss 11.631036758422852\n",
      "step 879 : loss 11.648117065429688\n",
      "step 880 : loss 11.56092357635498\n",
      "step 881 : loss 11.422394752502441\n",
      "step 882 : loss 11.335963249206543\n",
      "step 883 : loss 11.296415328979492\n",
      "step 884 : loss 11.300724029541016\n",
      "step 885 : loss 11.321730613708496\n",
      "step 886 : loss 11.351243019104004\n",
      "step 887 : loss 11.354622840881348\n",
      "step 888 : loss 11.34309196472168\n",
      "step 889 : loss 11.305720329284668\n",
      "step 890 : loss 11.279253005981445\n",
      "step 891 : loss 11.262163162231445\n",
      "step 892 : loss 11.273625373840332\n",
      "step 893 : loss 11.299524307250977\n",
      "step 894 : loss 11.34339714050293\n",
      "step 895 : loss 11.37738037109375\n",
      "step 896 : loss 11.394927024841309\n",
      "step 897 : loss 11.377111434936523\n",
      "step 898 : loss 11.35203742980957\n",
      "step 899 : loss 11.32634162902832\n",
      "step 900 : loss 11.332334518432617\n",
      "step 901 : loss 11.358134269714355\n",
      "step 902 : loss 11.407377243041992\n",
      "step 903 : loss 11.420117378234863\n",
      "step 904 : loss 11.402547836303711\n",
      "step 905 : loss 11.332367897033691\n",
      "step 906 : loss 11.264592170715332\n",
      "step 907 : loss 11.209602355957031\n",
      "step 908 : loss 11.178491592407227\n",
      "step 909 : loss 11.1671781539917\n",
      "step 910 : loss 11.171911239624023\n",
      "step 911 : loss 11.19238567352295\n",
      "step 912 : loss 11.219198226928711\n",
      "step 913 : loss 11.24846363067627\n",
      "step 914 : loss 11.258458137512207\n",
      "step 915 : loss 11.252508163452148\n",
      "step 916 : loss 11.230359077453613\n",
      "step 917 : loss 11.212760925292969\n",
      "step 918 : loss 11.212743759155273\n",
      "step 919 : loss 11.235562324523926\n",
      "step 920 : loss 11.290937423706055\n",
      "step 921 : loss 11.338826179504395\n",
      "step 922 : loss 11.371333122253418\n",
      "step 923 : loss 11.32210922241211\n",
      "step 924 : loss 11.255746841430664\n",
      "step 925 : loss 11.176956176757812\n",
      "step 926 : loss 11.127900123596191\n",
      "step 927 : loss 11.096993446350098\n",
      "step 928 : loss 11.086141586303711\n",
      "step 929 : loss 11.087181091308594\n",
      "step 930 : loss 11.105859756469727\n",
      "step 931 : loss 11.137765884399414\n",
      "step 932 : loss 11.191061019897461\n",
      "step 933 : loss 11.235968589782715\n",
      "step 934 : loss 11.271027565002441\n",
      "step 935 : loss 11.248873710632324\n",
      "step 936 : loss 11.211674690246582\n",
      "step 937 : loss 11.158145904541016\n",
      "step 938 : loss 11.124441146850586\n",
      "step 939 : loss 11.106404304504395\n",
      "step 940 : loss 11.110770225524902\n",
      "step 941 : loss 11.132043838500977\n",
      "step 942 : loss 11.166509628295898\n",
      "step 943 : loss 11.197893142700195\n",
      "step 944 : loss 11.200784683227539\n",
      "step 945 : loss 11.172800064086914\n",
      "step 946 : loss 11.120633125305176\n",
      "step 947 : loss 11.076499938964844\n",
      "step 948 : loss 11.048410415649414\n",
      "step 949 : loss 11.04679012298584\n",
      "step 950 : loss 11.070314407348633\n",
      "step 951 : loss 11.120790481567383\n",
      "step 952 : loss 11.188194274902344\n",
      "step 953 : loss 11.234437942504883\n",
      "step 954 : loss 11.240827560424805\n",
      "step 955 : loss 11.179622650146484\n",
      "step 956 : loss 11.106019973754883\n",
      "step 957 : loss 11.036429405212402\n",
      "step 958 : loss 10.996930122375488\n",
      "step 959 : loss 10.986132621765137\n",
      "step 960 : loss 11.005592346191406\n",
      "step 961 : loss 11.040605545043945\n",
      "step 962 : loss 11.076227188110352\n",
      "step 963 : loss 11.067937850952148\n",
      "step 964 : loss 11.033879280090332\n",
      "step 965 : loss 10.982584953308105\n",
      "step 966 : loss 10.94491195678711\n",
      "step 967 : loss 10.923673629760742\n",
      "step 968 : loss 10.91997241973877\n",
      "step 969 : loss 10.940486907958984\n",
      "step 970 : loss 10.998067855834961\n",
      "step 971 : loss 11.118322372436523\n",
      "step 972 : loss 11.256309509277344\n",
      "step 973 : loss 11.346105575561523\n",
      "step 974 : loss 11.247031211853027\n",
      "step 975 : loss 11.115911483764648\n",
      "step 976 : loss 11.006444931030273\n",
      "step 977 : loss 10.963875770568848\n",
      "step 978 : loss 10.9633150100708\n",
      "step 979 : loss 10.991144180297852\n",
      "step 980 : loss 11.03308391571045\n",
      "step 981 : loss 11.066827774047852\n",
      "step 982 : loss 11.073762893676758\n",
      "step 983 : loss 11.045953750610352\n",
      "step 984 : loss 11.00248908996582\n",
      "step 985 : loss 10.969758987426758\n",
      "step 986 : loss 10.959972381591797\n",
      "step 987 : loss 10.988191604614258\n",
      "step 988 : loss 11.03454875946045\n",
      "step 989 : loss 11.091952323913574\n",
      "step 990 : loss 11.090112686157227\n",
      "step 991 : loss 11.053739547729492\n",
      "step 992 : loss 10.974782943725586\n",
      "step 993 : loss 10.913832664489746\n",
      "step 994 : loss 10.87048625946045\n",
      "step 995 : loss 10.851834297180176\n",
      "step 996 : loss 10.849074363708496\n",
      "step 997 : loss 10.865670204162598\n",
      "step 998 : loss 10.898048400878906\n",
      "step 999 : loss 10.951601028442383\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder()\n",
    "autoencoder.train()\n",
    "lr = 1e-4\n",
    "num_steps = 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters() , lr=lr)\n",
    "for step in range(num_steps):\n",
    "    loss_iter = 0\n",
    "    for image in dataset_train:\n",
    "        y = autoencoder(image)\n",
    "        loss = (y-image).norm()\n",
    "        loss_iter += loss\n",
    "    loss = loss_iter/num_images_train\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"step {step} : loss {loss.item()}\")\n",
    "\n",
    "# result = autoencoder(new_init_tensor)\n",
    "# tensor_to_image(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD2eiiiviCwoopCQoJJAA5JNAtgZlRSzMFA7k4qjLqsSnEas/v0FULy8a6fuIx91f6mq1fV4LIoKKlidX27HyWOz+o5uGG0Xfuaq6upYboSB6hs1dguYrgZjcE91PUVztOR2jcOhIYdCK3xGQ4ecf3Xuv71+Jhhs/xMJfvveXpZ/Kx0tFV7O6F1DuwA44YVYr5GrSnSm6c1Zo+wo1YVoKpB3TCiiiszUKKKKACiiigAooooAKp6lIY7MgZ+chcg/wCfSrlUtUQtZ5GPlYE/y/rXZl6i8VTUtro4sxclhKjjvZmLRRRX6EfnIUUUUAXdMkKXYTnDgjr+NbVYmmIWvVIx8oJP8v61t18Zn6isWrdlf8f0sfbcPOTwjv3dvw/W4UUUV4h7oUUUUAFFFFABRRRQAUjKHQqwyCMEUtFCbTuhNJqzOeubZ7aTa3IP3W9ahrpJYkmTZIoZetZ0uknOYpB9H/xr7DBZ5SnFRxDtLv0f+R8bjshq05uWHXNHt1X+f5mZSgFiAASTwAKvrpMpYbpEA9Rk1etrGK2IYAs/94/0rfEZ1haUfcfM/L/Mww2R4qrL31yru/8AISwtTbQnfje3Jx29qtUUV8bXrTr1HUnuz7ShQhQpqlDZBRRRWRsFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADHUlEQVR4Ae2bQWoUURgGjXpGL+LOO3gRz+YJBMGWIUOIUJCerveb7spCox959VLVk7iITz9/ffvQ25yBj3PoyH8NFGD4OSjAcIDPw/xn/JffX5/fff37j0/fX//Vif78NPtNGLz/K/mUJcYCvEn9yxgnyzAQYLf6U2ZY/U34EPtbiaPOeRl15P2lAY61duxpI/Y36LoAhi/jzMUlFgXwTHknrymxIoDtyD5fLaEHWGNnDcUooQcwLn2mM90AKx/MlawDnwA3wIEXPetRYoD1j+R64uOPhRjg8ctd4YQCDFe2Akx9NZji7s5oBdh9oat9YAGGixegAMMGhvG9AgowbGAY3yugAMMGhvHWK2Dqh0emuLszWgF2X+hqH1iA4eJigPVfDdYTH68nBnj8clc4wQ2w8pFcyTrwyXADHHjRsx6lB1jzYK6hGA+BHmC7tG3HPt/wfj9zRQC1wbu2v5lZFEBq8N7tLw1weIMT2F8d4MAG57C/CRn4L0obdXvb/eMLp1F/8zAW4IZ/U4aTqf8vAtwusf0KJU7p/f6JD78C7ve47Dvr/hl6WcX8iReA/ehrAXTFDCgA+9HXAuiKGVAA9qOvBdAVM6AA7EdfC6ArZkAB2I++FkBXzIACsB99LYCumAEFYD/6WgBdMQMKwH70tQC6YgYUgP3oawF0xQwoAPvR1wLoihlQAPajrwXQFTOgAOxHXwugK2ZAAdiPvhZAV8yAArAffS2ArpgBBWA/+loAXTEDCsB+9LUAumIGFID96GsBdMUMKAD70dcC6IoZUAD2o68F0BUzoADsR18LoCtmQAHYj74WQFfMgAKwH30tgK6YAQVgP/paAF0xAwrAfvS1ALpiBhSA/ehrAXTFDCgA+9HXAuiKGVAA9qOvBdAVM6AA7EdfC6ArZkAB2I++FkBXzIACsB99LYCumAEFYD/6WgBdMQMKwH70tQC6YgYUgP3oawF0xQwoAPvR1wLoihlQAPajrwXQFTOgAOxHXwugK2ZAAdiPvhZAV8yAArAffS2ArpgBBWA/+loAXTEDCsB+9LUAumIGFID96OsfAYBe+5rE4JsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDjaKKK+xPTCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAopyo752qzY9Bmho3QZZGA9xWftYKXJzK/Ynnje19RtFFFaFBRRRQAUUUUAFFFFABRRRQAUUUUAFXrazGN8o57Ke31qGzj8ycE9F5/GtOvkeIs2qUpfVaLs92+vp/n/AMOeLmeMlB+yg7dwAAAAGAKKKK+KbueEVbizV1LRgK3oOhrOIIJBGCK26z7+PDiQfxcH619hw7m1SVRYSs7p7N7+np2+49rLMZJy9jN37f5FSiiivsz3QooooAKKKKACiiigAooooAu6f/y0/D+tXqyrWURTgnoeDWrX53xJQlTxzqPaSTXyVv0Pmc0puOIcns/+GCiiivnzzgqrf/6hf97+hq1WdfSh5Qg6J/OvayChKrjoNbR1f9ep3ZdTc8RG3TUq0UUV+lH1IUUUUAFFFFABRRRQAUUUUAFW7a78sbJMlex9KqUVy4zB0cZT9lWV1+K9DGtQhWjyTRspIkgyjA/SlZlQZZgB7msWivmnwnDmuqrt6a/ff9Dy3k0b6T09C7cXoKlYieerf4VSoor6LA4Chgqfs6K9X1fqenQw9OhHlgj/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAC8klEQVR4Ae2d0U3DMBRFC2IcRmALxkBiGPZAYggGoxWtkziO7fQlHKEcfnBS+110Ti7wUcTD2/PXyQ+OwCMXbfKFgALg50ABCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA45/g/H8V//3+uvT1vnx8Lr1Uv//g/xOuAzq/WuE+P7vWhALmDIc7q9APx06nfg0KGHMb1nejH0b0afC3oDGx63oT+udZPXMUkAvooZafWb5uTlPABF6T12R330V9pgIGinVSw771q8pkBVxxVhitB144sTRfARdYS3QKIAO3iikKCBDd4qgC/ujx/5U1L4ECtniMAzOOLmD+SAZgdh3NEo8uoIvZnpsUsCfdjtmHFpB9N+jAtc2Wce6hBWyDMzZFATF+4dMKCCOMDVBAjF/4tALCCGMDFBDjFz6tgDDC2AAFxPiFTx9aQP+bR8KcJwPGuYcWMKECXSgAAn+LPbqA8XeDG5N9P2eJRxewL+yO6QpY8T7ODp6NLdnjf96tgAayvV9WwIXw/MHcg3sxRQFX1EU6G2pYmq+AAfISo2HHvavKZAVMoFZITfatuajPVEDOss4r3926bk5TQAFhk1rhTOlWzxz/RKlE7nZv/PaF272uzz3ofwcpoA10lYZ+9Apoo892VEys5Z4m24CEgln4Q5jhnlIVkFAwCwUw3FOqAhIKZqEAhntKVUBCwSwUwHBPqT96M1YzgQ+U0QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDYooor8/OkKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKhnnEK+rnoKoSTySZ3McHsOld2GwFSuubZHj4/OqGDl7O3NLsv1Zq0VjAkHIOCKsRXboQHO5ffrXRVymcVeEr/gcWH4lozly1YOPne/36I0aKRWDqGU5B70teU007M+kTUldbBRRRSGFFFFABRRRQAUUUUAFIzBVLHoBmlqG7/49n/D+daUoc9SMX1aMMTVdKjOouib+5Gc7F3LN1JptFFfYJJKyPy6UnJuT3YUUUUCLVlIVkMfZv51frJh/wBfH/vD+da1fPZrTUaqkuqPuOHK0p4Zwl9l6fMKKKK8w+hCiiigAooooAKKKKACmyIHjZT3GKdRTTcXdEyipxcZbMxyCrFT1BwaStG5thKNy8OP1qgyMjbWBB96+pwuKhXjdb9j85zDLauDqNNXj0f9dRtFFSxW7ykYGF/vGuidSNOPNN2Rx0aNStNQpq7Y+zj3zbuy81o02ONYkCqOP506vl8ZiPb1eZbdD9DyvA/U8Oqb3er9QooorlPRCiiigAooooAKKKKACiiigApCAwwwBHoaWihO2qE0mrMYIowciNQR7U+iim5OW7JjCMPhVgooopFhRRRQAUUUUAFFFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACqklEQVR4Ae2bQU7jUBAFAXGJXIoLcD5YsMyluAZGlr4cRUqk7+4uuV2zckbjX54qP5gF8/r9c3nxF2fgjUNL/jdgAPg9MIABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDjXYABYAMw3gUYADYA412AAWADMN4FGAA2AONdABzgHeZv8J+Xj82nm8uv3+vN50Yf+AAPvA/P48/0K0EGGFqH6KcX6y2dMjABJtRv23TKAHwT3ml/lIg6ZxyIXFQHiLUWe1r/ABm+Ms6sLFG3gDxTeScXlCgKkO0o+/y8EhUBauzUUMJLVAQIf+hOB6YHqHwxK1lRL0F6gKgH7XpOboD6V7KeuPPNyA2w8+HOcLsB4MqJAaivBhR3rmRigLkHOttdBoCLG8AAsAEY7wIMABuA8S7AALABGJ+4AOqHRyjuXMnEAHMPdLa7DAAXzw1Q/9WgnrgzYG6AnQ93htvTA1S+kpWsqJcjPUDUg3Y9pyJAzYtZQwl/DyoCLA+dbSf7/HDv48CiAKkNjmt/0VIXIKnBoe1XBwhvcHT7QIDABg3sLzaY/6K0upv+8YUe6hf7WICVPZGhk/pVArOAlb3NsFw/GEQ/78MAH2A8SmPL4+94f1H6z9B7vL9jAPgdMIABYAMw3gUYADYA412AAWADMN4FGAA2AONdgAFgAzDeBRgANgDj/wDou1i30UP8bgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCvRRRXwZ+ohRRRQAVFcXMNpEZZ5VjQd2PX2HqfaodS1CLTLMzyAtztVR/E3p7dK4G8vJr+5aedsuegHRR6D2r3MpyaeOvOT5YLr39P8zgxmOjh/dWsjrZPFenJIVVZ5AP4lQYP5kGprXxHp10QplMLE4AlGO3XPT9a4SivpJcM4Jxsm0+9/wDgHmLNa6d3Y9RoridD1xtPcW9wS1qx+pjPqPb1H+T21fIZlltXAVeSeqez7/11R7OGxMMRDmjv1QUUUV5x0hRRRQAUUUUAFFFFABRRRQBxfiq6M2prACdsCAYIH3jySPwx+VYVa/iWN01yZmGBIqsvPUYA/mDWRX6nlUYxwVJQ25V971f4nyeLbded+4UUUV3nMFd34cujdaPEGJLRExEkAdOmPwIrhK7XwpG6aQWYYEkrMvPUYA/mDXzvE0YvBJvdNW/E9PKm1XsuxuUUUV+fn0QUUUUAFFFFABRRRQAUUUUAYfiTS3vrZZ4E3TxdQOrL6e5HYfWuKr1GsPVPDcN9I88D+TO3JGPlY+/oTxz+lfVZJncMPD6viPh6Pt5PyPJx+AdR+0p79UcVRWvJ4a1RJCqwrIB/EsgwfzwamtfCt9MQZ2SBc4OTubp1AHH619TLNcFGPO6sbet39y1PJWErt25GZVnZzX9ysEC5c9Seij1PtXoltbx2ltHBEMJGu0e/ufeodP0230yExwKfmOWduWb61br4jOc2+vTUYK0I7efn/ke7gcH9XjeXxMKKKK8M7wooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAADJUlEQVR4Ae2by00DQRAFMSI9yIAkCIQkCIHguLLCkmUJ+Wm9nureT/mALJ53Xm/VjPHBnH7enp989BGQfh/7v2YFKKCZQHO9J6BZwEtz/4369++PG8nT1+vnrWiLvz+t6lNQ4P4f7j5MrEXAXeivZWxdQ7+Axej3oaH5j/AQ+pOJUetcS6153ilgLLWxq9XQn1raBBC8iDVpEz0COFLcypCJBgE0I3r9sSaqBdTQqWkZYqJawJCh97RIqYDKjVnZ9ciGKBXwyKB7vbZOQP2WrG9csEvqBCwY7giXKKDZcpGArneDrt75VosEzB/oaK9UQLNxBSigmUBzvSdAAc0Emus9AQpoJtBcX3QCur480tU732qRgPkDHe2VCmg2Xieg/t2gvnGBzDoBC4Y7wiWlAiq3ZGXXIxulVMAjg+712moBNRuzpmXInqgWMA1N06HXH8L9skiDANTBtuhPKHoEQA42R79TwHAHW6TfLGCgg43Snwj0/4vSNMT0WPz1he2iP9/4WgScp7lLw9bRr1HAeabpZzCxD+6XO13XCbiMdZwnbR9Dj4M436kCMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzgQIyHzxVAI44Fygg88FTBeCIc4ECMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzgQIyHzxVAI44Fygg88FTBeCIc4ECMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzgQIyHzxVAI44Fygg88FTBeCIc4ECMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzgQIyHzxVAI44Fygg88FTBeCIc4ECMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzgQIyHzxVAI44Fygg88FTBeCIc4ECMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzgQIyHzxVAI44Fygg88FTBeCIc4ECMh88VQCOOBcoIPPBUwXgiHOBAjIfPFUAjjgXKCDzwVMF4IhzwS+98lYz6FiO9QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAeLUlEQVR4Ae2d23IkSVKG81glqWd7dwaztYU7uMC44IZH4wF4Rl5gDS6ABVuwmdkZSXXKA/F/v4cqsks9rZ5uqW5SMouK9PA4/+7hccjI+l/++fdVVf3T4e+T+9e//XNy2+o2udPuXXJv5l1yN9/cJ/d46pP7bq6T27Wj3M2Q3LFqklsPCp0mhe5HUYY5OdUR+kGM1e50k9zTJP8Iz/7YJv/9YSN6pQi7QSmccI8nhU6T6HOjaJuUW1XddfIrLPl7+e865XjXH5K7bcTfmL8X/+1WdPP3rYrSteLp2pPo8M81sXBrXIpTzSJXM2VOhUj+mri1vNVQd8k9HlX+jlLtibsnzXG8E5MqXXW98v2vGz18/8Nvk3v4YZ9ckkm/69+VWqD7x2+E/b/9Vh39+wf1581Ad9ePyV/X3yf33SAE7Q/C+HskoN2q9+q9+m8yLkbQOiruI2gzindIwJ7Qx1GIOxyF8QHKiTR/mISOHfl2YH84qSQHhGjGnUB03yru+0a43mzE8xsguplFeT+pnHe96mIZ7ZujeI6i943ibiaVoR+Rg0ZuC6KrwL7K3wbskzckoBoV1+60kd/InQZh/zCoJD0i9khqw7hNlGqrvOpOPPWgGH83i/7HdyrtrlesVQJSI1zzr3sP4ua98D4MwsthDy7A6bxVX4213OEBZIHHxxthqkYRzuClMXIrJGCSrMwn9e4Arg+TULAfhIgT6B4IPR7Efzwp5YFRYbRkHJXCCbrTn9GqzvHACDQaiZ3KNnXgeqN0ZuSjh6dXMlUNvQPvfSPODZLxAGeLW3nkQA5a1D/eLAFHkEodJ8Bd1Sq5W+xxJ/Dvtsp910s+hpNasmGMbI7vk38i3/tHlXZLa9S/E/8qAakRrvnX3TXC8nd7ud9Osnm+3Um/Y25Up1rSMHVyT8jEN2jPBploGvWfATRj/xxQlQ/060GdneRApKM6u+rRgwO2TXOUBqwYRRp06DAxiiAfIzwITJI/pWArpSblwVj22CDRquaW1BhLulbY7EBiB5Zb0LqlLl0nbG4KabCs1MhxjRxYAmwF2Z0pG5WuJsy4hjQr14iSdNhjB6R/qtAiyMHAyJcaIuV757IN75L/z0jnKgGpKa751/UjOnQUggYwO9Cr7uADo7+1+UQ/24pPpk/ib0BNi6YekYAd7iP0g7q8wgCpHknnEWnYI2AnhIKhp7oXXKqHER1Kah4VbC8RKWUmOSD5ZLmLvwJBM/q96RR3cyOysQ8Qq/lGsRgmwtqZKXkNEiu7gPxkCSjqFWOAkkxNo/RrajTBT3GSWlBRLBMdc5ERK7FCgmdstvooifSYYb3SM3be8rBKgBrnin8durO6tT0guLiTqtNGGnlgdjej3Rrr2UYxatshHRFwTrb0J/XobhIU95M4D6303T3j/k/MsXeePRL6gOXz43CbeH4khT3YOdj1jMGTTrTzHeB/xObBmE5zden0Y/qXtWZXiNsw471lrjthcU9YPj2z5akVp8e2CU6Gs2TXCekttpxHhTT4JMrMzKYlNFrDk2Nao8Ff30qBdA3TgUZDk6XkdKsUur1CW+q7Qeu0HkcTdf27Ygt4UaR6sEWBXu5R8zM2rzX7EV3Wn+hDdGVCTyr0CE9N/9vvFZ4BjX9C1w979f9pEC5Oe6RqJ8kYd1Lhw73pMq1HxqHxIMqEBEyCaYIgFGbCsFR2p63KYJtq6MQzMnqdmAlj8lQdOn2YKC1DQYN9MmIFjVgmDXicY4VHnK6dxwBLQA3PSGoT/tDptNvM4Hk4qn0OrUpSM5KdEIEdrboZNNDNSEDDvH2DRbeOAalZrvnXzejNDkQ36DuvWoxbIRfFm/Sa+sl2kQ2RiYCRVaAUkkKHWVrviGY/jhoDdqMwfjp9k9xHRoUHdL3pI3Pdn0H6zweNE3+ZhZ3DQXkdkJjjQRp5YAzwWtDAqDD1Eg3PDHr0e43R0zGiGV8nxomKqXCDBPSgD2/CnmsEWmdhM2t/5ei5NCKXnvSnGspVaKx9WRNQXxov6Xd4qEXNeObhtMGM23rdjHarkaSZcWiVALXaFf8SZtSrHX3sGWBLx9narUBHuBjhnvHOrHpOscau8k/o/RGd6LVMFpbSWrmM5yOIOB6wixgbRk2907qTdO5+J0QfLAE7JMAuEjDGGGArQljcHOD3HJiZ7ZGVlgP+LRp2RgI2nbI5YfmcsJdqijViEY22kRgPamYhxrhHNc8YAv2guzFyUQVqtRTmiclEMPnGjBprsKFVG9qkYU1stC5BFkPrkM7qXK0F0hKhMNjPQkrr8QCM2HZuQM3MGks1S6c3IQfoTaRn9tyPdDz6z9gAU+GexJ6kQbixO4Juz4p3LCE9gq/jTjzeQWOBCqslSRjK0sa3J7AsYlWjphDVVsq8usX1xBNxqlhYqtiSqIB4WFAevEo3I1e516j/PAZAsb1HLWwdhazYaLJ8eC7tsYGkva9wQO/3yA1Cm+YsitbYBlPB17/rtUCXV/ikVUds1REsB37RWUdWPHpbwZTVtoS7f/I8AICRQKzjn1g5GbABRtwBG39gJBj3Gg/Cz06c98gG1ta9HuVxYmIMGJmrU4TYofU+wQjgR+9DsWo7gsHG6/KqVjXin9DImGbVCLwnRMnlt5E3ek5rQSMz2z/eC7NVMyECdmvGQls1nh64hA1zKQ8Qe6+YIiVRTko4uZy05+pcrQW6ipUQL+r7fEDnHrbNi+YyCgIW1vsGv9SjzGY5jOxGSsyiseW9sj/h9/x2YHdpKtwRv/fCgo7V5IX0sZwHeP5BgUb8DSPNgKXh8QCopd0oFW3yHhau5w0enxba37Y5eETY0higitnmCQkoxwBIdWHPNEh/gzQ0DFaeS9nG32Ef1tARxTRnUvP9ibY1j8q6/l2lBRLc1dszM0Nrz4meNN4JTCszKlvtsYFijkz7MAoS9gWJkZWi0N2sjQxeP7Gf0BFjaDAqiRU7EFhZZFuN7D0B6GS3qGy2nSfvzSGdIwvzI/4G+oitNASPyuU9qZF5jNd8YuUH28P+KZ1FSpwx1xEq3RpTadEnavqzOkdwrLtd2obSzvCnA0yJ0SZhS92tIO5hHagqm3upbOL8H3TJKgFq3iv+ddZ3PVZE655nd6lmJ8Ar3Q3WQsPaXo0tUeP3erfnzCi0pDipC24TCEWfojcx09OsG1wHo/wdSLfr+WfH3NJyAFxSrnBiZNi+RlHHapVTbowpQJjzUsayt55m+4CwjfIrNHa2rfddcocSi4KnPWc4kQCfuWtIJ0YCWm+mlWrvYJNODf9DpEw7xMihpD0OucHIanWu0QLpQKP6YLaVwknNdNhFJcF6dTdNaH+v9iU9qVAsE+s7YBdrpTUkoyZkBX73c6YrAVO8uxsuepahIc3JxQvsUl5Cc22bgWgO7UAWG7GxSxzphLxKz8bpDWQxUgjbg1CKXmPe1wia5UwjyPLPY0PQYtRUCWPVqBgw45QUFI+g90it5wehJyQMqV1UL7eDnte/q7RAWEHWmOoV/dG3Lg4o86ZsBPqHrrQE2IKawKxVctjRxRjQMvP0ec0Nu6YDKW/QnltGnTusCLa2EjaQAHTxUMxa74h1i7sF6T3uFp7NnZDbM2e221FOr/J6TPK6vwsbNXKlXd+oEiQD2HRYLdMRC+xOwS8cez5sntl6BdZYK0Plh59Qy9kqAdHA1/rxuYbcy6EreQQEBsfC5cH2j12f2iy1uVHs7WbvtdrC6UF6ZyuLndsed2M/GtlGeWwogUHLEwZ3OvOjsrWRjvxOzZRMx16CxxRbcT4FzZZtsnxIB0lyyS3lWT6UQkiJGBei4taAHHRUeiLwa5lwsAPCLUIL+ioBbqqruclEVnfk0V/lsN/vqzjU9nUoeaTEp+m9amR82N6YrLU9Iw0NrtnmyJmi2ieTGXC8mdoDyC0roLfeT0Z89pwWPWAGjfD4dPEtuv4bdlPfgd+ePYzf4L5jn+s98+SOk0K3zHXvKM+Whd8Om8dv2niN3jtTtoKMx9JVi6Q/YL/QBNFuDn5iCZk4U3PcBaV4WCWgaIxreD1JjB4OLWZdFqUp1FVQZGmE1Yw0eEbn9dSYIXuFlaW/upcEtCjdVoclkvVgbah0OjauujthqwX1LTZSxyAwsX/QQPH+bcdJ0JZTET3+/k7pd+TYbbQS2nP+p+U0Xzr6qpSZ1fvUdBuzBJUh20WuEaUqFXy0g0nndpgD+6Lb73YzR7xpU8QtjSnzWInYv0pAasZr/vk4RCqB+qO0jm0PlL3nvsqjhVCTEUQF0M5+26RDd29IzsjdYjUz3a5m7JMJhKZXGhXZlgMnfI7slB0xeo6YJiMSEGMAce+QgDtLACce3iEBd5yE2HI2omOc2DDq+K2Y1u9EMh74XKbnxuFSiSwAgWZogVoVk+dL58yRmUwp+T/mXyXgsj3flJLHgCLTjAKT1HMZ9aYo3JSgg3RLTBOrqoplzVtZF6OXe1Y//H7LTKwBC2dD6Jb92/oduUjA0nqUfvyG2szpnQ0S0LP3u/lGod2txoB4+5e3Eu1vtlrR8tu5LWND4zPeuDGGIbVR30W1VX5jNma26Vl/puHDu4gEx6UZdI5jhqW7SsCyPd78KUuAuxLXPVb2W+n3KB+LePA7auNZJQO8j1rcsLA5sRM0M8EFcOltd6XnVcMtK50coE/vT2mCcPLbAz5Jx0nQeEfMEkASW7//zvy5ZQy4taWPJN0yG2gZA3rHKmYADSudMbMpEO2WzzV1nUSbC0j7XMXiDRnk2NIcjE6iaMmgwyndkf7OyUdDirj+XaUF8loQdkjYPBkGHxaopMcqoEln12fKYoRAumoslsCd3wkBp+mVrZR+t5EObDfMFThXMXPIbWK9cOI0UYMtNIHluOMBOfCcoLthHoAExDwAy6fmboYW66hlLtKQQuOdPq/+M2LFrnhA0z/UyJiNRhE9QLxol3PdgxzpLJh4uOBEENYx4LKl3pSSx4DIlH6mZ/JsjQ51aOG1PWCCOfN+k/rZtlDns2wgvWcFZvZpSGa/M++geT+2xtKPd+0ZFTZIwKkXPkLzktqGkWbjkYCTPx3Ss0G2NhvsIqPeLrH8jnyD5FkOQkYpen4DWZX0yJRPiojiOl6iV2FuBf0umqagK8x/HktiRyET0+8qAUVjXMO7lAA62r3twmR7n6diWhwzAKwIzwCSLZ6YvBbkN9O93tlwvsEnwrw34JcKfJTG717NPmrKGDBwgrpFAjr2pS0BcQMEUPM+1433BrjNIlY3sfE3oL5lC9tjT+wEID2WTr/h7J2G2O+ldj7TEHUvGmKp/Uu8lxLiVnLsl7qrBLy0pV6JbykBZJLRzUMoP/+ce96+Uj7ynoG0sN+nDfWGLra2bFk43bAeOaO1vTl6w8+RFdPRs2UoAzsKE1bQjJ1jSfI+BINIMqaYCdvSZ55saaiRA5cwdqThsbx6D9xljtVZqqvSq6znmkJIjih5T5jWwA40XyEqmf3Fv6sEvLipXocxHVO76O1Fhy4eUhmyxnRxFJolRuhxWr5JZPbGK1a5Vx99msinyeJtyzhzqXg+MdCyReszxn6vPE5dgOgeZRy3nwAe38njXa2YJficj6fd2Ps+9e133xbr9TbgPB+Oin5YX9fJ1lFe/T/X/ZLbYS93Vwl4eVu9CmfMhJdrfuTkzi3cgItRXtJh996ADaW4O8HnxdDpXn3MN/+o133a0uLX87PxyThK5Jlw8HswQYP7rJzHmDhBhGRYj8f7DZ7lFofQslyq0Fle7VfRXa9sp4tikQh5Dl2vNLIV78o7Vfi/wFkl4Asa72tEfcYKip536oZogXffFBQWjulwBir4KVEWOtf6mtA5sInej6NFCrDA+Fa4OElpYwVEO1ZYL4DW+26+tyXOS2Pp+7xmDEcZzE9lXMx1XZ4wfSgDY0xMeAg1zk0p6W6eL3dXCfjyNvyiFLIE0NsBl2f84MDSUGYXFIVaPxovvpHE6xyeW3omGVDjwZxWt8a7zzlPPnUK2PwWirWzJcA2mMUmn2jTuQegH+fdPMvNKbsyKnT4Aswus+hxMWJRO3P6pHegHsZnnHPyzwS+hLRKwEta6RV5sgSQRR7lnV/G6FPu7m1slbgxAtTYOo7TQTAHLAC833gJ2EVSCgiUPSUukmjZHisQavFBMox9lyy7jrVISA+RGvRAd5En3kC3Q12X4CQBojoXx4SQJckPX+yuEvDFTfhlCTxJgHu6SMydXqDDc+awlwMpBT6s4CF4TltOM51uzKLzhKLITF7r8Y+9P260Or8QCeL7VH7YPGYKYBMc0NVPLhX+bCQleswDXCMnbZRHC5TpyL/UE4R+gbNKwBc03teI+iQBSqyAyzJtAspQo+kZiJjTGAyj55xUQNPRALMR7ZzDQjc2wWOElvz4Q4RCHEjfJChlLo5q2TW4i8Sixq5LgD6CnZBSjhTIZMFjSukuWMuAX/KvEvBLrfMGYQsJWOZHh5a9an+4+lkQHBlSLAJByRp2mfZTZJNh8hiQ8auEfIddjilKAfTQxZ51m27hKeXGJVwUNLS8uDKn8Y5rb0RTzpflj1gK/PhfkcLHmRSySsAvt8+rh35cAsLOKUpQWj4Xoc9gbUEinRIXBZDibdvISkzZ0ihMK2Ace7aIiVdYsxyW5VQKzqrU70XyT+CHllmJpWK5cpfYD8ZI6Ov8rBLwddrxV6eykIAClDnB6PQixBTCY++0lAyWdXxW0oueGTUgi4ecVpEQ2jjWUAvZypyRWfrJ66yihN6PuPAY96SQQ03HDWAX/oK/4HtKeUFLD5bLsgzBUValjPQC/yoBL2ik12RJElDgLMP1l3KEJ3QrfLZbAgRFCmHPFGllSvAqxKkVRQjl7VgFY0ko2TPdCfkJ94IQYdBLFEcm8SOuUlSifYrQSKdst8sCZaZP/q4S8Mkmel2GxRjwiayMAuzo6HIo9pduKVWxqk5w5uG3iPucIFAWR4hiLR4SLYNSv9lqCtbzjyMV44qDYvQ68wWeM91pFzlGOhcRCsKv864S8Ova7avFekYCin7P2QTYipCACBRUe7AYa34o/U4JenDmtPUbCHWI0nyGB9oS6b/EH2VdJFSU37kXoYU3hV1wmv9j7oWEfYwx08/prxKQ2+RKv89IQFmS6Klzhz0Ffkha2sgOPbv2nRH7lEx4Pgz5MHWxXdIuKed0l4g+05/xlcuk5FLaSOZf5LR4eCa9F5DOpVsl4AXN9Zosn5CAyPrcYb+yLIsE/PASHL2c83PLVRao9JOO5yuxbPq5KX8m/yoBn9lgX5v9oxJQAtRYMGVBN3ZsAxQBXocp+WNlpsQafvNEAmVoWc+P0Use+yO5y4DPpSih2MF21KK0sb8WSYpzeWo6Aj74cdEWREirBCza5O0ffI3t84b3uTQlBj9m85qnDC1jndN6Td9r5/ii9JEeamnUl9gv/Z7srBLwmoB4Qdr5vqCyawp/ePnJ/gIGC7pyC8vBrBHhw1JckoNyGfBh1K/+XGZZ+AtvZGkKbn4r1CS1xqLWecn32bK67eJ+aRJYJeDZhno74nNjQAFxF8R9nQt1fsorM6DgaT3xAz5GhTNaUmiY2eJb0HPEr/MbYCsSW9RLOefcCz8k0yOc0maKUyuf8JuH9HObmKd0FTdiFpyrBLhNr+ameYC6w5Zs9E8UZgGYooCiZ07zfOjmUEciFNJyn9axzgkvY53pv9J3mVxJKTR1OeM1fhetUcTKXpW89LsNTfE6Urw1TduaEvwxRTq32CoBv7J/v1a0mAnncZy+pbOiP8spLP6yJ20PRKcWvW1EBC5y0qnEgaxI4lyFYPlQJM4MX9+3KIMzhrSoI9kWsuLjEouoZckc8IlamOnsrhJQNuEV/NyukPPN/WI5wCXogQD3ldGaORUcFEjlG2H+yspZW+ocpOOdQbJ49kMuzKv+LrMqnqjMcgwgFHp8HySkxHSKWSQQVXSgK+rQS5eoqwS8akd/OvEYA9zn1tpnfGZd/oh+X9Dpz1j4Kfo5p6OMM47c9VDweswwUkLBnlk+XeKvwlEq9nwQzoVQZfKCFhUramdO37CV24Z6UabLSuR0zjyOlXNS0qsE0HjXc9KnLdQfc3ypiL7hdXS/9Zhfalc/xa6vuzVeijRedIbZ91qZZ6Jf3be+68Qwyq5/VWmPH+dn0d7ib4nW81N5Lsj+CKPWcXeXW6xQ9lEvRNvjnKU/XMStlLnYcybpVQLeor9/IY8u3vj17Wx8N9hfIK18jwqd67e34kup7jIrcvow8MsrvDHrC0goXyMoeCiIbYmyTIGIkvTa/ihQ4NvlSm48W8pdx6LoJvhNnuDktrvyzq0IJdgmpm1If/XMt734trD/4CT5KgGv3dWfSD9dNaU+iJtKuOFwomfs1nG5oWAQN44bEUQICsAwimMNxJSwnRXBc4KQJIMH194CZJ8o7lcLjtyVc+h6a2oPSlh9zqtc/TdaXcd4ow0m+7OFoxHRX42KL/6RV9z/S8qeLflGvFUC3M5Xc7vJ9gwvY9XcA+F3E6fwq4fufa+nedCTYRFZ77uHSSfGAKygmhvQ8+lomEAZvkWFA3YL2ps8IHr5vTPlGP5SAvDH98JpgTxvgJ8b4rnMSx86UAq0j7+xHDaVx1eHcmNkaU+uEqB2vOJfNzLi+3uNcZ8sfdj428LuXA//3N+Z9b7AMDBN8D22vt3TWJ58Y2HIhGoXuLAGpLrW+5aG0k/gGzuXMqkChPZ34Wglf+FypnaBYirccadp+uQ45RamW76C0MbtdXzhm5vaRy4+5YrrmASvEkCjXc+JUxEtBmpL/zApTh/xVdcb708fGRAFafDNoP62dOAnujIGisRpY9o/9i/eLCvqnFdXCtJre6NwzuZCAiCEXEagfmIm7FUDgn3DXVzVhYYwsCfuTK2xME326OK7VP3l2ZERdJWA1+7qT6TfGX1xP7OtGncK/tl6HKvft9YiJOkKXDF18PApl/QVX8NGdN3mrD/fcRgQSs8L2MHhsMt1UwJf0zkX6imXkmT0PwUlj+rldZ4TOsBz2ibu+VVNJ9+J7RYj2Bg/chmev4z2QNvuaJn/pDkglBmt/rdtgW5ERdkKsj3DNf4xRvub8iNTPdMtAb4W2hSvePiWWxcekzd5wZQNI7yhVfkJPxEsGbYr3qj6lxB3acvsLRLmBKgjfsz64HNNXXJ/XfuErAxRa0Uw6v3thIGR1XevHrGRVgkom/wK/m7a6FMsu+NtcqfuXXIfb/bJrflSYwXI93y3q+6l6Y7Y/jWK/9joIxZ7KBN6zavefA4y1n8sB8a4weRJRfhLySjmnynZ1/27HI4ivwL2eF1a3/XesH9oWz4LtpisDyY+h+Z5Vf6alPB9wIgc+HLOIync8qXME1/XWSXgdTv6k6l3A9/Vqm4kBxtWb27BtW380auhwHWLwuvoen99ZYDeMorYjvKy6ol1JFvBWQKElHwr3EWpCthdhL0iwdnGWHWZT1Eq7wzaCvK9jjETRu59a/uE5eO5gr9/YNF42KqZLCW3nURgpsX8Pe9VAi4b/k0p3VQL+xOmzD19+OM7PtDFeP0TI/WJL3CNN+qtHWDgw1+V1z48J3Cv+mtiB0YI97kVu2cbtvdD1YOvAmSXVsibNsTlAJTlQ8Xw+2K2cE6e+BB8wG19mzC6fg+67TabbYq79/dbWVU70s4zcnC/uUuhqwSofa/415280DPep0J0fHNx62+YAon2pLvJvf7DR3wrd7/ngbb9Yz2EvqzRiScmC83RxysEkrCCyqMBVLpEGYQ3dXLu+VcFLQpgMpTYBfNo5+mAJRYrsfE9YYj8AI9XEDpaY8eo2XsGsFWiB9ZNR74zu0pA0eLX8HaPvTTRA1/7GsffJ//uoLWc+viYXOus+1r2fse36TasdXyfnqvqR1DwE5Q4DRAjB8F8zci+BfRLlC0gZ963czPELyQAQswAKO0PFMrfrrGlP9rmQfv7aza2A/+X4XGe1Krf1zfJ/YtX1W6kD3YipO9/y/1pq7nXKgFqiyv+df8361Puf7gTxm8Z1PlEY9XQrV67+B0qvOHbv1v0WmeNDzrqQbatd4NneturgLM/cVFULuSgkICIVfC8jfdz153iLJNXfJnNDi0/TI5rdPrM2sGxEn1uBPWG8bW/ZfcQyXjHQvF/Y1U+/HwQz9tUeM3lYy3Q/dtPPyvs/T8k57u/uU/un76T9m8O75MrG0jo3if37g+y+2+YGfTsef6ROUF8LNKWAKH/bvnw5hjSU4A+wWPxpAyu/FeMAR8pScxgjG5wPdICpse+Opr9aEvphn1gjwd/JWkYWfnZ7XfJ/9P+u+T+64F2/kiOK/mNWuD/AV0kiKTUJXqkAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACAAIADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD2Siiiuw8EKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKQuqnBYD6mgBaKQMrfdYH6GloAKKKKACiiigAooooAKKKKACiiigAoJwMmiq90+FCDv1oSuVGPM7DJbgscISF9fWoKKK0SsdsYqKsgBwcirENwc7XPHY1XooauEoqSszSoqK3fdFg9V4qWs2cMlZ2CiiigQUUUUAFFFFABRRRQAVUuv9aP92rdQ3KbkDD+GnHc0pO0inRRRWh2hRRRQBZtP4/wqzUcCbIgD1PJqSs3ucNR3k2FFFFIgKKKKACiiigAooooAKKKKAK0tsSd0ePpVcqVOCCPrWjRVKRtGs0rMzlRnOFBNWobfadz4J7D0qeik5XFOs5KyCiiikZBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAACsElEQVR4Ae2bMW7CQBQFkygF58j921yHc1CkiCNLK2Rk7Bjvzlt7qIwx+75n9iEKeP++/bz54Ah8cNEm/xFQALwPFKAAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wAFwATgeBugAJgAHG8DFAATgONtgAJgAnC8DVAATACOtwEKgAnA8TZAATABOP4Tzv9//OV6m3vT7esy91Ls+W4EPOFe4JZrOjLRgYCCtYBePBjf0oWGaAEb0N+76UJD7regF+kXE3utUxbc9yBUwL7U9l3t+AJq8Kqx5i4m4hpQj1S9lV8xkSWgNqPa628wESSgDZ02KetNBAlYP/SRrkwR0HJjtsxa3CspAhYHPeoFEQLab8n2iXMbKELA3HBnOK8A2DIvgPo0oHInwnkBk4HO9lQBsHEFKAAmAMfbAAXABOB4G6AAmAAczzeA+vEIlTsRzguYDHS2pwqAjUcIaP9p0D5xznOEgLnhznA+RUDLLdkya3EPpQhYHPSoFwQJaLMx26Ss3y5BAoaha9Opvf567uXKLAFVHQTSH+43TkAlB5n0QwXs7iCWfq6AHR0k0x9uM/ovSiO7zT9fCEc/0E8XMI64QUMX6Me7i27AOOK9huH4SSE64l5urRsBZeIeKZfhHw8Sv4Y+TnngMwqA5SpAATABON4GKAAmAMfbAAXABOB4G6AAmAAcbwMUABOA422AAmACcLwNUABMAI63AQqACcDxNkABMAE43gYoACYAx9sABcAE4HgboACYABxvAxQAE4DjbYACYAJwvA1QAEwAjrcBCoAJwPE2QAEwATjeBigAJgDH2wBYwC9JQljbXRyMeQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoencoder.eval()\n",
    "for image in dataset_test:\n",
    "    result = autoencoder(image)\n",
    "    tensor_to_image(image.data).show()\n",
    "    tensor_to_image(result.data).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
